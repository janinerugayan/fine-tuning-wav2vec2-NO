{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbest_log_distribution: torch.Size([3, 8]) tensor([[ -86.1507,  -34.0924, -141.7790,  -31.1852,  -26.1941, -140.0957,\n",
      "          -73.2255, -100.2174],\n",
      "        [-124.9425,  -75.4899, -182.2957,  -34.4313,  -91.5705, -142.6367,\n",
      "          -81.5478, -147.0352],\n",
      "        [-116.9663,  -67.3523, -177.0863,  -40.7054,  -70.1901, -145.5527,\n",
      "          -85.9843, -179.9039]], device='cuda:0', requires_grad=True)\n",
      "sum_nbest_log_distribution: torch.Size([8]) tensor([ -86.1507,  -34.0924, -141.7790,  -31.1470,  -26.1941, -140.0159,\n",
      "         -73.2253, -100.2174], device='cuda:0', grad_fn=<LogsumexpBackward0>)\n",
      "normal_nbest_distribution: torch.Size([3, 8]) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6247e-01, 1.0000e+00, 9.2331e-01,\n",
      "         9.9976e-01, 1.0000e+00],\n",
      "        [1.4221e-17, 1.0503e-18, 2.5340e-18, 3.7465e-02, 4.0492e-29, 7.2744e-02,\n",
      "         2.4298e-04, 4.6480e-21],\n",
      "        [4.1393e-14, 3.5927e-15, 4.6369e-16, 7.0606e-05, 7.8121e-20, 3.9390e-03,\n",
      "         2.8762e-06, 2.4694e-35]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "asd_loss: tensor([0.5059, 0.2090, 3.2362, 0.3981, 0.8621, 4.6285, 2.8125, 1.6432],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "asd_loss_mean: tensor(1.7869, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"nbest_log_dist.pkl\", \"rb\") as file:\n",
    "    nbest_log_distribution = pickle.load(file)\n",
    "\n",
    "with open(\"asd_scores.pkl\", \"rb\") as file:\n",
    "    asd_scores = pickle.load(file)\n",
    "\n",
    "def compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=True):\n",
    "    # Computes log distribution\n",
    "    # (n, b) -> (b,): log( p1+p2+...+pn ) = log( exp(log_p1)+exp(log_p2)+...+exp(log_pn) )\n",
    "    sum_nbest_log_distribution = torch.logsumexp(nbest_log_distribution, 0)\n",
    "    print(\"nbest_log_distribution:\", nbest_log_distribution.size(), nbest_log_distribution)\n",
    "    print(\"sum_nbest_log_distribution:\", sum_nbest_log_distribution.size(), sum_nbest_log_distribution)\n",
    "\n",
    "    # Re-normalized over just the N-best hypotheses.\n",
    "    # (n, b) - (b,) -> (n, b): exp(log_p)/exp(log_p_sum) = exp(log_p-log_p_sum)\n",
    "    normal_nbest_distribution = torch.exp(nbest_log_distribution - sum_nbest_log_distribution)\n",
    "    print(\"normal_nbest_distribution:\", normal_nbest_distribution.size(), normal_nbest_distribution)\n",
    "\n",
    "    if normalized_asd == True:\n",
    "        mean_asd = torch.mean(asd_scores, 0)\n",
    "        asd_norm = asd_scores - mean_asd\n",
    "        print(\"mean_asd:\", mean_asd.size(), mean_asd)\n",
    "        print(\"asd_norm:\", asd_norm.size(), asd_norm)\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_norm, 0)\n",
    "    else:\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_scores, 0)\n",
    "\n",
    "    return asd_loss\n",
    "\n",
    "asd_loss = compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=False)\n",
    "print(\"asd_loss:\", asd_loss)\n",
    "print(\"asd_loss_mean:\", torch.mean(asd_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - mASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import jiwer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import BertModel, AutoModel, AutoModelForSequenceClassification\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from dtw import *\n",
    "from scipy.spatial import distance\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, display\n",
    "from scipy import stats\n",
    "# pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "metric_modelname = 'ltg/norbert2'  # changed to latest version of NorBERT (20-Mar-2023)\n",
    "metric_model = BertModel.from_pretrained(metric_modelname)\n",
    "metric_tokenizer = AutoTokenizer.from_pretrained(metric_modelname)\n",
    "\n",
    "# this is not working at the moment:\n",
    "# metric_modelname = \"ltg/norbert3-small\"\n",
    "# metric_model = AutoModel.from_pretrained(metric_modelname, trust_remote_code=True)\n",
    "# metric_tokenizer = AutoTokenizer.from_pretrained(metric_modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wordpieces: 17087\n",
      "Total whole words: 32917\n",
      "Total unused tokens: 100\n",
      "vocab dict length: 50104\n",
      "counter sum: 50104\n"
     ]
    }
   ],
   "source": [
    "vocabulary = metric_tokenizer.vocab\n",
    "\n",
    "wordpiece_counter = 0\n",
    "word_counter = 0\n",
    "unused_token_counter = 0\n",
    "for key, value in vocabulary.items():\n",
    "    if \"##\" in key:\n",
    "        wordpiece_counter += 1\n",
    "    elif \"unused\" in key:\n",
    "        unused_token_counter += 1\n",
    "    elif \"##\" not in key:\n",
    "        word_counter += 1\n",
    "print(f\"Total wordpieces: {wordpiece_counter}\")\n",
    "print(f\"Total whole words: {word_counter}\")\n",
    "print(f\"Total unused tokens: {unused_token_counter}\")\n",
    "\n",
    "print(\"vocab dict length:\", len(vocabulary))\n",
    "\n",
    "total_count = word_counter + wordpiece_counter + unused_token_counter\n",
    "print(\"counter sum:\", total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VISUAL INSPECTION OF ALIGNMENTS\n",
    "\n",
    "def get_asd_alignment(tokenized_ref, tokenized_hyp, model):\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                             hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                             hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(x=hyp_embedding_sequence, y=ref_embedding_sequence, dist_method=\"cosine\", keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "    return alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score\n",
    "\n",
    "\n",
    "def print_token_alignment(tokenizer, alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids):\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "\n",
    "    hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(hyp_alignment_idxs):\n",
    "        hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    hyp_alignment_input_ids_tensor = torch.from_numpy(hyp_alignment_input_ids)\n",
    "\n",
    "    ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(ref_alignment_idxs):\n",
    "        ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    ref_alignment_inpud_ids_tensor = torch.from_numpy(ref_alignment_input_ids)\n",
    "\n",
    "    hyp_alignment_tokens = tokenizer.convert_ids_to_tokens(hyp_alignment_input_ids_tensor)\n",
    "    ref_alignment_tokens = tokenizer.convert_ids_to_tokens(ref_alignment_inpud_ids_tensor)\n",
    "\n",
    "    ref_alignment_token_embeddings = []\n",
    "    for index in ref_alignment_idxs:\n",
    "        ref_alignment_token_embeddings.append(ref_embedding_sequence[index])\n",
    "\n",
    "    hyp_alignment_token_embeddings = []\n",
    "    for index in hyp_alignment_idxs:\n",
    "        hyp_alignment_token_embeddings.append(hyp_embedding_sequence[index])\n",
    "\n",
    "    cosdist_alignment_tokens = []\n",
    "    for i in range(len(ref_alignment_token_embeddings)):\n",
    "        ref_embedding = ref_alignment_token_embeddings[i]\n",
    "        hyp_embedding = hyp_alignment_token_embeddings[i]\n",
    "        cosdist_alignment_tokens.append(round((distance.cosine(ref_embedding, hyp_embedding)), 3))\n",
    "    hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    print(\"Token alignment table:\")\n",
    "    display(HTML(table))\n",
    "\n",
    "    # print(\"ASD score from alignment:\", total_dist/len(ref_alignment_token_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_modelname = \"bert-base-multilingual-cased\"\n",
    "# metric_model_multi = BertModel.from_pretrained(metric_modelname)\n",
    "# metric_tokenizer_multi = AutoTokenizer.from_pretrained(metric_modelname)\n",
    "\n",
    "# def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis, normalized=True):\n",
    "#     ref_text = re.sub(r\"\\s+\", \" \", reference.replace(\"[UNK]\", \"\"))\n",
    "#     hyp_text = re.sub(r\"\\s+\", \" \", hypothesis.replace(\"[UNK]\", \"\").replace(\"</s>\", \"\"))\n",
    "#     tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "#         model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "#     hidden_states_ref = model_output_ref.hidden_states\n",
    "#     hidden_states_hyp = model_output_hyp.hidden_states\n",
    "#     all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "#                             hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "#                             hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "#     all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "#                                 hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "#                                 hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "#     output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "#     output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "#     alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "#     num_tokens = len(output_mean_reference)\n",
    "#     if normalized == True:\n",
    "#         asd_score = alignment.distance / num_tokens\n",
    "#     else:\n",
    "#         asd_score = alignment.distance\n",
    "#     return asd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_results(mod_loss_results, orig_loss_results):\n",
    "    merged_results = orig_loss_results.join(mod_loss_results.set_index(\"segment_id\"), on=\"segment_id\")\n",
    "    merged_results = merged_results[['segment_id', 'ref_str', 'orig_asr', 'orig_cer', 'orig_wer', 'orig_asd', 'mod_asr', 'mod_cer', 'mod_wer', 'mod_asd']]\n",
    "    return merged_results\n",
    "\n",
    "def load_csv_to_df(csv_file, mod_loss):\n",
    "    results_df = pd.read_csv(csv_file, index_col=0)\n",
    "    if mod_loss == True:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\", \"ref_str\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"mod_asr\", \"wer\":\"mod_wer\", \"asd\":\"mod_asd\", \"cer\":\"mod_cer\"})\n",
    "    else:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"orig_asr\", \"wer\":\"orig_wer\", \"asd\":\"orig_asd\", \"cer\":\"orig_cer\"})\n",
    "    return results_df\n",
    "\n",
    "def load_results_to_df(dir_path, mod_loss):\n",
    "    df_list = []\n",
    "    for (root, dirs, files) in os.walk(dir_path, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(dir_path, fn)\n",
    "                df_list.append(load_csv_to_df(csv_path, mod_loss))\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_dir = \"../../nlp_models/ner_pos/finetuned_bert_pos_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pos_model_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pos_model_dir)\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "def get_pos_tags(nlp_pipeline, text, label_list_pos):\n",
    "    pos_tags = nlp_pipeline(text)\n",
    "\n",
    "    # merging subword tokens and making word list wiht pos tag list\n",
    "    labels = [label_list_pos[int(x[\"entity\"].split(\"_\")[1])] for x in pos_tags]\n",
    "    sub_words = [x[\"word\"] for x in pos_tags]\n",
    "    idx_for_labels = []\n",
    "    for i, item in enumerate(sub_words):\n",
    "        if item[0:2] != \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                idx_for_labels.append(i)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                idx_for_labels.append(i)\n",
    "        if item[0:2] == \"##\" and sub_words[i-1][0:2] != \"##\":\n",
    "            sub_word_combined = []\n",
    "            sub_word_combined.append(i-1)\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "        elif item[0:2] == \"##\" and sub_words[i-1][0:2] == \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "    new_word_list = []\n",
    "    new_label_list = []\n",
    "    for idx in idx_for_labels:\n",
    "        if type(idx) is int:\n",
    "            new_word_list.append(sub_words[idx])\n",
    "            new_label_list.append(labels[idx])\n",
    "        else:\n",
    "            new_word_list.append(\"\".join(sub_words[idx[0]:idx[1]]).replace(\"##\", \"\"))\n",
    "            new_label_list.append(labels[idx[0]])\n",
    "\n",
    "    return new_word_list, new_label_list\n",
    "\n",
    "\n",
    "def extract_errors_pos_tags(sub_count, ins_count, del_count, pos_tags, alignment, ref_labels, hyp_labels,\n",
    "                            ref_words, hyp_words):\n",
    "\n",
    "    label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "    sub = 0\n",
    "    ins = 0\n",
    "    dele = 0\n",
    "    pos = []\n",
    "    # ref_error_words = []\n",
    "    # asr_error_words = []\n",
    "    error_dict = []\n",
    "    pos_err_count = np.zeros(len(label_list_pos))\n",
    "\n",
    "    for i in range(len(alignment)):\n",
    "        ref_start = alignment[i].ref_start_idx\n",
    "        ref_end = alignment[i].ref_end_idx\n",
    "        hyp_start = alignment[i].hyp_start_idx\n",
    "        hyp_end = alignment[i].hyp_end_idx\n",
    "\n",
    "        if alignment[i].type != \"equal\":\n",
    "            errors = {\n",
    "                \"type\": alignment[i].type,\n",
    "                \"ref\": ref_words[ref_start:ref_end],\n",
    "                \"ref_pos\": ref_labels[ref_start:ref_end],\n",
    "                \"hyp\": hyp_words[hyp_start:hyp_end],\n",
    "                \"hyp_pos\": hyp_labels[hyp_start:hyp_end]\n",
    "            }\n",
    "            error_dict.append(errors)\n",
    "            if alignment[i].type == \"insert\":\n",
    "                hyp_pos = hyp_labels[hyp_start:hyp_end]\n",
    "                for tag in hyp_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "            elif alignment[i].type == \"substitute\":\n",
    "                ref_pos = ref_labels[ref_start:ref_end]\n",
    "                for tag in ref_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "                hyp_pos = hyp_labels[hyp_start:hyp_end]\n",
    "                for tag in hyp_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "            elif alignment[i].type == \"insert\":\n",
    "                hyp_pos = hyp_labels[hyp_start:hyp_end]\n",
    "                for tag in hyp_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "\n",
    "        if alignment[i].type == \"substitute\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                sub += 1\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "            # for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "            #     ref_words[ref_start+i] = word.upper()\n",
    "            #     ref_error_words.append(word)\n",
    "            # for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "            #     hyp_words[ref_start+i] = word.upper()\n",
    "            #     asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"insert\":\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "                ins += 1\n",
    "            # for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "            #     hyp_words[ref_start+i] = word.upper()\n",
    "            #     asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"delete\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                dele += 1\n",
    "            # for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "            #     ref_words[ref_start+i] = word.upper()\n",
    "            #     ref_error_words.append(word)\n",
    "\n",
    "    sub_count.append(sub)\n",
    "    ins_count.append(ins)\n",
    "    del_count.append(dele)\n",
    "    pos_tags.append(pos)\n",
    "\n",
    "    error_ref_text = \" \".join(ref_words)\n",
    "    error_asr_text = \" \".join(hyp_words)\n",
    "\n",
    "    return error_ref_text, error_asr_text, error_dict, pos_err_count\n",
    "\n",
    "\n",
    "def merge_df_wer_pos_tags(df, nlp_pipeline, label_list_pos):\n",
    "    mod_error_ref = []\n",
    "    mod_error_asr = []\n",
    "    # mod_ref_error_words = []\n",
    "    # mod_asr_error_words = []\n",
    "    mod_error_dict_list = []\n",
    "    mod_pos_err_count_list = []\n",
    "\n",
    "    orig_error_ref = []\n",
    "    orig_error_asr = []\n",
    "    # orig_ref_error_words = []\n",
    "    # orig_asr_error_words = []\n",
    "    orig_error_dict_list = []\n",
    "    orig_pos_err_count_list = []\n",
    "\n",
    "    mod_sub_count = []\n",
    "    mod_ins_count = []\n",
    "    mod_del_count = []\n",
    "    mod_pos_tags = []\n",
    "\n",
    "    orig_sub_count = []\n",
    "    orig_ins_count = []\n",
    "    orig_del_count = []\n",
    "    orig_pos_tags = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Error Tagging\"):\n",
    "        ref_words, ref_labels = get_pos_tags(nlp_pipeline, row.ref_str, label_list_pos)\n",
    "        orig_words, orig_labels = get_pos_tags(nlp_pipeline, row.orig_asr, label_list_pos)\n",
    "        mod_words, mod_labels = get_pos_tags(nlp_pipeline, row.mod_asr, label_list_pos)\n",
    "        out = jiwer.process_words([row.ref_str, row.ref_str], [row.orig_asr, row.mod_asr])\n",
    "        orig_alignment = out.alignments[0]\n",
    "        mod_alignment = out.alignments[1]\n",
    "\n",
    "        # substitute, insert, delete\n",
    "        mod_ref_text, mod_asr_text, mod_error_dict, mod_pos_err_count = extract_errors_pos_tags(mod_sub_count, mod_ins_count,\n",
    "                                                                             mod_del_count, mod_pos_tags, mod_alignment,\n",
    "                                                                             ref_labels, mod_labels, ref_words, mod_words)\n",
    "        orig_ref_text, orig_asr_text, orig_error_dict, orig_pos_err_count = extract_errors_pos_tags(orig_sub_count, orig_ins_count,\n",
    "                                                                                orig_del_count, orig_pos_tags,\n",
    "                                                                                orig_alignment, ref_labels,\n",
    "                                                                                orig_labels, ref_words, orig_words)\n",
    "\n",
    "        # whole text of the utterances\n",
    "        mod_error_ref.append(mod_ref_text)\n",
    "        mod_error_asr.append(mod_asr_text)\n",
    "        orig_error_ref.append(orig_ref_text)\n",
    "        orig_error_asr.append(orig_asr_text)\n",
    "\n",
    "        # specific words that are mistaken\n",
    "        # mod_ref_error_words.append(mod_ref_error)\n",
    "        # mod_asr_error_words.append(mod_asr_error)\n",
    "        # orig_ref_error_words.append(orig_ref_error)\n",
    "        # orig_asr_error_words.append(orig_asr_error)\n",
    "\n",
    "        # dictionary of wrong words\n",
    "        mod_error_dict_list.append(mod_error_dict)\n",
    "        mod_pos_err_count_list.append(mod_pos_err_count)\n",
    "        orig_error_dict_list.append(orig_error_dict)\n",
    "        orig_pos_err_count_list.append(orig_pos_err_count)\n",
    "\n",
    "    df.drop(labels=[\"ref_str\", \"orig_asr\", \"mod_asr\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    df[\"orig_ref\"] = orig_error_ref  # text/sentence form with uppercase errors\n",
    "    df[\"orig_asr\"] = orig_error_asr  # text/sentence form with uppercase errors\n",
    "    # df[\"orig_ref_error_words\"] = orig_ref_error_words\n",
    "    # df[\"orig_asr_error_words\"] = orig_asr_error_words\n",
    "    df[\"orig_error_dict\"] = orig_error_dict_list\n",
    "    df[\"orig_pos_err_count\"] = orig_pos_err_count_list\n",
    "\n",
    "    df[\"orig_sub\"] = orig_sub_count\n",
    "    df[\"orig_ins\"] = orig_ins_count\n",
    "    df[\"orig_del\"] = orig_del_count\n",
    "    df[\"orig_pos_tags\"] = orig_pos_tags\n",
    "\n",
    "    df[\"mod_ref\"] = mod_error_ref  # text/sentence form with uppercase errors\n",
    "    df[\"mod_asr\"] = mod_error_asr  # text/sentence form with uppercase errors\n",
    "    # df[\"mod_ref_error_words\"] = mod_ref_error_words\n",
    "    # df[\"mod_asr_error_words\"] = mod_asr_error_words\n",
    "    df[\"mod_error_dict\"] = mod_error_dict_list\n",
    "    df[\"mod_pos_err_count\"] = mod_pos_err_count_list\n",
    "\n",
    "    df[\"mod_sub\"] = mod_sub_count\n",
    "    df[\"mod_ins\"] = mod_ins_count\n",
    "    df[\"mod_del\"] = mod_del_count\n",
    "    df[\"mod_pos_tags\"] = mod_pos_tags\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the test transcriptions\n",
    "# path = \"./logs/trial35_masd_RundkastOnly_aulus7\"\n",
    "# mod_results = load_results_to_df(path, mod_loss=True)\n",
    "# path = \"./logs/trial_origloss_RundkastOnly_aulus7\"\n",
    "# orig_results = load_results_to_df(path, mod_loss=False)\n",
    "\n",
    "# merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# # higher asd score but lower wer when using modified loss\n",
    "# mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "# mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "# mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "# print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# # lower asd score but higher wer when using modified loss\n",
    "# mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "# mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "# mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "# print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# # merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "# df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "# df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('mod_pos_tags').reset_index(name='count')\n",
    "# print(\"mod loss pos errors:\", df_mod_pos_count[\"count\"].sum(axis=0))\n",
    "# print(df_mod_pos_count)\n",
    "\n",
    "# df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "# df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('orig_pos_tags').reset_index(name='count')\n",
    "# print(\"orig loss pos errors:\", df_orig_pos_count[\"count\"].sum(axis=0))\n",
    "# print(df_orig_pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results & POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD but lower WER - using mod loss: 7\n",
      "lower ASD but higher or equal WER - using mod loss: 222\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "mod_results = load_results_to_df(\"./logs/trial60_masd_6ep_allDataSmall_titan1\", mod_loss=True)\n",
    "orig_results = load_results_to_df(\"./logs/trial_origloss_6ep_allDataSmall_aulus7\", mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd_df = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"mod_wer\"] < mod_higher_asd_df[\"orig_wer\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD but lower WER - using mod loss:\", len(mod_higher_asd_df))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd_df = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"mod_wer\"] >= mod_lower_asd_df[\"orig_wer\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD but higher or equal WER - using mod loss:\", len(mod_lower_asd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_mod = []\n",
    "# multi_orig = []\n",
    "\n",
    "# for row in tqdm(merged_results.itertuples(), total=merged_results.shape[0], desc=\"multi-ASD scoring\"):\n",
    "#     multi_orig.append(compute_asd_score_single_utt(metric_model_multi, metric_tokenizer_multi, row.ref_str, row.orig_asr, normalized=True))\n",
    "#     multi_mod.append(compute_asd_score_single_utt(metric_model_multi, metric_tokenizer_multi, row.ref_str, row.mod_asr, normalized=True))\n",
    "\n",
    "# merged_results[\"orig_multi-asd\"] = multi_orig\n",
    "# merged_results[\"mod_multi-asd\"] = multi_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS error count (NOT equal CER utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 382/382 [00:32<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances for POS analysis: 382\n",
      "mod loss pos errors: 2817\n",
      "orig loss pos errors: 2885\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "\n",
    "not_equal_cer = merged_results[merged_results[\"mod_cer\"] != merged_results[\"orig_cer\"]]\n",
    "df_pos = merge_df_wer_pos_tags(not_equal_cer.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "print(\"number of utterances for POS analysis:\", len(df_pos))\n",
    "\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "# print(\"number of utterances for POS analysis:\", len(df_pos))\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('pos_tags').reset_index(name='Wav2Vec2+ASD')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"Wav2Vec2+ASD\"].sum(axis=0))\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('pos_tags').reset_index(name='Wav2Vec2+CTC')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"Wav2Vec2+CTC\"].sum(axis=0))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "merged_pos_count = pd.merge(df_orig_pos_count, df_mod_pos_count, on=[\"pos_tags\"])\n",
    "merged_pos_count = pd.melt(merged_pos_count, id_vars=[\"pos_tags\"], value_vars=[\"Wav2Vec2+CTC\", \"Wav2Vec2+ASD\"])\n",
    "merged_pos_count = merged_pos_count.rename(columns={\"variable\":\"model\", \"value\":\"count\"})\n",
    "# merged_pos_count = merged_pos_count.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax = sns.barplot(data=merged_pos_count, x=\"pos_tags\", y=\"count\", hue=\"model\", palette=\"dark\", alpha=0.7)\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.grid(axis='y')\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"POS Tags\")\n",
    "# sns.move_legend(ax, bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>orig_cer</th>\n",
       "      <th>orig_wer</th>\n",
       "      <th>orig_asd</th>\n",
       "      <th>mod_cer</th>\n",
       "      <th>mod_wer</th>\n",
       "      <th>mod_asd</th>\n",
       "      <th>orig_ref</th>\n",
       "      <th>orig_asr</th>\n",
       "      <th>orig_error_dict</th>\n",
       "      <th>orig_pos_err_count</th>\n",
       "      <th>orig_sub</th>\n",
       "      <th>orig_ins</th>\n",
       "      <th>orig_del</th>\n",
       "      <th>orig_pos_tags</th>\n",
       "      <th>mod_ref</th>\n",
       "      <th>mod_asr</th>\n",
       "      <th>mod_error_dict</th>\n",
       "      <th>mod_pos_err_count</th>\n",
       "      <th>mod_sub</th>\n",
       "      <th>mod_ins</th>\n",
       "      <th>mod_del</th>\n",
       "      <th>mod_pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_100</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.138008</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.129134</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['an'], 'ref_po...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PROPN, PROPN, PROPN]</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['an', 'cathrin...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PROPN, PROPN, PROPN, PROPN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_102</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.212464</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.084633</td>\n",
       "      <td>kjent at viruset kan overleve flere timer uten...</td>\n",
       "      <td>at viruset kan overleve flere timer utenfor me...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['kjent'], 'ref_pos...</td>\n",
       "      <td>[3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, DET, NOUN, NOUN, NOUN]</td>\n",
       "      <td>kjent at viruset kan overleve flere timer uten...</td>\n",
       "      <td>at viruset kan overleve flere timer utenfor me...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['kjent'], 'ref_pos...</td>\n",
       "      <td>[2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, PROPN, NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_103</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.091840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, PROPN, PROPN]</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_106</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.196988</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076057</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['märtha', 'lou...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, PROPN, X, X, PROPN, X, VERB]</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['märtha'], 're...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_108</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.168720</td>\n",
       "      <td>gang slottets informasjonssjef wenche rask for...</td>\n",
       "      <td>gang slottets informasjonssjef venke rask forr...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['wenche'], 're...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, VERB, PRON]</td>\n",
       "      <td>gang slottets informasjonssjef wenche rask for...</td>\n",
       "      <td>gang slottets informasjonssjef wenke rask for ...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['wenche'], 're...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, PROPN, DET, ADP, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.139344</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.268045</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.249815</td>\n",
       "      <td>ikke liker det de står for vil de altså tvinge...</td>\n",
       "      <td>ikke liker det de står for så vil også tvinge ...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[ADV, PRON, ADV, ADV, ADP, AUX, PRON, VERB]</td>\n",
       "      <td>ikke liker det de står for vil de altså tvinge...</td>\n",
       "      <td>liker det de står for så vil det også tvinge f...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['ikke'], 'ref_pos'...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[PART, ADV, PRON, ADV, PRON, ADV, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.127637</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.112867</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelser a...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelse av...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['masseoppsigel...</td>\n",
       "      <td>[8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOU...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelser a...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelse av...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['masseoppsigel...</td>\n",
       "      <td>[6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.253129</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.151328</td>\n",
       "      <td>fremskrittspartiet nå vil fjerne hele formuess...</td>\n",
       "      <td>partiene vil fjerne hele formunsskatten som jo...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['fremskrittspa...</td>\n",
       "      <td>[4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, ADV, NOUN, NOUN, PRON]</td>\n",
       "      <td>fremskrittspartiet nå vil fjerne hele formuess...</td>\n",
       "      <td>partiene vil fjerne hele formuesskatten som jo...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['fremskrittspa...</td>\n",
       "      <td>[2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, ADV, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.079557</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.192340</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['å'], 'ref_pos...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PART, VERB, VERB]</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['å'], 'ref_pos...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[PART, VERB, VERB, VERB, VERB, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.817789</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.853918</td>\n",
       "      <td>økonomisk men derfor står de også helt aleine ...</td>\n",
       "      <td>økonomisk men derfor så står det også helt ale...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...</td>\n",
       "      <td>økonomisk men derfor står de også helt aleine ...</td>\n",
       "      <td>økonomisk men derfor så står det også helt ale...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            segment_id  orig_cer  orig_wer  \\\n",
       "0                         N2_030505_NRK_D12_NO_cut_100  0.031496      0.10   \n",
       "1                         N2_030505_NRK_D12_NO_cut_102  0.084746      0.20   \n",
       "2                         N2_030505_NRK_D12_NO_cut_103  0.015625      0.10   \n",
       "3                         N2_030505_NRK_D12_NO_cut_106  0.062500      0.20   \n",
       "4                         N2_030505_NRK_D12_NO_cut_108  0.048276      0.10   \n",
       "..                                                 ...       ...       ...   \n",
       "377  2021H264-fullStorting0510Stortinget-20210510-1...  0.139344      0.30   \n",
       "378  2021H264-fullStorting0510Stortinget-20210510-1...  0.058394      0.25   \n",
       "379  2021H264-fullStorting0510Stortinget-20210510-1...  0.138686      0.20   \n",
       "380  2021H264-fullStorting0510Stortinget-20210510-1...  0.026786      0.10   \n",
       "381  2021H264-fullStorting0510Stortinget-20210510-1...  0.796748      0.85   \n",
       "\n",
       "     orig_asd   mod_cer  mod_wer   mod_asd  \\\n",
       "0    0.138008  0.023622     0.15  0.129134   \n",
       "1    0.212464  0.076271     0.15  0.084633   \n",
       "2    0.091840  0.000000     0.00  0.000000   \n",
       "3    0.196988  0.008929     0.05  0.076057   \n",
       "4    0.143233  0.068966     0.15  0.168720   \n",
       "..        ...       ...      ...       ...   \n",
       "377  0.268045  0.147541     0.30  0.249815   \n",
       "378  0.127637  0.051095     0.20  0.112867   \n",
       "379  0.253129  0.131387     0.15  0.151328   \n",
       "380  0.079557  0.053571     0.20  0.192340   \n",
       "381  0.817789  0.804878     0.85  0.853918   \n",
       "\n",
       "                                              orig_ref  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    kjent at viruset kan overleve flere timer uten...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenche rask for...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for vil de altså tvinge...   \n",
       "378  flertall så ville det blitt masseoppsigelser a...   \n",
       "379  fremskrittspartiet nå vil fjerne hele formuess...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor står de også helt aleine ...   \n",
       "\n",
       "                                              orig_asr  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    at viruset kan overleve flere timer utenfor me...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef venke rask forr...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for så vil også tvinge ...   \n",
       "378  flertall så ville det blitt masseoppsigelse av...   \n",
       "379  partiene vil fjerne hele formunsskatten som jo...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor så står det også helt ale...   \n",
       "\n",
       "                                       orig_error_dict  \\\n",
       "0    [{'type': 'substitute', 'ref': ['an'], 'ref_po...   \n",
       "1    [{'type': 'delete', 'ref': ['kjent'], 'ref_pos...   \n",
       "2    [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "3    [{'type': 'substitute', 'ref': ['märtha', 'lou...   \n",
       "4    [{'type': 'substitute', 'ref': ['wenche'], 're...   \n",
       "..                                                 ...   \n",
       "377  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "378  [{'type': 'substitute', 'ref': ['masseoppsigel...   \n",
       "379  [{'type': 'substitute', 'ref': ['fremskrittspa...   \n",
       "380  [{'type': 'substitute', 'ref': ['å'], 'ref_pos...   \n",
       "381  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "\n",
       "                                    orig_pos_err_count  orig_sub  orig_ins  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         2         0   \n",
       "1    [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...         1         2   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1         1   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         3         0   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1         0   \n",
       "..                                                 ...       ...       ...   \n",
       "377  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         2         1   \n",
       "378  [8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         4         0   \n",
       "379  [4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         2         1   \n",
       "380  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...         1         0   \n",
       "381  [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...         2        15   \n",
       "\n",
       "     orig_del                                      orig_pos_tags  \\\n",
       "0           0                         [ADV, PROPN, PROPN, PROPN]   \n",
       "1           1                       [ADJ, DET, NOUN, NOUN, NOUN]   \n",
       "2           0                              [PROPN, PROPN, PROPN]   \n",
       "3           1               [PROPN, PROPN, X, X, PROPN, X, VERB]   \n",
       "4           1                                [PROPN, VERB, PRON]   \n",
       "..        ...                                                ...   \n",
       "377         3        [ADV, PRON, ADV, ADV, ADP, AUX, PRON, VERB]   \n",
       "378         1  [NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOU...   \n",
       "379         1                [NOUN, NOUN, ADV, NOUN, NOUN, PRON]   \n",
       "380         1                                 [PART, VERB, VERB]   \n",
       "381         0  [ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...   \n",
       "\n",
       "                                               mod_ref  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    kjent at viruset kan overleve flere timer uten...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenche rask for...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for vil de altså tvinge...   \n",
       "378  flertall så ville det blitt masseoppsigelser a...   \n",
       "379  fremskrittspartiet nå vil fjerne hele formuess...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor står de også helt aleine ...   \n",
       "\n",
       "                                               mod_asr  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    at viruset kan overleve flere timer utenfor me...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenke rask for ...   \n",
       "..                                                 ...   \n",
       "377  liker det de står for så vil det også tvinge f...   \n",
       "378  flertall så ville det blitt masseoppsigelse av...   \n",
       "379  partiene vil fjerne hele formuesskatten som jo...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor så står det også helt ale...   \n",
       "\n",
       "                                        mod_error_dict  \\\n",
       "0    [{'type': 'substitute', 'ref': ['an', 'cathrin...   \n",
       "1    [{'type': 'delete', 'ref': ['kjent'], 'ref_pos...   \n",
       "2                                                   []   \n",
       "3    [{'type': 'substitute', 'ref': ['märtha'], 're...   \n",
       "4    [{'type': 'substitute', 'ref': ['wenche'], 're...   \n",
       "..                                                 ...   \n",
       "377  [{'type': 'delete', 'ref': ['ikke'], 'ref_pos'...   \n",
       "378  [{'type': 'substitute', 'ref': ['masseoppsigel...   \n",
       "379  [{'type': 'substitute', 'ref': ['fremskrittspa...   \n",
       "380  [{'type': 'substitute', 'ref': ['å'], 'ref_pos...   \n",
       "381  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "\n",
       "                                     mod_pos_err_count  mod_sub  mod_ins  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        3        0   \n",
       "1    [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1        1   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0        0   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1        0   \n",
       "4    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...        2        0   \n",
       "..                                                 ...      ...      ...   \n",
       "377  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        2        1   \n",
       "378  [6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        3        0   \n",
       "379  [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1        1   \n",
       "380  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...        2        1   \n",
       "381  [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...        2       15   \n",
       "\n",
       "     mod_del                                       mod_pos_tags  \n",
       "0          0           [ADV, PROPN, PROPN, PROPN, PROPN, PROPN]  \n",
       "1          1                           [ADJ, PROPN, NOUN, NOUN]  \n",
       "2          0                                                 []  \n",
       "3          0                                     [PROPN, PROPN]  \n",
       "4          1                     [PROPN, PROPN, DET, ADP, PRON]  \n",
       "..       ...                                                ...  \n",
       "377        3       [PART, ADV, PRON, ADV, PRON, ADV, ADP, VERB]  \n",
       "378        1          [NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, ADP]  \n",
       "379        1                            [NOUN, NOUN, ADV, PRON]  \n",
       "380        1                [PART, VERB, VERB, VERB, VERB, ADP]  \n",
       "381        0  [ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...  \n",
       "\n",
       "[382 rows x 23 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- among the nouns that were wrong, look at the lemmas\n",
    "- look at the adpositions and pronouns as well\n",
    "- is ASD making improvements, where are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD but lower WER - using mod loss: 6\n",
      "lower ASD but higher or equal WER - using mod loss: 47\n"
     ]
    }
   ],
   "source": [
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd_df = df_pos[df_pos[\"orig_asd\"] < df_pos[\"mod_asd\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"mod_wer\"] < mod_higher_asd_df[\"orig_wer\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD but lower WER - using mod loss:\", len(mod_higher_asd_df))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd_df = df_pos[df_pos[\"mod_asd\"] < df_pos[\"orig_asd\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"mod_wer\"] >= mod_lower_asd_df[\"orig_wer\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD but higher or equal WER - using mod loss:\", len(mod_lower_asd_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression between POS errors & ASD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- instead of a correlation analysis, perform regression analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 829/829 [01:08<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "linreg_df = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGRESSION MODEL TO PREDICT ASD:\n",
      "Results on TRAIN set\n",
      "MSE: 0.0065\n",
      "MAE: 0.0573\n",
      "R^2: 0.9327\n",
      "Results on TEST set\n",
      "MSE: 0.0048\n",
      "MAE: 0.0522\n",
      "R^2: 0.7026\n"
     ]
    }
   ],
   "source": [
    "error_data = linreg_df.mod_pos_err_count.tolist()\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "X = pd.DataFrame(error_data, columns=label_list_pos)\n",
    "# X = X.drop(labels=['PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'X', 'ADV', 'INTJ', 'AUX'], axis=\"columns\")\n",
    "Y = linreg_df.mod_asd\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print('REGRESSION MODEL TO PREDICT ASD:')\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "print('Results on TRAIN set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_train, Y_pred_train))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "print('R^2: %.4f' % r2_score(Y_train, Y_pred_train))\n",
    "\n",
    "Y_pred_test = model.predict(X_test)\n",
    "print('Results on TEST set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_test, Y_pred_test))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_test, Y_pred_test))\n",
    "print('R^2: %.4f' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGRESSION MODEL TO PREDICT WER:\n",
      "Results on TRAIN set\n",
      "MSE: 0.0063\n",
      "MAE: 0.0523\n",
      "R^2: 0.9606\n",
      "Results on TEST set\n",
      "MSE: 0.0056\n",
      "MAE: 0.0544\n",
      "R^2: 0.7010\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(error_data, columns=label_list_pos)\n",
    "# X = X.drop(labels=['PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'X', 'ADV', 'INTJ', 'AUX'], axis=\"columns\")\n",
    "Y = linreg_df.mod_wer\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print('REGRESSION MODEL TO PREDICT WER:')\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "print('Results on TRAIN set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_train, Y_pred_train))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "print('R^2: %.4f' % r2_score(Y_train, Y_pred_train))\n",
    "\n",
    "Y_pred_test = model.predict(X_test)\n",
    "print('Results on TEST set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_test, Y_pred_test))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_test, Y_pred_test))\n",
    "print('R^2: %.4f' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NOUN  PROPN  PRON  VERB\n",
       "0     0.0    0.0   0.0   0.0\n",
       "1     0.0    2.0   0.0   0.0\n",
       "2     2.0    0.0   0.0   0.0\n",
       "3     0.0    5.0   0.0   0.0\n",
       "4     2.0    0.0   0.0   0.0\n",
       "..    ...    ...   ...   ...\n",
       "824   2.0    0.0   0.0   0.0\n",
       "825   0.0    0.0   0.0   0.0\n",
       "826   2.0    0.0   1.0   0.0\n",
       "827   0.0    0.0   0.0   3.0\n",
       "828   3.0    4.0   2.0   0.0\n",
       "\n",
       "[829 rows x 4 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "# data_source = df_pos.copy(deep=True)\n",
    "# data_source = data_source[data_source[\"mod_wer\"] <= 0.2]\n",
    "# data_source = data_source.reset_index(drop=True)\n",
    "\n",
    "# data_matrix = np.zeros((len(label_list_pos),len(data_source)), dtype=int)\n",
    "# for row_num, row in data_source.iterrows():\n",
    "#     for idx, count in enumerate(row.mod_pos_err_count):\n",
    "#         data_matrix[idx][row_num] += count\n",
    "\n",
    "# asd_values = data_source.mod_asd\n",
    "# wer_values = data_source.mod_wer\n",
    "\n",
    "# pos_asd_pearson_stat = []\n",
    "# pos_wer_pearson_stat = []\n",
    "# for row_num, row in enumerate(data_matrix):\n",
    "#     pos_item = label_list_pos[row_num]\n",
    "#     pos_error = row\n",
    "#     pos_asd_pearson_stat.append(stats.pearsonr(pos_error, asd_values)[0])\n",
    "#     pos_wer_pearson_stat.append(stats.pearsonr(pos_error, wer_values)[0])\n",
    "\n",
    "# pos_corr_df = pd.DataFrame(data=zip(label_list_pos, pos_asd_pearson_stat, pos_wer_pearson_stat), columns=[\"POS\", \"ASD_Pearson_r\", \"WER_Pearson_r\"])\n",
    "# pos_corr_ASDsorted_df = pos_corr_df.sort_values(by=\"ASD_Pearson_r\", ascending=False)\n",
    "# pos_corr_ASDsorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "import subprocess\n",
    "\n",
    "finetuned_bert_ner = \"../../nlp_models/ner_pos/finetuned_bert_ner_v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_bert_ner)\n",
    "model = AutoModelForTokenClassification.from_pretrained(finetuned_bert_ner)\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclite_path = \"../../aulus6_janinelr/kaldi/tools/sctk/bin/sclite\"\n",
    "sclite_output_dir = \"./sclite_outputs/Interspeech2025\"\n",
    "\n",
    "label_list_ner = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-GPE_LOC', 'I-GPE_LOC', 'B-PROD', 'I-PROD', 'B-LOC', 'I-LOC', 'B-GPE_ORG', 'I-GPE_ORG', 'B-DRV', 'I-DRV', 'B-EVT', 'I-EVT', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_tags(df, ner_pipeline):\n",
    "    ref_tags = []\n",
    "    mod_asr_tags = []\n",
    "    orig_asr_tags = []\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"NLP Tagging - ORIG ASR\"):\n",
    "        if (len(ner_pipeline(row.ref_str)) == 0) & (len(ner_pipeline(row.orig_asr)) == 0):\n",
    "            ref_tags.append(None)\n",
    "            mod_asr_tags.append(None)\n",
    "        else:\n",
    "            ref_tags.append(ner_pipeline(row.ref_str))\n",
    "            mod_asr_tags.append(ner_pipeline(row.orig_asr))\n",
    "    df[\"ref_ner_tags\"] = ref_tags\n",
    "    df[\"orig_asr_ner_tags\"] = mod_asr_tags\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"NLP Tagging - MOD ASR\"):\n",
    "        if (len(ner_pipeline(row.ref_str)) == 0) & (len(ner_pipeline(row.mod_asr)) == 0):\n",
    "            ref_tags.append(None)\n",
    "            orig_asr_tags.append(None)\n",
    "        else:\n",
    "            ref_tags.append(ner_pipeline(row.ref_str))\n",
    "            orig_asr_tags.append(ner_pipeline(row.mod_asr))\n",
    "    df[\"mod_asr_ner_tags\"] = orig_asr_tags\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# FUNCTION FOR NER ALIGNMENT\n",
    "def get_ner_alignment(input_df, sclite_output_dir, sclite_path, label_list, mod_asr=True):\n",
    "    df = input_df.copy(deep=True)\n",
    "    # combining words and tags into string of text\n",
    "    ref_tokenizedtext_list = []\n",
    "    asr_tokenizedtext_list = []\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Extracting NER tags\"):\n",
    "        ref_labels = []\n",
    "        asr_labels = []\n",
    "        for item in row.ref_ner_tags:\n",
    "            word_list = item['word'].split()\n",
    "            for word in word_list:\n",
    "                ref_labels.append(word + \"_\" + label_list[int(item['entity_group'].replace(\"LABEL_\", \"\"))])\n",
    "        if mod_asr == True:\n",
    "            for item in row.mod_asr_ner_tags:\n",
    "                word_list = item['word'].split()\n",
    "                for word in word_list:\n",
    "                    asr_labels.append(word + \"_\" + label_list[int(item['entity_group'].replace(\"LABEL_\", \"\"))])\n",
    "        else:\n",
    "            for item in row.orig_asr_ner_tags:\n",
    "                word_list = item['word'].split()\n",
    "                for word in word_list:\n",
    "                    asr_labels.append(word + \"_\" + label_list[int(item['entity_group'].replace(\"LABEL_\", \"\"))])\n",
    "\n",
    "        ref_tokenizedtext_list.append(' '.join([str(elem) for elem in ref_labels]))\n",
    "        asr_tokenizedtext_list.append(' '.join([str(elem) for elem in asr_labels]))\n",
    "\n",
    "    # using sclite to find the alignment\n",
    "    with open(os.path.join(sclite_output_dir, \"references.txt\"), \"w\") as f:\n",
    "        for idx, utterance in enumerate(ref_tokenizedtext_list):\n",
    "            f.write(utterance + \" (utt_\" + str(idx) + \")\" + \"\\n\")\n",
    "    with open(os.path.join(sclite_output_dir, \"hypothesis.txt\"), \"w\") as f:\n",
    "        for idx, utterance in enumerate(asr_tokenizedtext_list):\n",
    "            f.write(utterance + \" (utt_\" + str(idx) + \")\" + \"\\n\")\n",
    "    sclite_path = sclite_path\n",
    "    with open(os.path.join(sclite_output_dir, 'temp_output.txt'), 'a') as outfile:\n",
    "        subprocess.run([\n",
    "            sclite_path,\n",
    "            '-r', os.path.join(sclite_output_dir, \"references.txt\"),\n",
    "            '-h', os.path.join(sclite_output_dir, \"hypothesis.txt\"),\n",
    "            '-i', 'rm',\n",
    "            '-o', 'all'\n",
    "        ], stdout=outfile)\n",
    "\n",
    "    # placing alignment results into the dataframe\n",
    "    with open(os.path.join(sclite_output_dir, \"hypothesis.txt.pra\")) as f:\n",
    "        lines = f.readlines()\n",
    "    ref_align_list = []\n",
    "    asr_align_list = []\n",
    "    utt_ids_list = []\n",
    "    eval_align_list = []\n",
    "    for line in lines:\n",
    "        clean_line = re.sub(' +', ' ', line).strip()\n",
    "        text_list = clean_line.split(\" \")\n",
    "        # text_list = [item.lower() for item in text_list]\n",
    "        if text_list[0] == \"id:\":\n",
    "            utt_ids_list.append(text_list[1])\n",
    "        elif text_list[0] == \"REF:\":\n",
    "            ref_align_list.append(text_list[1:])\n",
    "        elif text_list[0] == \"HYP:\":\n",
    "            asr_align_list.append(text_list[1:])\n",
    "    df[\"ref_token\"] = ref_align_list\n",
    "    df[\"asr_token\"] = asr_align_list\n",
    "    eval_align_list = []\n",
    "    for row in df.itertuples():\n",
    "        eval_token_list = []\n",
    "        ref_token_list = row.ref_token\n",
    "        asr_token_list = row.asr_token\n",
    "        for idx, ref_token in enumerate(ref_token_list):\n",
    "            if ref_token == asr_token_list[idx]:\n",
    "                eval_token_list.append(\"m\")\n",
    "            elif \"*\" in ref_token:\n",
    "                eval_token_list.append(\"I\")\n",
    "            elif \"*\" in asr_token_list[idx]:\n",
    "                eval_token_list.append(\"D\")\n",
    "            elif ref_token != asr_token_list[idx]:\n",
    "                eval_token_list.append(\"S\")\n",
    "        eval_align_list.append(eval_token_list)\n",
    "    df[\"eval_token\"] = eval_align_list\n",
    "\n",
    "    # cleanup dataframe by removing items with no NER tags at all\n",
    "    idx_to_drop = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Remove no NER tags\"):\n",
    "        if (len(row.ref_ner_tags) == 1) & (row.ref_ner_tags[0][\"entity_group\"] == \"LABEL_0\"):\n",
    "            idx_to_drop.append(idx)\n",
    "    df.drop(idx_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # getting aligned ASR and REF labels (individually, word-wise)\n",
    "    ref_labels = []\n",
    "    asr_labels = []\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Aligning REF & ASR labels\"):\n",
    "        ref_tags = []\n",
    "        for item in row.ref_token:\n",
    "            tokens = item.split(\"_\")\n",
    "            if len(tokens) == 1:\n",
    "                ref_tags.append(\"O\")\n",
    "            elif len(tokens) == 2:\n",
    "                ref_tags.append(tokens[1].upper())\n",
    "            elif len(tokens) == 3:\n",
    "                ref_tags.append(tokens[1].upper() + \"_\" + tokens[2].upper())\n",
    "        ref_labels.append(ref_tags)\n",
    "        asr_tags = []\n",
    "        for item in row.asr_token:\n",
    "            tokens = item.split(\"_\")\n",
    "            if len(tokens) == 1:\n",
    "                asr_tags.append(\"O\")\n",
    "            elif len(tokens) == 2:\n",
    "                asr_tags.append(tokens[1].upper())\n",
    "            elif len(tokens) == 3:\n",
    "                asr_tags.append(tokens[1].upper() + \"_\" + tokens[2].upper())\n",
    "        asr_labels.append(asr_tags)\n",
    "    df[\"ref_labels\"] = ref_labels\n",
    "    df[\"asr_labels\"] = asr_labels\n",
    "\n",
    "    print(\"get alignment - DONE\")\n",
    "\n",
    "    return df, idx_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLP Tagging - ORIG ASR:   0%|          | 0/829 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "NLP Tagging - ORIG ASR: 100%|██████████| 829/829 [01:35<00:00,  8.71it/s]\n",
      "NLP Tagging - MOD ASR: 100%|██████████| 829/829 [01:36<00:00,  8.63it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_df = merged_results.copy(deep=True)\n",
    "ner_df = get_ner_tags(ner_df, nlp_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting NER tags: 100%|██████████| 829/829 [00:00<00:00, 25159.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove no NER tags: 100%|██████████| 829/829 [00:00<00:00, 18633.16it/s]\n",
      "Aligning REF & ASR labels: 100%|██████████| 316/316 [00:00<00:00, 73073.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get alignment - DONE\n",
      "TUL ASR baseline - NER\n",
      "F1 score micro: 0.7222787385554424\n",
      "F1 score macro: 0.5583556296988624\n",
      "WER: 0.19281656309066958\n",
      "ASD: 0.1714380419865155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mod_ner_df, idx_to_drop = get_ner_alignment(ner_df, sclite_output_dir, sclite_path, label_list_ner, mod_asr=True)\n",
    "\n",
    "mod_ner_df[\"f1_score\"] = mod_ner_df[[\"ref_labels\", \"asr_labels\"]].apply(lambda row: f1_score([row.ref_labels], [row.asr_labels], average=\"micro\"), axis=1)\n",
    "\n",
    "print(\"TUL ASR baseline - NER\")\n",
    "y_true = list(mod_ner_df[\"ref_labels\"])\n",
    "y_pred = list(mod_ner_df[\"asr_labels\"])\n",
    "print(\"F1 score micro:\", f1_score(y_true, y_pred, average=\"micro\"))\n",
    "print(\"F1 score macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", mod_ner_df[\"mod_wer\"].mean())\n",
    "print(\"ASD:\", mod_ner_df[\"mod_asd\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting NER tags: 100%|██████████| 829/829 [00:00<00:00, 25622.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove no NER tags: 100%|██████████| 829/829 [00:00<00:00, 19057.81it/s]\n",
      "Aligning REF & ASR labels: 100%|██████████| 316/316 [00:00<00:00, 70462.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get alignment - DONE\n",
      "TUL ASR baseline - NER\n",
      "F1 score micro: 0.718266253869969\n",
      "F1 score macro: 0.5058669152636408\n",
      "WER: 0.19675650810470247\n",
      "ASD: 0.1766881681958569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_ner_df, ___ = get_ner_alignment(ner_df, sclite_output_dir, sclite_path, label_list_ner, mod_asr=False)\n",
    "\n",
    "orig_ner_df[\"f1_score\"] = orig_ner_df[[\"ref_labels\", \"asr_labels\"]].apply(lambda row: f1_score([row.ref_labels], [row.asr_labels], average=\"micro\"), axis=1)\n",
    "\n",
    "print(\"TUL ASR baseline - NER\")\n",
    "y_true = list(orig_ner_df[\"ref_labels\"])\n",
    "y_pred = list(orig_ner_df[\"asr_labels\"])\n",
    "print(\"F1 score micro:\", f1_score(y_true, y_pred, average=\"micro\"))\n",
    "print(\"F1 score macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", orig_ner_df[\"orig_wer\"].mean())\n",
    "print(\"ASD:\", orig_ner_df[\"orig_asd\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-polarity Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bert_sa = \"../../nlp_models/sentiment_analysis/finetuned_model\"\n",
    "sa_model = AutoModelForSequenceClassification.from_pretrained(finetuned_bert_sa, num_labels=3)\n",
    "sa_tokenizer = AutoTokenizer.from_pretrained(finetuned_bert_sa, num_labels=3)\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR SENTIMENT ANALYSIS\n",
    "def get_pred_label(x):\n",
    "    if x == 0:\n",
    "        return ['Neutral']\n",
    "    elif x == 1:\n",
    "        return ['Positive']\n",
    "    else:\n",
    "        return ['Negative']\n",
    "\n",
    "def get_sentiment(df, model, tokenizer):\n",
    "    mod_asr_sentiment = []\n",
    "    orig_asr_sentiment = []\n",
    "    ref_sentiment = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Sentiment tagging\"):\n",
    "        tokenized_ref = tokenizer(row.ref_str, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        tokenized_mod_asr = tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        tokenized_orig_asr = tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            model_output_ref = model(**tokenized_ref, output_hidden_states=False)\n",
    "            model_output_mod_asr = model(**tokenized_mod_asr, output_hidden_states=False)\n",
    "            model_output_orig_asr = model(**tokenized_orig_asr, output_hidden_states=False)\n",
    "        pred_ref = model_output_ref['logits'].numpy()\n",
    "        ref_sentiment.append(np.argmax(pred_ref, axis=1))\n",
    "        pred_mod_asr = model_output_mod_asr['logits'].numpy()\n",
    "        pred_orig_asr = model_output_orig_asr['logits'].numpy()\n",
    "        mod_asr_sentiment.append(np.argmax(pred_mod_asr, axis=1))\n",
    "        orig_asr_sentiment.append(np.argmax(pred_orig_asr, axis=1))\n",
    "\n",
    "    df['ref_sentiment'] = ref_sentiment\n",
    "    df['mod_asr_sentiment'] = mod_asr_sentiment\n",
    "    df['orig_asr_sentiment'] = orig_asr_sentiment\n",
    "\n",
    "    df['ref_sentiment'] = df['ref_sentiment'].map(get_pred_label)\n",
    "    df['mod_asr_sentiment'] = df['mod_asr_sentiment'].map(get_pred_label)\n",
    "    df['orig_asr_sentiment'] = df['orig_asr_sentiment'].map(get_pred_label)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment tagging: 100%|██████████| 316/316 [00:26<00:00, 11.83it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Neutral seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Positive seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Negative seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis - Orig ASR\n",
      "F1 score: 0.9028613898179115\n",
      "WER: 0.19675650810470247\n",
      "ASD: 0.1766881681958569\n",
      "Sentiment Analysis - MOD ASR\n",
      "F1 score: 0.9225205174572263\n",
      "WER: 0.19281656309066958\n",
      "ASD: 0.1714380419865155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Neutral seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Positive seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Negative seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Neutral seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Positive seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Negative seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "sa_df = merged_results.copy(deep=True)\n",
    "sa_df = sa_df.drop(idx_to_drop)\n",
    "sa_df = sa_df.reset_index(drop=True)\n",
    "\n",
    "sa_df = get_sentiment(sa_df, sa_model, sa_tokenizer)\n",
    "\n",
    "sa_df[\"f1_score_mod\"] = sa_df[[\"ref_sentiment\", \"mod_asr_sentiment\"]].apply(lambda row: f1_score([row.ref_sentiment], [row.mod_asr_sentiment], average=\"macro\"), axis=1)\n",
    "sa_df[\"f1_score_orig\"] = sa_df[[\"ref_sentiment\", \"orig_asr_sentiment\"]].apply(lambda row: f1_score([row.ref_sentiment], [row.orig_asr_sentiment], average=\"macro\"), axis=1)\n",
    "\n",
    "print(\"Sentiment Analysis - Orig ASR\")\n",
    "y_true = sa_df[\"ref_sentiment\"]\n",
    "y_pred = sa_df[\"orig_asr_sentiment\"]\n",
    "# print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", sa_df[\"orig_wer\"].mean())\n",
    "print(\"ASD:\", sa_df[\"orig_asd\"].mean())\n",
    "\n",
    "print(\"Sentiment Analysis - MOD ASR\")\n",
    "y_true = sa_df[\"ref_sentiment\"]\n",
    "y_pred = sa_df[\"mod_asr_sentiment\"]\n",
    "# print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", sa_df[\"mod_wer\"].mean())\n",
    "print(\"ASD:\", sa_df[\"mod_asd\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the errors made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher asd: 180\n",
      "lower asd: 202\n"
     ]
    }
   ],
   "source": [
    "higher_asd_df = df_pos[df_pos[\"orig_asd\"] < df_pos[\"mod_asd\"]]\n",
    "lower_asd_df = df_pos[df_pos[\"mod_asd\"] < df_pos[\"orig_asd\"]]\n",
    "\n",
    "print(\"higher asd:\", len(higher_asd_df))\n",
    "print(\"lower asd:\", len(lower_asd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_556170/2773266186.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lower_asd_df[\"asd_score_diff\"] = lower_asd_df[\"orig_asd\"] - lower_asd_df[\"mod_asd\"]\n"
     ]
    }
   ],
   "source": [
    "lower_asd_df[\"asd_score_diff\"] = lower_asd_df[\"orig_asd\"] - lower_asd_df[\"mod_asd\"]\n",
    "lower_asd_df = lower_asd_df.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_556170/3867863823.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  higher_asd_df[\"asd_score_diff\"] = higher_asd_df[\"mod_asd\"] - higher_asd_df[\"orig_asd\"]\n"
     ]
    }
   ],
   "source": [
    "higher_asd_df[\"asd_score_diff\"] = higher_asd_df[\"mod_asd\"] - higher_asd_df[\"orig_asd\"]\n",
    "higher_asd_df = higher_asd_df.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an example with errors to demonstrate errors that we want to analyze: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when nouns are incorrect, we expect the asd score to be higher\n",
    "# fine-tuning with ASD should reduce these types of errors\n",
    "\n",
    "noun_errors_df = lower_asd_df.copy(deep=True)\n",
    "\n",
    "orig_noun_errors = []\n",
    "mod_noun_errors = []\n",
    "for row in noun_errors_df.itertuples():\n",
    "    orig_errors = []\n",
    "    mod_errors = []\n",
    "    for error_dict in row.orig_error_dict:\n",
    "        if \"NOUN\" in error_dict[\"ref_pos\"]:\n",
    "            orig_errors.append((error_dict[\"ref\"], error_dict[\"hyp\"]))\n",
    "    for error_dict in row.mod_error_dict:\n",
    "        if \"NOUN\" in error_dict[\"ref_pos\"]:\n",
    "            mod_errors.append((error_dict[\"ref\"], error_dict[\"hyp\"]))\n",
    "    orig_noun_errors.append(orig_errors)\n",
    "    mod_noun_errors.append(mod_errors)\n",
    "\n",
    "noun_errors_df[\"orig_noun_errors\"] = orig_noun_errors\n",
    "noun_errors_df[\"mod_noun_errors\"] = mod_noun_errors\n",
    "\n",
    "# noun_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower ASD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Notes for paper:</u>\n",
    "- difference in the results will highly depend on the token vocabulary, look at the vocab of the tokenizer\n",
    "- systematic analysis - looking at the overall POS counts, word stems & lemmas (we expect ASD not to be too high if mistaken)\n",
    "- look if fixed phrases, multi-word expressions, or compound words were correctly transcribed\n",
    "- distribution \n",
    "- entropy of the logits produced by the model with original loss & with modified loss\n",
    "- example-based \n",
    "- changing the metric does not necessarily reduce the word error rate\n",
    "- it's okay to introduce the proposed approach and the difficulties faced during implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['utføre', 'ut'], 'hyp_pos': ['VERB', 'ADP']}\n",
      "{'type': 'substitute', 'ref': ['utfor', 'autovernet'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['o', 'vernet'], 'hyp_pos': ['NUM', 'NOUN']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.40\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utfor </td><td>utfor</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utføre</td><td>ut   </td><td>o    </td><td>vernet  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.022</td><td>0.024 </td><td>0.015  </td><td>0.016</td><td>0.054 </td><td>0.028</td><td>0.057</td><td>0.13</td><td>0.637 </td><td>0.664</td><td>0.706</td><td>0.532   </td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['utfor'], 'ref_pos': ['ADP'], 'hyp': ['utføre'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utfor </td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utføre</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.005</td><td>0.008 </td><td>0.004  </td><td>0.005</td><td>0.013 </td><td>0.015</td><td>0.024</td><td>0.113</td><td>0.541 </td><td>0.054</td><td>0.052   </td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['må'], 'ref_pos': ['AUX'], 'hyp': ['vil'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['ikke', 'herske', 'tvil', 'om'], 'ref_pos': ['PART', 'VERB', 'NOUN', 'ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mot  </td><td>at   </td><td>interesse</td><td>##grupper</td><td>får  </td><td>betale</td><td>reiser</td><td>og   </td><td>konferanser</td><td>han  </td><td>vil  </td><td>ha  </td><td>regler</td><td>mot  </td><td>dette</td><td>det  </td><td>må   </td><td>ikke </td><td>her  </td><td>##ske</td><td>tvil </td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mot  </td><td>at   </td><td>interesse</td><td>##grupper</td><td>får  </td><td>betale</td><td>reiser</td><td>og   </td><td>konferanser</td><td>han  </td><td>vil  </td><td>ha  </td><td>regler</td><td>mot  </td><td>dette</td><td>det  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.053</td><td>0.007</td><td>0.004</td><td>0.002    </td><td>0.002    </td><td>0.003</td><td>0.003 </td><td>0.002 </td><td>0.002</td><td>0.004      </td><td>0.009</td><td>0.021</td><td>0.01</td><td>0.006 </td><td>0.014</td><td>0.027</td><td>0.101</td><td>0.598</td><td>0.791</td><td>0.878</td><td>0.836</td><td>0.843</td><td>0.688</td><td>0.018</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mot</td><td>at</td><td>interesse</td><td>##grupper</td><td>får</td><td>betale</td><td>reiser</td><td>og</td><td>konferanser</td><td>han</td><td>vil</td><td>ha</td><td>regler</td><td>mot</td><td>dette</td><td>det</td><td>må</td><td>ikke</td><td>her</td><td>##ske</td><td>tvil</td><td>om</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mot</td><td>at</td><td>interesse</td><td>##grupper</td><td>får</td><td>betale</td><td>reiser</td><td>og</td><td>konferanser</td><td>han</td><td>vil</td><td>ha</td><td>regler</td><td>mot</td><td>dette</td><td>det</td><td>må</td><td>ikke</td><td>her</td><td>##ske</td><td>tvil</td><td>om</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0 </td><td>0        </td><td>0        </td><td>0  </td><td>0     </td><td>0     </td><td>0 </td><td>0          </td><td>0  </td><td>0  </td><td>0 </td><td>0     </td><td>0  </td><td>0    </td><td>0  </td><td>0 </td><td>0   </td><td>0  </td><td>0    </td><td>0   </td><td>0 </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['uavhengig'], 'ref_pos': ['ADJ'], 'hyp': ['uavhenget'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['venstre'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['venstresida'], 'ref_pos': ['NOUN'], 'hyp': ['side'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['skyhøye', 'engangsavgifter'], 'ref_pos': ['ADJ', 'NOUN'], 'hyp': ['skyhøge', 'eengangsavgifter'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>uavhengig</td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>##høye</td><td>engangs</td><td>engangs</td><td>##avgifter</td><td>##avgifter</td><td>og  </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uav      </td><td>##heng   </td><td>##et     </td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstre </td><td>side </td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høg </td><td>##e   </td><td>een    </td><td>##gang </td><td>##savgift </td><td>##er      </td><td>og  </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.012 </td><td>0.015  </td><td>0.014</td><td>0.021</td><td>0.035</td><td>0.345    </td><td>0.48     </td><td>0.557    </td><td>0.075</td><td>0.026</td><td>0.028</td><td>0.019</td><td>0.022</td><td>0.032 </td><td>0.051</td><td>0.361   </td><td>0.472</td><td>0.056</td><td>0.041</td><td>0.081</td><td>0.404 </td><td>0.449 </td><td>0.64   </td><td>0.611  </td><td>0.254     </td><td>0.457     </td><td>0.04</td><td>0.037</td><td>0.043</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['engangsavgifter'], 'ref_pos': ['NOUN'], 'hyp': ['eengangsavgifter'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.11\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>engangs</td><td>engangs</td><td>##avgifter</td><td>##avgifter</td><td>og   </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>een    </td><td>##gang </td><td>##savgift </td><td>##er      </td><td>og   </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.002 </td><td>0.002  </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001    </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002 </td><td>0.002</td><td>0.002   </td><td>0.001</td><td>0.002</td><td>0.003</td><td>0.009</td><td>0.015 </td><td>0.592  </td><td>0.588  </td><td>0.243     </td><td>0.433     </td><td>0.016</td><td>0.017</td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['åtte'], 'ref_pos': ['NUM'], 'hyp': ['åtteogsytti'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['og', 'sytti'], 'ref_pos': ['CCONJ', 'NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>og   </td><td>sy  </td><td>##tti</td><td>##tti</td><td>##tti</td><td>##tti</td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>i     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>åtte </td><td>åtte</td><td>åtte </td><td>##ogs</td><td>##ytt</td><td>##i  </td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>##aria</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.083</td><td>0.012</td><td>0.006</td><td>0.004</td><td>0.004</td><td>0.008</td><td>0.009 </td><td>0.024</td><td>0.097</td><td>0.722</td><td>0.53</td><td>0.455</td><td>0.687</td><td>0.598</td><td>0.584</td><td>0.019    </td><td>0.006</td><td>0.004</td><td>0.006</td><td>0.006</td><td>0.006 </td><td>0.01 </td><td>0.009</td><td>0.011</td><td>0.026 </td><td>0.667 </td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0   </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0     </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0    </td><td>0        </td><td>0 </td><td>0 </td><td>0  </td><td>0  </td><td>0     </td><td>0    </td><td>0   </td><td>0  </td><td>0     </td><td>0</td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['avhengig'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'substitute', 'ref': ['av', 'langsomhet', 'ormestad'], 'ref_pos': ['ADP', 'NOUN', 'PROPN'], 'hyp': ['som', 'heter', 'da'], 'hyp_pos': ['PRON', 'VERB', 'ADV']}\n",
      "{'type': 'substitute', 'ref': ['vel', 'av', 'den', 'oppfatning'], 'ref_pos': ['ADV', 'ADP', 'DET', 'NOUN'], 'hyp': ['i', 'har', 'denne', 'oppfatningen'], 'hyp_pos': ['ADP', 'VERB', 'DET', 'NOUN']}\n",
      "ASD-NorBERT ref len: 28, ASD=0.46\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e </td><td>av   </td><td>av      </td><td>av   </td><td>langsom</td><td>##het</td><td>or  </td><td>##mest</td><td>##ad </td><td>##ad </td><td>##ad</td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vel  </td><td>av   </td><td>av   </td><td>den  </td><td>oppfatning  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e </td><td>##e  </td><td>avhengig</td><td>som  </td><td>som    </td><td>som  </td><td>som </td><td>som   </td><td>som  </td><td>heter</td><td>da  </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vi   </td><td>i    </td><td>har  </td><td>denne</td><td>oppfatningen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.06 </td><td>0.045</td><td>0.025</td><td>0.027</td><td>0.019</td><td>0.021      </td><td>0.03</td><td>0.023</td><td>0.041</td><td>0.029  </td><td>0.046</td><td>0.07</td><td>0.573</td><td>0.696   </td><td>0.633</td><td>0.812  </td><td>0.707</td><td>0.77</td><td>0.807 </td><td>0.744</td><td>0.804</td><td>0.8 </td><td>0.086</td><td>0.046</td><td>0.043</td><td>0.139</td><td>0.083</td><td>0.729</td><td>0.634</td><td>0.599</td><td>0.446</td><td>0.229       </td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['av', 'langsomhet', 'ormestad'], 'ref_pos': ['ADP', 'NOUN', 'PROPN'], 'hyp': ['avhengig', 'somhet', 'sta'], 'hyp_pos': ['ADJ', 'NOUN', 'VERB']}\n",
      "{'type': 'delete', 'ref': ['vel'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 28, ASD=0.27\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e  </td><td>av  </td><td>av      </td><td>av    </td><td>langsom</td><td>##het </td><td>or    </td><td>##mest</td><td>##ad </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vel  </td><td>av  </td><td>den </td><td>oppfatning</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e  </td><td>##e </td><td>avhengig</td><td>somhet</td><td>somhet </td><td>somhet</td><td>somhet</td><td>somhet</td><td>sta  </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vi   </td><td>av  </td><td>den </td><td>oppfatning</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.017</td><td>0.015</td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.012      </td><td>0.02</td><td>0.019</td><td>0.048</td><td>0.031  </td><td>0.048</td><td>0.072</td><td>0.56</td><td>0.63    </td><td>0.556 </td><td>0.632  </td><td>0.571 </td><td>0.718 </td><td>0.727 </td><td>0.603</td><td>0.056</td><td>0.033</td><td>0.029</td><td>0.034</td><td>0.038</td><td>0.706</td><td>0.05</td><td>0.04</td><td>0.031     </td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['monika', 'kristensen', 'solås'], 'ref_pos': ['PROPN', 'PROPN', 'PROPN'], 'hyp': ['monica', 'christine', 'sone'], 'hyp_pos': ['PROPN', 'X', 'X']}\n",
      "{'type': 'delete', 'ref': ['da'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at  </td><td>man  </td><td>må   </td><td>stille</td><td>alle </td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ika</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##ås </td><td>##ås</td><td>bråket</td><td>startet</td><td>da     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at  </td><td>man  </td><td>må   </td><td>stille</td><td>alle </td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ica</td><td>christ </td><td>##ine</td><td>##ine</td><td>##ine</td><td>sone</td><td>bråket</td><td>startet</td><td>startet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.11 </td><td>0.01</td><td>0.006</td><td>0.006</td><td>0.005 </td><td>0.003</td><td>0.006</td><td>0.01     </td><td>0.005</td><td>0.005 </td><td>0.004</td><td>0.01   </td><td>0.004</td><td>0.006      </td><td>0.019   </td><td>0.094</td><td>0.383</td><td>0.497  </td><td>0.537</td><td>0.626</td><td>0.631</td><td>0.66</td><td>0.288 </td><td>0.265  </td><td>0.663  </td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['monika'], 'ref_pos': ['PROPN'], 'hyp': ['monica'], 'hyp_pos': ['PROPN']}\n",
      "{'type': 'substitute', 'ref': ['solås'], 'ref_pos': ['PROPN'], 'hyp': ['solus'], 'hyp_pos': ['X']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>man  </td><td>må   </td><td>stille</td><td>alle</td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ika</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##ås </td><td>bråket</td><td>startet</td><td>da   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>man  </td><td>må   </td><td>stille</td><td>alle</td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ica</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##us </td><td>bråket</td><td>startet</td><td>da   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001 </td><td>0.0 </td><td>0.001</td><td>0.002    </td><td>0.001</td><td>0.001 </td><td>0.001</td><td>0.002  </td><td>0.001</td><td>0.001      </td><td>0.009   </td><td>0.073</td><td>0.388</td><td>0.025  </td><td>0.035</td><td>0.088</td><td>0.499</td><td>0.015 </td><td>0.004  </td><td>0.003</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['han', 'seiar'], 'ref_pos': ['PRON', 'VERB'], 'hyp': ['hans', 'eier'], 'hyp_pos': ['PRON', 'VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['om'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['tenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['vil'], 'ref_pos': ['VERB'], 'hyp': ['ville'], 'hyp_pos': ['AUX']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.27\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når </td><td>han  </td><td>han  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk   </td><td>##ingstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når </td><td>hans </td><td>eier </td><td>han  </td><td>han  </td><td>han  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>om   </td><td>tenkning</td><td>##stid   </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>ville</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.017</td><td>0.007</td><td>0.006</td><td>0.005</td><td>0.009  </td><td>0.06</td><td>0.335</td><td>0.765</td><td>0.118</td><td>0.761</td><td>0.673</td><td>0.085</td><td>0.039</td><td>0.028</td><td>0.027</td><td>0.051</td><td>0.025</td><td>0.024   </td><td>0.102</td><td>0.603</td><td>0.51    </td><td>0.307    </td><td>0.021</td><td>0.019     </td><td>0.021</td><td>0.017</td><td>0.192</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['seier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['omtenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ingstid </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seier</td><td>seier</td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ningstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.012</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.004  </td><td>0.012</td><td>0.011</td><td>0.201</td><td>0.443</td><td>0.021</td><td>0.14 </td><td>0.019</td><td>0.019</td><td>0.031</td><td>0.012</td><td>0.014   </td><td>0.028</td><td>0.048</td><td>0.065</td><td>0.245     </td><td>0.018</td><td>0.011     </td><td>0.009</td><td>0.007</td><td>0.01</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['nedla'], 'ref_pos': ['VERB'], 'hyp': ['nedelaaktoratet'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['aktoratet'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>saken</td><td>kom  </td><td>opp  </td><td>for  </td><td>lagmannsretten</td><td>i    </td><td>januar</td><td>ned  </td><td>##la </td><td>aktor</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om   </td><td>seks </td><td>års </td><td>fengsel</td><td>for  </td><td>å    </td><td>å   </td><td>##ke </td><td>gren </td><td>men  </td><td>i    </td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>saken</td><td>kom  </td><td>opp  </td><td>for  </td><td>lagmannsretten</td><td>i    </td><td>januar</td><td>nede </td><td>##la </td><td>##akt</td><td>##ora</td><td>##tet </td><td>påstand</td><td>om   </td><td>seks </td><td>års </td><td>fengsel</td><td>for  </td><td>å    </td><td>å   </td><td>##ke </td><td>gren </td><td>men  </td><td>i    </td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.004</td><td>0.003</td><td>0.004</td><td>0.003</td><td>0.004         </td><td>0.004</td><td>0.008 </td><td>0.313</td><td>0.081</td><td>0.643</td><td>0.644</td><td>0.422 </td><td>0.021  </td><td>0.012</td><td>0.012</td><td>0.01</td><td>0.01   </td><td>0.025</td><td>0.148</td><td>0.13</td><td>0.037</td><td>0.021</td><td>0.017</td><td>0.014</td><td>0.011 </td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 25, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>saken</td><td>kom</td><td>opp</td><td>for</td><td>lagmannsretten</td><td>i</td><td>januar</td><td>ned</td><td>##la</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om</td><td>seks</td><td>års</td><td>fengsel</td><td>for</td><td>å</td><td>##ke</td><td>gren</td><td>men</td><td>i</td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>saken</td><td>kom</td><td>opp</td><td>for</td><td>lagmannsretten</td><td>i</td><td>januar</td><td>ned</td><td>##la</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om</td><td>seks</td><td>års</td><td>fengsel</td><td>for</td><td>å</td><td>##ke</td><td>gren</td><td>men</td><td>i</td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0             </td><td>0</td><td>0     </td><td>0  </td><td>0   </td><td>0    </td><td>0     </td><td>0      </td><td>0 </td><td>0   </td><td>0  </td><td>0      </td><td>0  </td><td>0</td><td>0   </td><td>0   </td><td>0  </td><td>0</td><td>0     </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['gjeruldsen', 'er'], 'ref_pos': ['PROPN', 'AUX'], 'hyp': ['gjeroldsenb', 'heleverden'], 'hyp_pos': ['PROPN', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['selvfølgelig', 'den', 'skjønneste', 'babyen', 'i', 'hele', 'verden', 'helt'], 'ref_pos': ['ADJ', 'DET', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['sett', 'ja'], 'ref_pos': ['VERB', 'INTJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.47\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal </td><td>se  </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld </td><td>##sen</td><td>er   </td><td>selvfølgelig</td><td>den  </td><td>skjønne</td><td>##ste</td><td>babyen</td><td>i    </td><td>hele </td><td>verden  </td><td>helt    </td><td>objektivt</td><td>sett     </td><td>ja       </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal </td><td>se  </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##olds</td><td>##en </td><td>##en </td><td>##en        </td><td>##en </td><td>##en   </td><td>##en </td><td>##b   </td><td>##b  </td><td>hele </td><td>##verden</td><td>##verden</td><td>objektivt</td><td>objektivt</td><td>objektivt</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.097</td><td>0.015</td><td>0.015</td><td>0.026</td><td>0.02</td><td>0.014</td><td>0.014</td><td>0.019</td><td>0.023</td><td>0.089</td><td>0.067</td><td>0.532 </td><td>0.357</td><td>0.613</td><td>0.732       </td><td>0.621</td><td>0.861  </td><td>0.775</td><td>0.749 </td><td>0.703</td><td>0.255</td><td>0.37    </td><td>0.818   </td><td>0.385    </td><td>0.732    </td><td>0.676    </td><td>0.022</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['nesten'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'delete', 'ref': ['selvfølgelig', 'den', 'skjønneste', 'babyen', 'i'], 'ref_pos': ['ADJ', 'DET', 'ADJ', 'NOUN', 'ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['ja'], 'ref_pos': ['INTJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal</td><td>se   </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld</td><td>##sen</td><td>er   </td><td>selvfølgelig</td><td>den   </td><td>skjønne</td><td>##ste </td><td>babyen</td><td>i     </td><td>hele </td><td>verden</td><td>helt </td><td>objektivt</td><td>sett </td><td>ja   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal</td><td>se   </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld</td><td>##sen</td><td>##sen</td><td>nesten      </td><td>nesten</td><td>nesten </td><td>nesten</td><td>nesten</td><td>nesten</td><td>hele </td><td>verden</td><td>helt </td><td>objektivt</td><td>sett </td><td>sett </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.046</td><td>0.011</td><td>0.012</td><td>0.02</td><td>0.017</td><td>0.012</td><td>0.012</td><td>0.013</td><td>0.015</td><td>0.087</td><td>0.025</td><td>0.021</td><td>0.068</td><td>0.662</td><td>0.579       </td><td>0.742 </td><td>0.782  </td><td>0.787 </td><td>0.835 </td><td>0.678 </td><td>0.197</td><td>0.115 </td><td>0.059</td><td>0.055    </td><td>0.081</td><td>0.647</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['politistasjon'], 'ref_pos': ['NOUN'], 'hyp': ['politivstersjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['flyet'], 'ref_pos': ['NOUN'], 'hyp': ['flyene'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ha'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>politistasjon</td><td>politistasjon</td><td>politistasjon</td><td>så   </td><td>skal </td><td>flyet </td><td>ha    </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som  </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politi       </td><td>##vs         </td><td>##ters       </td><td>##jon        </td><td>så   </td><td>skal </td><td>flyene</td><td>flyene</td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som  </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.034</td><td>0.164</td><td>0.195  </td><td>0.52         </td><td>0.732        </td><td>0.694        </td><td>0.587        </td><td>0.056</td><td>0.088</td><td>0.179 </td><td>0.655 </td><td>0.082  </td><td>0.04 </td><td>0.019</td><td>0.034 </td><td>0.034  </td><td>0.016</td><td>0.018</td><td>0.02  </td><td>0.028</td><td>0.014</td><td>0.017</td><td>0.016</td><td>0.016 </td><td>0.013 </td><td>0.014</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['flyet'], 'ref_pos': ['NOUN'], 'hyp': ['fly'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ha'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>så  </td><td>skal </td><td>flyet</td><td>ha   </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>så  </td><td>skal </td><td>fly  </td><td>fly  </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.007</td><td>0.014</td><td>0.015  </td><td>0.013        </td><td>0.02</td><td>0.107</td><td>0.187</td><td>0.572</td><td>0.074  </td><td>0.031</td><td>0.015</td><td>0.015 </td><td>0.016  </td><td>0.01</td><td>0.012</td><td>0.012 </td><td>0.018</td><td>0.009</td><td>0.012</td><td>0.012</td><td>0.011 </td><td>0.008 </td><td>0.009</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['karakterisere'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['karakteriserer'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'substitute', 'ref': ['late'], 'ref_pos': ['ADJ'], 'hyp': ['later'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['om'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['kreativitet'], 'ref_pos': ['NOUN'], 'hyp': ['kreativitetsnok'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['nok'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>karakteriserer</td><td>karakteriserer</td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>og   </td><td>nærmest</td><td>late </td><td>som </td><td>om   </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>kreativitet</td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>karakteris    </td><td>##ere         </td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>og   </td><td>nærmest</td><td>later</td><td>som </td><td>som  </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>##sn       </td><td>##ok       </td><td>##ok </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.038</td><td>0.632</td><td>0.288         </td><td>0.544         </td><td>0.016  </td><td>0.004  </td><td>0.012       </td><td>0.027</td><td>0.011</td><td>0.014</td><td>0.129</td><td>0.187</td><td>0.06   </td><td>0.348</td><td>0.16</td><td>0.568</td><td>0.067</td><td>0.015</td><td>0.036</td><td>0.033</td><td>0.148      </td><td>0.623      </td><td>0.615      </td><td>0.712</td><td>0.079</td><td>0.029</td><td>0.014</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['karakterisere'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['karakteriserer'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['om'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>karakteriserer</td><td>karakteriserer</td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>nærmest</td><td>late </td><td>som  </td><td>om   </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>karakteris    </td><td>##ere         </td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>nærmest</td><td>late </td><td>som  </td><td>som  </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.037</td><td>0.637</td><td>0.279         </td><td>0.533         </td><td>0.014  </td><td>0.002  </td><td>0.007       </td><td>0.017</td><td>0.004</td><td>0.005</td><td>0.011</td><td>0.005  </td><td>0.023</td><td>0.081</td><td>0.544</td><td>0.042</td><td>0.008</td><td>0.007</td><td>0.006</td><td>0.014      </td><td>0.007</td><td>0.006</td><td>0.007</td><td>0.007</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['er', 'blitt', 'et', 'diskusjonene'], 'ref_pos': ['AUX', 'VERB', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['den'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['lurt'], 'ref_pos': ['ADJ'], 'hyp': ['rushan'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['for', 'ham'], 'ref_pos': ['ADP', 'PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['selv'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['har'], 'hyp_pos': ['AUX']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.55\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er     </td><td>blitt  </td><td>et     </td><td>diskusjonen</td><td>##e    </td><td>omkring</td><td>hans </td><td>troverdighet</td><td>hadde</td><td>det </td><td>nok  </td><td>vært </td><td>lurt </td><td>lurt </td><td>lurt </td><td>for  </td><td>ham  </td><td>å    </td><td>gå   </td><td>selv </td><td>om   </td><td>han  </td><td>ikke </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>omkring</td><td>omkring</td><td>omkring</td><td>omkring    </td><td>omkring</td><td>omkring</td><td>hans </td><td>troverdighet</td><td>hadde</td><td>den </td><td>nok  </td><td>vært </td><td>vært </td><td>rush </td><td>##an </td><td>##an </td><td>##an </td><td>å    </td><td>gå   </td><td>gå   </td><td>om   </td><td>han  </td><td>ikke </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.059</td><td>0.518  </td><td>0.761  </td><td>0.738  </td><td>0.678      </td><td>0.675  </td><td>0.312  </td><td>0.063</td><td>0.072       </td><td>0.063</td><td>0.51</td><td>0.064</td><td>0.146</td><td>0.646</td><td>0.676</td><td>0.787</td><td>0.809</td><td>0.787</td><td>0.174</td><td>0.112</td><td>0.826</td><td>0.202</td><td>0.077</td><td>0.136</td><td>0.513</td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['er', 'blitt', 'et', 'diskusjonene'], 'ref_pos': ['AUX', 'VERB', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['ham'], 'ref_pos': ['PRON'], 'hyp': ['han'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'delete', 'ref': ['selv'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['han'], 'ref_pos': ['PRON'], 'hyp': ['man'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.38\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er     </td><td>blitt  </td><td>et     </td><td>diskusjonen</td><td>##e    </td><td>omkring</td><td>hans </td><td>troverdighet</td><td>troverdighet</td><td>hadde</td><td>det  </td><td>nok  </td><td>vært </td><td>lurt </td><td>for  </td><td>ham  </td><td>å    </td><td>gå   </td><td>selv </td><td>om  </td><td>han  </td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>omkring</td><td>omkring</td><td>omkring</td><td>omkring    </td><td>omkring</td><td>omkring</td><td>hans </td><td>troverdighet</td><td>så          </td><td>hadde</td><td>det  </td><td>nok  </td><td>vært </td><td>lurt </td><td>for  </td><td>han  </td><td>å    </td><td>gå   </td><td>gå   </td><td>om  </td><td>man  </td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.521  </td><td>0.762  </td><td>0.754  </td><td>0.669      </td><td>0.678  </td><td>0.28   </td><td>0.056</td><td>0.066       </td><td>0.752       </td><td>0.074</td><td>0.048</td><td>0.039</td><td>0.041</td><td>0.043</td><td>0.063</td><td>0.193</td><td>0.056</td><td>0.048</td><td>0.809</td><td>0.18</td><td>0.481</td><td>0.08</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['friksjon', 'enn'], 'ref_pos': ['NOUN', 'ADP'], 'hyp': ['fleksjon', 'en'], 'hyp_pos': ['NOUN', 'DET']}\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['john', 'gulldahl'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['jon', 'guldal'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.26\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>friksjon</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>vi   </td><td>kjenner</td><td>hjemme</td><td>takk</td><td>skal </td><td>du  </td><td>ha   </td><td>john </td><td>gull </td><td>##dahl</td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>fle     </td><td>##ksjon </td><td>en      </td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>jeg  </td><td>kjenner</td><td>hjemme</td><td>takk</td><td>skal </td><td>du  </td><td>ha   </td><td>jon  </td><td>gul  </td><td>##dal </td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.028</td><td>0.029</td><td>0.025</td><td>0.032</td><td>0.565   </td><td>0.548   </td><td>0.765   </td><td>0.105</td><td>0.049</td><td>0.022</td><td>0.035</td><td>0.287</td><td>0.032  </td><td>0.027 </td><td>0.04</td><td>0.021</td><td>0.03</td><td>0.024</td><td>0.206</td><td>0.436</td><td>0.36  </td><td>0.03     </td><td>0.018</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['john', 'gulldahl'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['jon', 'guldal'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>vi   </td><td>kjenner</td><td>hjemme</td><td>takk </td><td>skal </td><td>du   </td><td>ha   </td><td>john </td><td>gull </td><td>##dahl</td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>jeg  </td><td>kjenner</td><td>hjemme</td><td>takk </td><td>skal </td><td>du   </td><td>ha   </td><td>jon  </td><td>gul  </td><td>##dal </td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.007</td><td>0.004</td><td>0.004</td><td>0.002</td><td>0.004   </td><td>0.003</td><td>0.003</td><td>0.005</td><td>0.006</td><td>0.271</td><td>0.017  </td><td>0.009 </td><td>0.022</td><td>0.012</td><td>0.017</td><td>0.013</td><td>0.222</td><td>0.429</td><td>0.348 </td><td>0.02     </td><td>0.009</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['fordi', 'at'], 'ref_pos': ['SCONJ', 'SCONJ'], 'hyp': ['fred', 'vil'], 'hyp_pos': ['NOUN', 'AUX']}\n",
      "{'type': 'delete', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['variabler', 'som'], 'ref_pos': ['NOUN', 'PRON'], 'hyp': ['variable', 'så'], 'hyp_pos': ['ADJ', 'ADV']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>som     </td><td>var     </td><td>var  </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fred    </td><td>vil  </td><td>vil  </td><td>vil  </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variable </td><td>variable</td><td>variable</td><td>så   </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.048</td><td>0.047 </td><td>0.027</td><td>0.011</td><td>0.015  </td><td>0.016</td><td>0.025</td><td>0.019  </td><td>0.026</td><td>0.103   </td><td>0.428   </td><td>0.553</td><td>0.581</td><td>0.579</td><td>0.123</td><td>0.143</td><td>0.066</td><td>0.029</td><td>0.041</td><td>0.049</td><td>0.354    </td><td>0.631   </td><td>0.676   </td><td>0.707</td><td>0.166</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['at'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['som'], 'ref_pos': ['PRON'], 'hyp': ['svar'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['var', 'mye'], 'ref_pos': ['AUX', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>variabler</td><td>som  </td><td>var  </td><td>mye </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>fordi</td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>svar     </td><td>svar </td><td>svar </td><td>svar</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.078</td><td>0.009 </td><td>0.003</td><td>0.003</td><td>0.004  </td><td>0.004</td><td>0.005</td><td>0.005  </td><td>0.005</td><td>0.009   </td><td>0.041</td><td>0.451</td><td>0.019</td><td>0.019</td><td>0.024</td><td>0.019</td><td>0.015</td><td>0.021</td><td>0.038</td><td>0.148    </td><td>0.638    </td><td>0.656</td><td>0.697</td><td>0.68</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['n', 'r'], 'hyp_pos': ['ADV', 'ADV']}\n",
      "{'type': 'substitute', 'ref': ['nrk'], 'ref_pos': ['PROPN'], 'hyp': ['k'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.14\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hva  </td><td>hun  </td><td>får  </td><td>er   </td><td>hemmelig</td><td>her  </td><td>er   </td><td>er   </td><td>er   </td><td>nrk  </td><td>dags </td><td>##nytt</td><td>klokka</td><td>er  </td><td>tolv </td><td>tretti</td><td>først</td><td>til  </td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha   </td><td>##fs </td><td>##lund</td><td>oslo </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hva  </td><td>hun  </td><td>får  </td><td>er   </td><td>hemmelig</td><td>her  </td><td>er   </td><td>n    </td><td>r    </td><td>k    </td><td>dags </td><td>##nytt</td><td>klokka</td><td>er  </td><td>tolv </td><td>tretti</td><td>først</td><td>til  </td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha   </td><td>##fs </td><td>##lund</td><td>oslo </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.002</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.003   </td><td>0.013</td><td>0.046</td><td>0.722</td><td>0.643</td><td>0.659</td><td>0.054</td><td>0.042 </td><td>0.023 </td><td>0.02</td><td>0.023</td><td>0.024 </td><td>0.014</td><td>0.012</td><td>0.017  </td><td>0.016</td><td>0.015</td><td>0.015      </td><td>0.019</td><td>0.022</td><td>0.02  </td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 26, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hva</td><td>hun</td><td>får</td><td>er</td><td>hemmelig</td><td>her</td><td>er</td><td>nrk</td><td>dags</td><td>##nytt</td><td>klokka</td><td>er</td><td>tolv</td><td>tretti</td><td>først</td><td>til</td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha</td><td>##fs</td><td>##lund</td><td>oslo</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hva</td><td>hun</td><td>får</td><td>er</td><td>hemmelig</td><td>her</td><td>er</td><td>nrk</td><td>dags</td><td>##nytt</td><td>klokka</td><td>er</td><td>tolv</td><td>tretti</td><td>først</td><td>til</td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha</td><td>##fs</td><td>##lund</td><td>oslo</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0 </td><td>0       </td><td>0  </td><td>0 </td><td>0  </td><td>0   </td><td>0     </td><td>0     </td><td>0 </td><td>0   </td><td>0     </td><td>0    </td><td>0  </td><td>0      </td><td>0    </td><td>0    </td><td>0          </td><td>0 </td><td>0   </td><td>0     </td><td>0   </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved', 'politioverbetjent'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['politijobben', 'arnesen'], 'hyp_pos': ['NOUN', 'PROPN']}\n",
      "{'type': 'delete', 'ref': ['arne', 'skogen'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>##betjent</td><td>ar   </td><td>##ne   </td><td>skogen </td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>politi   </td><td>##jobben </td><td>ar   </td><td>##nesen</td><td>##nesen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.007         </td><td>0.007      </td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.006</td><td>0.008</td><td>0.012</td><td>0.018</td><td>0.021  </td><td>0.816  </td><td>0.712  </td><td>0.173</td><td>0.084</td><td>0.04    </td><td>0.089</td><td>0.761</td><td>0.12  </td><td>0.679 </td><td>0.531    </td><td>0.619    </td><td>0.238</td><td>0.628  </td><td>0.563  </td><td>0.085</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved'], 'ref_pos': ['ADP'], 'hyp': ['politibetjent'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['politioverbetjent'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du  </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne </td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du  </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>##betjent</td><td>ar   </td><td>##ne </td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.004         </td><td>0.004      </td><td>0.005</td><td>0.006</td><td>0.006</td><td>0.004</td><td>0.007</td><td>0.01</td><td>0.016</td><td>0.018  </td><td>0.822  </td><td>0.71   </td><td>0.169</td><td>0.086</td><td>0.047   </td><td>0.096</td><td>0.696</td><td>0.053 </td><td>0.575 </td><td>0.091    </td><td>0.037</td><td>0.041</td><td>0.038 </td><td>0.032</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['felleseiga', 'selskap'], 'ref_pos': ['ADJ', 'NOUN'], 'hyp': ['felles', 'eierselskap'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['driva', 'kanalane'], 'ref_pos': ['VERB', 'NOUN'], 'hyp': ['drive', 'kanalene'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['mekler', 'selskapet'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['meklarselskapet', 'first', 'securities'], 'ref_pos': ['NOUN', 'ADJ', 'X'], 'hyp': ['først', 'i', 'kuritis'], 'hyp_pos': ['ADV', 'ADP', 'NOUN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.46\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>##eig</td><td>##a  </td><td>selskap </td><td>skal </td><td>driv </td><td>##a  </td><td>kanal   </td><td>##ane   </td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##lar</td><td>##selskapet</td><td>first</td><td>first</td><td>first</td><td>sec  </td><td>sec  </td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>eiers</td><td>eiers</td><td>##elskap</td><td>skal </td><td>drive</td><td>drive</td><td>kanalene</td><td>kanalene</td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##ler</td><td>selskapet  </td><td>først</td><td>i    </td><td>kur  </td><td>kur  </td><td>##iti</td><td>##s  </td><td>##s      </td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.043</td><td>0.089</td><td>0.081 </td><td>0.523</td><td>0.821</td><td>0.251   </td><td>0.042</td><td>0.169</td><td>0.601</td><td>0.336   </td><td>0.596   </td><td>0.036 </td><td>0.032</td><td>0.037</td><td>0.136</td><td>0.057</td><td>0.04       </td><td>0.133</td><td>0.097</td><td>0.316</td><td>0.276      </td><td>0.606</td><td>0.713</td><td>0.718</td><td>0.748</td><td>0.75 </td><td>0.737</td><td>0.741    </td><td>0.076</td><td>0.033      </td><td>0.051</td><td>0.072</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['et', 'felleseiga', 'selskap'], 'ref_pos': ['DET', 'ADJ', 'NOUN'], 'hyp': ['eit', 'felles', 'eierselskap'], 'hyp_pos': ['DET', 'ADJ', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['driva', 'kanalane'], 'ref_pos': ['VERB', 'NOUN'], 'hyp': ['drive', 'kanalene'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['mekler'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['meklarselskapet', 'first'], 'ref_pos': ['NOUN', 'ADJ'], 'hyp': ['selskapet', 'først'], 'hyp_pos': ['NOUN', 'ADV']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.33\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>##eig</td><td>##a  </td><td>selskap </td><td>skal </td><td>driv </td><td>##a  </td><td>kanal   </td><td>##ane   </td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##lar</td><td>##selskapet</td><td>first</td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>eit  </td><td>felles</td><td>eiers</td><td>eiers</td><td>##elskap</td><td>skal </td><td>drive</td><td>drive</td><td>kanalene</td><td>kanalene</td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##ler</td><td>selskapet  </td><td>først</td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.252</td><td>0.082 </td><td>0.529</td><td>0.826</td><td>0.236   </td><td>0.036</td><td>0.183</td><td>0.615</td><td>0.422   </td><td>0.603   </td><td>0.033 </td><td>0.027</td><td>0.029</td><td>0.049</td><td>0.037</td><td>0.036      </td><td>0.126</td><td>0.092</td><td>0.307</td><td>0.255      </td><td>0.566</td><td>0.163</td><td>0.085    </td><td>0.048</td><td>0.039      </td><td>0.055</td><td>0.071</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['konflikter'], 'ref_pos': ['NOUN'], 'hyp': ['konflikten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['borgerkrig'], 'ref_pos': ['NOUN'], 'hyp': ['borgekrig'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>borgerkrig</td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikten</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borg      </td><td>##ek      </td><td>##rig     </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.006   </td><td>0.005</td><td>0.003</td><td>0.003</td><td>0.003</td><td>0.004   </td><td>0.025</td><td>0.158     </td><td>0.039     </td><td>0.021</td><td>0.037</td><td>0.007</td><td>0.022 </td><td>0.78    </td><td>0.1</td><td>0.025</td><td>0.018</td><td>0.014     </td><td>0.015</td><td>0.028</td><td>0.015</td><td>0.052</td><td>0.549     </td><td>0.743     </td><td>0.546     </td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.05\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.002</td><td>0.001   </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.001   </td><td>0.001</td><td>0.003     </td><td>0.003     </td><td>0.003</td><td>0.03 </td><td>0.005</td><td>0.02  </td><td>0.781   </td><td>0.097</td><td>0.022</td><td>0.016</td><td>0.01      </td><td>0.011</td><td>0.008</td><td>0.008</td><td>0.008</td><td>0.01      </td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['kjent'], 'ref_pos': ['ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['e', 'ø'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['eøsområdet'], 'ref_pos': ['NOUN'], 'hyp': ['sområdet'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.21\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kjent</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e   </td><td>##øs </td><td>##øs </td><td>##området</td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e   </td><td>ø    </td><td>som  </td><td>##rådet  </td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.036</td><td>0.709</td><td>0.174</td><td>0.018  </td><td>0.019</td><td>0.012   </td><td>0.016</td><td>0.013</td><td>0.013  </td><td>0.015   </td><td>0.023    </td><td>0.049</td><td>0.029</td><td>0.028</td><td>0.033</td><td>0.081</td><td>0.14</td><td>0.579</td><td>0.725</td><td>0.56     </td><td>0.047</td><td>0.027</td><td>0.019</td><td>0.011</td><td>0.013</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['kjent'], 'ref_pos': ['ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['eøs'], 'hyp_pos': ['PROPN']}\n",
      "{'type': 'substitute', 'ref': ['eøsområdet'], 'ref_pos': ['NOUN'], 'hyp': ['området'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kjent</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e    </td><td>##øs </td><td>##området</td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e    </td><td>##øs </td><td>området  </td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.034</td><td>0.703</td><td>0.173</td><td>0.014  </td><td>0.015</td><td>0.009   </td><td>0.012</td><td>0.01 </td><td>0.01   </td><td>0.012   </td><td>0.015    </td><td>0.019</td><td>0.013</td><td>0.012</td><td>0.009</td><td>0.017</td><td>0.032</td><td>0.097</td><td>0.175    </td><td>0.009</td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['martha', 'louis'], 'hyp_pos': ['X', 'X']}\n",
      "{'type': 'substitute', 'ref': ['behn'], 'ref_pos': ['PROPN'], 'hyp': ['ben'], 'hyp_pos': ['X']}\n",
      "{'type': 'delete', 'ref': ['har'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 29, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og  </td><td>ar   </td><td>##i  </td><td>beh </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>og  </td><td>ar   </td><td>##i  </td><td>##i </td><td>##i  </td><td>ben  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.041</td><td>0.008</td><td>0.004</td><td>0.003     </td><td>0.002</td><td>0.003 </td><td>0.002 </td><td>0.004</td><td>0.005</td><td>0.002</td><td>0.002</td><td>0.003 </td><td>0.002</td><td>0.003</td><td>0.006</td><td>0.004  </td><td>0.031</td><td>0.037    </td><td>0.608</td><td>0.563</td><td>0.081</td><td>0.121</td><td>0.579</td><td>0.03</td><td>0.035</td><td>0.066</td><td>0.72</td><td>0.504</td><td>0.578</td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['märtha'], 'ref_pos': ['PROPN'], 'hyp': ['martha'], 'hyp_pos': ['PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.001     </td><td>0.0</td><td>0.001 </td><td>0.001 </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.0   </td><td>0.0</td><td>0.001</td><td>0.002</td><td>0.001  </td><td>0.002</td><td>0.025    </td><td>0.605</td><td>0.556</td><td>0.048</td><td>0.028</td><td>0.026</td><td>0.009</td><td>0.014</td><td>0.018</td><td>0.012</td><td>0.015</td><td>0.005</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 20\n",
    "\n",
    "for row in lower_asd_df.itertuples():\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['jo'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['ut'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['utfor'], 'ref_pos': ['ADP'], 'hyp': ['før'], 'hyp_pos': ['SCONJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['kva'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['hva', 'altså'], 'ref_pos': ['PRON', 'ADV'], 'hyp': ['som', 'skjedd'], 'hyp_pos': ['PRON', 'VERB']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>jo   </td><td>sånn </td><td>rett</td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>utfor</td><td>utfor</td><td>auto </td><td>##vernet</td><td>de   </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klart</td><td>klart</td><td>så   </td><td>hva  </td><td>hva  </td><td>altså </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>sånn </td><td>rett</td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>ut   </td><td>før  </td><td>auto </td><td>##vernet</td><td>de   </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klart</td><td>og   </td><td>så   </td><td>kva  </td><td>som  </td><td>skjedd</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.049</td><td>0.755</td><td>0.169</td><td>0.03</td><td>0.031</td><td>0.026</td><td>0.021 </td><td>0.035</td><td>0.567</td><td>0.605</td><td>0.029</td><td>0.039   </td><td>0.032</td><td>0.01 </td><td>0.026</td><td>0.017</td><td>0.011</td><td>0.011</td><td>0.049</td><td>0.108</td><td>0.715</td><td>0.205</td><td>0.322</td><td>0.693</td><td>0.704 </td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['jo'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['utfor', 'autovernet', 'de'], 'ref_pos': ['ADP', 'NOUN', 'DET'], 'hyp': ['utføre', 'ut', 'overnetde'], 'hyp_pos': ['VERB', 'ADP', 'ADP']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['klarte'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['klart'], 'ref_pos': ['ADJ'], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['hva'], 'ref_pos': ['PRON'], 'hyp': ['kvasse'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'delete', 'ref': ['altså'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 23, ASD=0.55\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>jo   </td><td>sånn </td><td>rett </td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>utfor </td><td>utfor</td><td>utfor</td><td>auto </td><td>##vernet</td><td>##vernet</td><td>de   </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klart </td><td>klart</td><td>så   </td><td>hva </td><td>hva  </td><td>altså</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>sånn </td><td>sånn </td><td>rett </td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>utføre</td><td>ut   </td><td>overn</td><td>overn</td><td>overn   </td><td>##et    </td><td>##de </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klarte</td><td>å    </td><td>så   </td><td>kva </td><td>##sse</td><td>##sse</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.091</td><td>0.457</td><td>0.184</td><td>0.037</td><td>0.041</td><td>0.026</td><td>0.033 </td><td>0.066</td><td>0.555 </td><td>0.628</td><td>0.706</td><td>0.73 </td><td>0.691   </td><td>0.711   </td><td>0.517</td><td>0.118</td><td>0.031</td><td>0.022</td><td>0.015</td><td>0.018</td><td>0.094</td><td>0.513 </td><td>0.743</td><td>0.275</td><td>0.53</td><td>0.747</td><td>0.753</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['martha', 'louis'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.10\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til</td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da </td><td>har</td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å  </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og  </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til</td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da </td><td>har</td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å  </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>og  </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.0</td><td>0.002</td><td>0.001     </td><td>0.0</td><td>0.001 </td><td>0.001 </td><td>0.0</td><td>0.0</td><td>0.001</td><td>0.0</td><td>0.0   </td><td>0.0</td><td>0.0</td><td>0.002</td><td>0.001  </td><td>0.002</td><td>0.026    </td><td>0.602</td><td>0.56 </td><td>0.074</td><td>0.108</td><td>0.569</td><td>0.01</td><td>0.008</td><td>0.013</td><td>0.005</td><td>0.006</td><td>0.001</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise', 'og', 'ari', 'behn'], 'ref_pos': ['PROPN', 'PROPN', 'CCONJ', 'PROPN', 'PROPN'], 'hyp': ['martha', 'louis', 'var', 'i', 'bena'], 'hyp_pos': ['PROPN', 'X', 'AUX', 'ADP', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['har'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 29, ASD=0.31\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>har  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>var  </td><td>i    </td><td>bena </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.011</td><td>0.006</td><td>0.004     </td><td>0.004</td><td>0.005 </td><td>0.005 </td><td>0.008</td><td>0.015</td><td>0.004</td><td>0.004</td><td>0.005 </td><td>0.003</td><td>0.004</td><td>0.008</td><td>0.006  </td><td>0.024</td><td>0.04     </td><td>0.598</td><td>0.566</td><td>0.109</td><td>0.151</td><td>0.587</td><td>0.753</td><td>0.8  </td><td>0.658</td><td>0.76 </td><td>0.678</td><td>0.484</td><td>0.534</td><td>0.566</td><td>0.016</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['ryddig'], 'ref_pos': ['ADJ'], 'hyp': ['rødde'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>sku  </td><td>##ta </td><td>til  </td><td>havn </td><td>på   </td><td>en   </td><td>ryddig</td><td>ryddig</td><td>ryddig</td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>sku  </td><td>##ta </td><td>til  </td><td>havn </td><td>på   </td><td>en   </td><td>en    </td><td>rød   </td><td>##de  </td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.003</td><td>0.003</td><td>0.002</td><td>0.001</td><td>0.002</td><td>0.003</td><td>0.004</td><td>0.005</td><td>0.007</td><td>0.005</td><td>0.005</td><td>0.008</td><td>0.007</td><td>0.006</td><td>0.003</td><td>0.005</td><td>0.012</td><td>0.033</td><td>0.058</td><td>0.659 </td><td>0.718 </td><td>0.8   </td><td>0.073</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['skuta'], 'ref_pos': ['NOUN'], 'hyp': ['skutte'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'delete', 'ref': ['til'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ryddig'], 'ref_pos': ['ADJ'], 'hyp': ['rødde'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>sku  </td><td>##ta </td><td>til  </td><td>havn </td><td>på   </td><td>en   </td><td>ryddig</td><td>ryddig</td><td>ryddig</td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>skutt</td><td>##e  </td><td>##e  </td><td>havn </td><td>på   </td><td>en   </td><td>en    </td><td>rød   </td><td>##de  </td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.013</td><td>0.006</td><td>0.008</td><td>0.005</td><td>0.003</td><td>0.005</td><td>0.006</td><td>0.008</td><td>0.013</td><td>0.011</td><td>0.012</td><td>0.014</td><td>0.023</td><td>0.069</td><td>0.598</td><td>0.636</td><td>0.621</td><td>0.204</td><td>0.053</td><td>0.073</td><td>0.667 </td><td>0.709 </td><td>0.788 </td><td>0.075</td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['DET'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['telegrambyrået'], 'ref_pos': ['NOUN'], 'hyp': ['telegrambyrå'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.04\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hvor</td><td>mange</td><td>som</td><td>er </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>##gram</td><td>##byrået</td><td>ir   </td><td>##na</td><td>så   </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hvor</td><td>mange</td><td>som</td><td>er </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>##gram</td><td>##byrå  </td><td>ir   </td><td>##na</td><td>så   </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.0 </td><td>0.0  </td><td>0.0</td><td>0.0</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002  </td><td>0.01  </td><td>0.111</td><td>0.106</td><td>0.019  </td><td>0.02</td><td>0.026 </td><td>0.186   </td><td>0.037</td><td>0.02</td><td>0.011</td><td>0.007</td><td>0.006</td><td>0.006</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['DET'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['telegrambyrået'], 'ref_pos': ['NOUN'], 'hyp': ['telegranbyirna'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['irna'], 'ref_pos': ['PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hvor </td><td>mange</td><td>som  </td><td>er   </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>##gram</td><td>##gram</td><td>##gram</td><td>##byrået</td><td>ir   </td><td>##na </td><td>så  </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hvor </td><td>mange</td><td>som  </td><td>er   </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>tele  </td><td>##gran</td><td>##by  </td><td>##ir    </td><td>##ir </td><td>##na </td><td>så  </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.004</td><td>0.001</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.008</td><td>0.002</td><td>0.004</td><td>0.002</td><td>0.002</td><td>0.003  </td><td>0.012 </td><td>0.101</td><td>0.135</td><td>0.028  </td><td>0.11</td><td>0.494 </td><td>0.688 </td><td>0.716 </td><td>0.602   </td><td>0.624</td><td>0.262</td><td>0.02</td><td>0.011</td><td>0.009</td><td>0.008</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['høystbydende'], 'ref_pos': ['ADJ'], 'hyp': ['høydsbydene'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.16\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ett  </td><td>om   </td><td>gangen</td><td>mens </td><td>det  </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er   </td><td>for  </td><td>salg </td><td>til  </td><td>høyst</td><td>høyst</td><td>##by </td><td>##dende</td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ett  </td><td>om   </td><td>gangen</td><td>mens </td><td>det  </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er   </td><td>for  </td><td>salg </td><td>til  </td><td>høy  </td><td>##ds </td><td>##by </td><td>##dene </td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.004</td><td>0.002</td><td>0.002 </td><td>0.002</td><td>0.003</td><td>0.003</td><td>0.005</td><td>0.002</td><td>0.003</td><td>0.002</td><td>0.004</td><td>0.009</td><td>0.011</td><td>0.015</td><td>0.019</td><td>0.039</td><td>0.126</td><td>0.547</td><td>0.734</td><td>0.214</td><td>0.547  </td><td>0.026</td><td>0.016</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['om'], 'ref_pos': ['ADP'], 'hyp': ['omgangen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['gangen'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['de'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['høystbydende'], 'ref_pos': ['ADJ'], 'hyp': ['høydsbydene'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.32\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ett </td><td>ett     </td><td>om      </td><td>gangen  </td><td>mens </td><td>det  </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er  </td><td>for  </td><td>salg </td><td>til  </td><td>høyst</td><td>høyst</td><td>##by </td><td>##dende</td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ett </td><td>omgangen</td><td>omgangen</td><td>omgangen</td><td>mens </td><td>de   </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er  </td><td>for  </td><td>salg </td><td>til  </td><td>høy  </td><td>##ds </td><td>##by </td><td>##dene </td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.12</td><td>0.675   </td><td>0.68    </td><td>0.686   </td><td>0.102</td><td>0.449</td><td>0.084</td><td>0.024</td><td>0.014</td><td>0.016</td><td>0.019</td><td>0.02 </td><td>0.017</td><td>0.021</td><td>0.02</td><td>0.026</td><td>0.046</td><td>0.131</td><td>0.545</td><td>0.729</td><td>0.214</td><td>0.545  </td><td>0.022</td><td>0.015</td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regjering'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['i'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['innsigelsesregime'], 'ref_pos': ['NOUN'], 'hyp': ['innsigelseregimet'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['blant'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.35\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>[CLS]    </td><td>da   </td><td>vi   </td><td>var  </td><td>en   </td><td>del  </td><td>av  </td><td>regjeringen</td><td>regjeringen</td><td>snudde</td><td>vi   </td><td>et   </td><td>innsigelse </td><td>##sreg</td><td>##ime</td><td>##ime</td><td>som  </td><td>var  </td><td>bygd </td><td>opp </td><td>gjennom</td><td>flere</td><td>år  </td><td>med  </td><td>blant</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>regjering</td><td>da   </td><td>da   </td><td>i    </td><td>en   </td><td>del  </td><td>av  </td><td>regjeringen</td><td>så         </td><td>snudde</td><td>vi   </td><td>et   </td><td>innsigelser</td><td>##eg  </td><td>##ime</td><td>##t  </td><td>som  </td><td>var  </td><td>bygd </td><td>opp </td><td>gjennom</td><td>flere</td><td>år  </td><td>med  </td><td>med  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.082</td><td>0.755    </td><td>0.228</td><td>0.657</td><td>0.592</td><td>0.197</td><td>0.139</td><td>0.09</td><td>0.067      </td><td>0.719      </td><td>0.05  </td><td>0.053</td><td>0.058</td><td>0.144      </td><td>0.495 </td><td>0.149</td><td>0.607</td><td>0.028</td><td>0.022</td><td>0.023</td><td>0.02</td><td>0.015  </td><td>0.022</td><td>0.02</td><td>0.099</td><td>0.646</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regjering'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['innsigelsesregime'], 'ref_pos': ['NOUN'], 'hyp': ['insigelseregimet'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': ['har'], 'hyp_pos': ['AUX']}\n",
      "{'type': 'delete', 'ref': ['blant'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.51\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>[CLS]    </td><td>da   </td><td>vi   </td><td>var  </td><td>en   </td><td>del  </td><td>av   </td><td>regjeringen</td><td>regjeringen</td><td>snudde</td><td>vi   </td><td>et   </td><td>innsigelse</td><td>##sreg</td><td>##sreg</td><td>##sreg</td><td>##sreg</td><td>##ime</td><td>som  </td><td>var  </td><td>bygd </td><td>opp  </td><td>gjennom</td><td>flere</td><td>år   </td><td>med  </td><td>blant</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>regjering</td><td>da   </td><td>vi   </td><td>vi   </td><td>en   </td><td>del  </td><td>av   </td><td>regjeringen</td><td>så         </td><td>snudde</td><td>vi   </td><td>et   </td><td>ins       </td><td>##ige </td><td>##ls  </td><td>##ere </td><td>##gi  </td><td>##met</td><td>som  </td><td>har  </td><td>bygd </td><td>opp  </td><td>gjennom</td><td>flere</td><td>år   </td><td>med  </td><td>med  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.084</td><td>0.756    </td><td>0.135</td><td>0.078</td><td>0.611</td><td>0.119</td><td>0.073</td><td>0.049</td><td>0.06       </td><td>0.72       </td><td>0.074 </td><td>0.044</td><td>0.094</td><td>0.657     </td><td>0.728 </td><td>0.731 </td><td>0.633 </td><td>0.669 </td><td>0.579</td><td>0.081</td><td>0.446</td><td>0.127</td><td>0.078</td><td>0.041  </td><td>0.043</td><td>0.046</td><td>0.122</td><td>0.658</td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['saddam'], 'ref_pos': ['NOUN'], 'hyp': ['sadam'], 'hyp_pos': ['X']}\n",
      "{'type': 'substitute', 'ref': ['nitten'], 'ref_pos': ['NUM'], 'hyp': ['nittenåttito'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['åttito'], 'ref_pos': ['NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 31, ASD=0.14\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten</td><td>##ta </td><td>##tfor</td><td>##søk</td><td>mot </td><td>sad  </td><td>##dam</td><td>i    </td><td>nit  </td><td>##ten</td><td>åt   </td><td>##tit</td><td>##tit</td><td>##o  </td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten</td><td>##ta </td><td>##tfor</td><td>##søk</td><td>mot </td><td>sad  </td><td>##am </td><td>i    </td><td>nit  </td><td>##ten</td><td>##ten</td><td>##ten</td><td>##ått</td><td>##ito</td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.002    </td><td>0.003</td><td>0.003</td><td>0.003 </td><td>0.003</td><td>0.01</td><td>0.054</td><td>0.383</td><td>0.031</td><td>0.055</td><td>0.117</td><td>0.524</td><td>0.508</td><td>0.614</td><td>0.475</td><td>0.052</td><td>0.015</td><td>0.014  </td><td>0.011</td><td>0.009 </td><td>0.012</td><td>0.01</td><td>0.009</td><td>0.015</td><td>0.009  </td><td>0.009</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['attentatforsøk'], 'ref_pos': ['NOUN'], 'hyp': ['atentatforsøk'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['saddam'], 'ref_pos': ['NOUN'], 'hyp': ['sadam'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['nitten'], 'ref_pos': ['NUM'], 'hyp': ['nittenåttito'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['åttito'], 'ref_pos': ['NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 31, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten    </td><td>atten</td><td>##ta </td><td>##ta </td><td>##tfor</td><td>##søk   </td><td>mot  </td><td>sad  </td><td>##dam</td><td>i    </td><td>nit  </td><td>##ten</td><td>åt   </td><td>##tit</td><td>##tit</td><td>##o  </td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>mislykket</td><td>ate  </td><td>##nt </td><td>##at </td><td>##at  </td><td>##forsøk</td><td>mot  </td><td>sad  </td><td>##am </td><td>i    </td><td>nit  </td><td>##ten</td><td>##ten</td><td>##ten</td><td>##ått</td><td>##ito</td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.004</td><td>0.006</td><td>0.007</td><td>0.012</td><td>0.025    </td><td>0.626    </td><td>0.64 </td><td>0.621</td><td>0.533</td><td>0.645 </td><td>0.349   </td><td>0.042</td><td>0.094</td><td>0.419</td><td>0.037</td><td>0.059</td><td>0.117</td><td>0.522</td><td>0.509</td><td>0.615</td><td>0.477</td><td>0.053</td><td>0.016</td><td>0.016  </td><td>0.011</td><td>0.01  </td><td>0.012</td><td>0.01</td><td>0.008</td><td>0.014</td><td>0.011  </td><td>0.009</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.04\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i  </td><td>tjenestetilbud</td><td>##et</td><td>til</td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>eiendomsskatt</td><td>##en </td><td>##en </td><td>er  </td><td>det  </td><td>helt </td><td>feil</td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i  </td><td>tjenestetilbud</td><td>##et</td><td>til</td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>eiendomsskatt</td><td>##en </td><td>så   </td><td>er  </td><td>det  </td><td>helt </td><td>feil</td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.002</td><td>0.001</td><td>0.001  </td><td>0.001</td><td>0.0  </td><td>0.0</td><td>0.0           </td><td>0.0 </td><td>0.0</td><td>0.001      </td><td>0.004 </td><td>0.002</td><td>0.003  </td><td>0.001        </td><td>0.008</td><td>0.673</td><td>0.07</td><td>0.022</td><td>0.011</td><td>0.01</td><td>0.01     </td><td>0.008</td><td>0.013</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regendomsskatten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['eiendomsskatten'], 'ref_pos': ['NOUN'], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i    </td><td>tjenestetilbud</td><td>##et </td><td>til  </td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>fjerner</td><td>fjerner</td><td>fjerner</td><td>eiendomsskatt</td><td>##en </td><td>##en </td><td>er   </td><td>det  </td><td>helt </td><td>feil </td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i    </td><td>tjenestetilbud</td><td>##et </td><td>til  </td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>reg    </td><td>##endom</td><td>##ss   </td><td>##katt       </td><td>##en </td><td>så   </td><td>er   </td><td>det  </td><td>helt </td><td>feil </td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.004</td><td>0.002</td><td>0.002  </td><td>0.003</td><td>0.006</td><td>0.003</td><td>0.008         </td><td>0.004</td><td>0.004</td><td>0.017      </td><td>0.008 </td><td>0.006</td><td>0.019  </td><td>0.715  </td><td>0.747  </td><td>0.778  </td><td>0.279        </td><td>0.078</td><td>0.682</td><td>0.087</td><td>0.039</td><td>0.032</td><td>0.029</td><td>0.04     </td><td>0.025</td><td>0.034</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>det</td><td>tar</td><td>åtte</td><td>dager</td><td>å</td><td>komme</td><td>inn</td><td>med</td><td>båt</td><td>og</td><td>to</td><td>dagers</td><td>kjøring</td><td>og</td><td>med</td><td>en</td><td>fly</td><td>##stri</td><td>##pe</td><td>så</td><td>er</td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>det</td><td>tar</td><td>åtte</td><td>dager</td><td>å</td><td>komme</td><td>inn</td><td>med</td><td>båt</td><td>og</td><td>to</td><td>dagers</td><td>kjøring</td><td>og</td><td>med</td><td>en</td><td>fly</td><td>##stri</td><td>##pe</td><td>så</td><td>er</td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0  </td><td>0   </td><td>0    </td><td>0</td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0 </td><td>0 </td><td>0     </td><td>0      </td><td>0 </td><td>0  </td><td>0 </td><td>0  </td><td>0     </td><td>0   </td><td>0 </td><td>0 </td><td>0    </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['med', 'en'], 'ref_pos': ['ADP', 'DET'], 'hyp': ['men', 'flystripet'], 'hyp_pos': ['CCONJ', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['flystripe'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>det  </td><td>tar  </td><td>åtte </td><td>dager</td><td>å    </td><td>komme</td><td>inn  </td><td>med  </td><td>båt  </td><td>og   </td><td>to   </td><td>dagers</td><td>kjøring</td><td>og   </td><td>og   </td><td>med  </td><td>en   </td><td>fly  </td><td>##stri</td><td>##pe </td><td>så   </td><td>er  </td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>det  </td><td>tar  </td><td>åtte </td><td>dager</td><td>å    </td><td>komme</td><td>inn  </td><td>med  </td><td>båt  </td><td>og   </td><td>to   </td><td>dagers</td><td>kjøring</td><td>og   </td><td>men  </td><td>men  </td><td>men  </td><td>fly  </td><td>##stri</td><td>##pet</td><td>så   </td><td>er  </td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.007</td><td>0.006</td><td>0.004</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.005</td><td>0.002</td><td>0.006</td><td>0.004</td><td>0.005 </td><td>0.006  </td><td>0.096</td><td>0.555</td><td>0.583</td><td>0.667</td><td>0.061</td><td>0.075 </td><td>0.328</td><td>0.036</td><td>0.03</td><td>0.013</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['seier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['omtenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ingstid </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seier</td><td>seier</td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ningstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.012</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.004  </td><td>0.012</td><td>0.011</td><td>0.201</td><td>0.443</td><td>0.021</td><td>0.14 </td><td>0.019</td><td>0.019</td><td>0.031</td><td>0.012</td><td>0.014   </td><td>0.028</td><td>0.048</td><td>0.065</td><td>0.245     </td><td>0.018</td><td>0.011     </td><td>0.009</td><td>0.007</td><td>0.01</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['sier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['om'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['tenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.22\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk   </td><td>##ingstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikke </td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>sier </td><td>sier </td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den </td><td>domfelte</td><td>tok  </td><td>om   </td><td>tenkning</td><td>##stid   </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.132</td><td>0.012</td><td>0.008</td><td>0.006</td><td>0.012  </td><td>0.021</td><td>0.042</td><td>0.309</td><td>0.481</td><td>0.049</td><td>0.138</td><td>0.068</td><td>0.068</td><td>0.179</td><td>0.04</td><td>0.028   </td><td>0.098</td><td>0.591</td><td>0.501   </td><td>0.29     </td><td>0.022</td><td>0.021     </td><td>0.019</td><td>0.029</td><td>0.021</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['ryggraden'], 'ref_pos': ['NOUN'], 'hyp': ['ryggrad'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['bestående', 'av', 'regjeringspartiene'], 'ref_pos': ['ADJ', 'ADP', 'NOUN'], 'hyp': ['bestå', 'stående', 'regjeringspartiet'], 'hyp_pos': ['VERB', 'ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.15\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##raden</td><td>i    </td><td>praksis</td><td>så   </td><td>er   </td><td>ikke </td><td>dagens</td><td>flertall</td><td>bestående</td><td>av     </td><td>regjerings</td><td>##partiene</td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr   </td><td>##p  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##rad  </td><td>i    </td><td>praksis</td><td>så   </td><td>er   </td><td>ikke </td><td>dagens</td><td>flertall</td><td>bestå    </td><td>stående</td><td>regjerings</td><td>##partiet </td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr   </td><td>##p  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.005</td><td>0.007</td><td>0.015</td><td>0.039</td><td>0.022  </td><td>0.012     </td><td>0.039</td><td>0.269  </td><td>0.016</td><td>0.007  </td><td>0.002</td><td>0.017</td><td>0.009</td><td>0.015 </td><td>0.046   </td><td>0.435    </td><td>0.587  </td><td>0.052     </td><td>0.257     </td><td>0.038</td><td>0.014 </td><td>0.012    </td><td>0.006</td><td>0.005</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['ryggrad'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['ryggraden'], 'ref_pos': ['NOUN'], 'hyp': ['da'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['erke'], 'hyp_pos': ['AUX']}\n",
      "{'type': 'delete', 'ref': ['ikke'], 'ref_pos': ['PART'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['bestående', 'av', 'regjeringspartiene'], 'ref_pos': ['ADJ', 'ADP', 'NOUN'], 'hyp': ['bestå', 'stående', 'regjeringspartiet'], 'hyp_pos': ['VERB', 'ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##raden</td><td>##raden</td><td>i    </td><td>praksis</td><td>så   </td><td>er   </td><td>ikke </td><td>dagens</td><td>flertall</td><td>bestående</td><td>av     </td><td>regjerings</td><td>##partiene</td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr  </td><td>##p </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##rad  </td><td>da     </td><td>i    </td><td>praksis</td><td>så   </td><td>erke </td><td>erke </td><td>dagens</td><td>flertall</td><td>bestå    </td><td>stående</td><td>regjerings</td><td>##partiet </td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr  </td><td>##p </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.024</td><td>0.03</td><td>0.029</td><td>0.028</td><td>0.043</td><td>0.031  </td><td>0.016     </td><td>0.043</td><td>0.28   </td><td>0.821  </td><td>0.174</td><td>0.084  </td><td>0.163</td><td>0.485</td><td>0.625</td><td>0.059 </td><td>0.046   </td><td>0.407    </td><td>0.563  </td><td>0.051     </td><td>0.258     </td><td>0.042</td><td>0.016 </td><td>0.015    </td><td>0.01</td><td>0.01</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['dei', 'aksjane'], 'ref_pos': ['DET', 'NOUN'], 'hyp': ['de', 'aksjene'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['jo', 'en'], 'hyp_pos': ['ADV', 'DET']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['medie'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['mediediskusjon'], 'ref_pos': ['NOUN'], 'hyp': ['diskusjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['jeg'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.23\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>dei  </td><td>aks    </td><td>##jan  </td><td>##e    </td><td>føler</td><td>du   </td><td>deg  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>er   </td><td>er   </td><td>veldig</td><td>stor </td><td>medie</td><td>##diskusjon</td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>jeg  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>de   </td><td>aksjene</td><td>aksjene</td><td>aksjene</td><td>føler</td><td>du   </td><td>deg  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>jo   </td><td>en   </td><td>veldig</td><td>stor </td><td>medie</td><td>diskusjon  </td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>men  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.051</td><td>0.191</td><td>0.53   </td><td>0.608  </td><td>0.58   </td><td>0.023</td><td>0.024</td><td>0.022</td><td>0.018  </td><td>0.022</td><td>0.027</td><td>0.028</td><td>0.028</td><td>0.057</td><td>0.689</td><td>0.371</td><td>0.042 </td><td>0.043</td><td>0.057</td><td>0.14       </td><td>0.018</td><td>0.009</td><td>0.022    </td><td>0.011</td><td>0.068</td><td>0.581</td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['dei', 'aksjane'], 'ref_pos': ['DET', 'NOUN'], 'hyp': ['aksjene', 'følger'], 'hyp_pos': ['NOUN', 'VERB']}\n",
      "{'type': 'delete', 'ref': ['føler'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['deg'], 'ref_pos': ['PRON'], 'hyp': ['det'], 'hyp_pos': ['DET']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['jo', 'en'], 'hyp_pos': ['ADV', 'DET']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['medie'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['mediediskusjon'], 'ref_pos': ['NOUN'], 'hyp': ['diskusjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['jeg'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.35\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>dei    </td><td>aks    </td><td>##jan  </td><td>##e    </td><td>føler </td><td>du   </td><td>deg  </td><td>deg  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>er   </td><td>er   </td><td>veldig</td><td>stor </td><td>medie</td><td>##diskusjon</td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>jeg  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>aksjene</td><td>aksjene</td><td>aksjene</td><td>aksjene</td><td>følger</td><td>du   </td><td>du   </td><td>det  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>jo   </td><td>en   </td><td>veldig</td><td>stor </td><td>medie</td><td>diskusjon  </td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>men  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.427  </td><td>0.644  </td><td>0.696  </td><td>0.572  </td><td>0.568 </td><td>0.085</td><td>0.515</td><td>0.681</td><td>0.192  </td><td>0.042</td><td>0.045</td><td>0.043</td><td>0.038</td><td>0.065</td><td>0.694</td><td>0.378</td><td>0.051 </td><td>0.048</td><td>0.076</td><td>0.149      </td><td>0.022</td><td>0.015</td><td>0.028    </td><td>0.018</td><td>0.084</td><td>0.579</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['morgan'], 'ref_pos': ['PROPN'], 'hyp': ['morgen'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.05\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er </td><td>president</td><td>##er</td><td>men</td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir</td><td>tema</td><td>på </td><td>møter</td><td>de </td><td>skal</td><td>ha </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>morg  </td><td>##an  </td><td>ts   </td><td>##vang</td><td>##ira</td><td>##i  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er </td><td>president</td><td>##er</td><td>men</td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir</td><td>tema</td><td>på </td><td>møter</td><td>de </td><td>skal</td><td>ha </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>morgen</td><td>morgen</td><td>ts   </td><td>##vang</td><td>##ira</td><td>##i  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.001</td><td>0.001   </td><td>0.0</td><td>0.0      </td><td>0.0 </td><td>0.0</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.0  </td><td>0.0   </td><td>0.0 </td><td>0.0 </td><td>0.0</td><td>0.0  </td><td>0.0</td><td>0.0 </td><td>0.0</td><td>0.001</td><td>0.003      </td><td>0.003</td><td>0.002     </td><td>0.018     </td><td>0.375 </td><td>0.591 </td><td>0.045</td><td>0.037 </td><td>0.031</td><td>0.026</td><td>0.003</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['morgan', 'tsvangirai'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['mogen', 'sanger'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er   </td><td>president</td><td>##er </td><td>men  </td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir </td><td>tema </td><td>på   </td><td>møter</td><td>de   </td><td>skal </td><td>ha   </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>morg </td><td>##an </td><td>ts   </td><td>##vang</td><td>##ira</td><td>##i  </td><td>##i   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er   </td><td>president</td><td>##er </td><td>men  </td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir </td><td>tema </td><td>på   </td><td>møter</td><td>de   </td><td>skal </td><td>ha   </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>mog  </td><td>##en </td><td>##en </td><td>##en  </td><td>##en </td><td>##en </td><td>sanger</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.058</td><td>0.005</td><td>0.007   </td><td>0.002</td><td>0.004    </td><td>0.003</td><td>0.002</td><td>0.014</td><td>0.012</td><td>0.009</td><td>0.004</td><td>0.002 </td><td>0.002</td><td>0.004</td><td>0.002</td><td>0.002</td><td>0.003</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.005      </td><td>0.007</td><td>0.01      </td><td>0.027     </td><td>0.557</td><td>0.428</td><td>0.745</td><td>0.677 </td><td>0.687</td><td>0.532</td><td>0.57  </td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['sakte'], 'ref_pos': ['ADJ'], 'hyp': ['sagt'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['sakte'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skjer</td><td>for  </td><td>sakte</td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>hvorfor</td><td>og     </td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>så   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skjer</td><td>for  </td><td>sagt </td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>hvorfor</td><td>hvorfor</td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>sakte</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.049</td><td>0.022</td><td>0.013</td><td>0.027</td><td>0.129</td><td>0.636</td><td>0.037</td><td>0.014</td><td>0.007</td><td>0.005</td><td>0.005</td><td>0.004</td><td>0.004       </td><td>0.005   </td><td>0.005</td><td>0.012</td><td>0.084  </td><td>0.634  </td><td>0.08   </td><td>0.042</td><td>0.031</td><td>0.09</td><td>0.68 </td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['skjer'], 'ref_pos': ['VERB'], 'hyp': ['skjeforsagt'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['for', 'sakte'], 'ref_pos': ['ADV', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['hvorfor'], 'ref_pos': ['ADV'], 'hyp': ['hvor'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'delete', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['sakte'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skjer</td><td>for   </td><td>sakte </td><td>sakte</td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>til  </td><td>hvorfor</td><td>og     </td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>så   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skje </td><td>##fors</td><td>##fors</td><td>##agt</td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>hvor </td><td>hvorfor</td><td>hvorfor</td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>sakte</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.049</td><td>0.037</td><td>0.059</td><td>0.288</td><td>0.658 </td><td>0.742 </td><td>0.747</td><td>0.035</td><td>0.019</td><td>0.012</td><td>0.007</td><td>0.006</td><td>0.008</td><td>0.005       </td><td>0.01    </td><td>0.007</td><td>0.026</td><td>0.674</td><td>0.142  </td><td>0.663  </td><td>0.087  </td><td>0.053</td><td>0.036</td><td>0.09</td><td>0.684</td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['politioverbetjent'], 'ref_pos': ['NOUN'], 'hyp': ['politibetjent'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>politi</td><td>##betjent</td><td>ar   </td><td>##ne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.003         </td><td>0.003      </td><td>0.005</td><td>0.004</td><td>0.005</td><td>0.004</td><td>0.006</td><td>0.009</td><td>0.016</td><td>0.018  </td><td>0.824  </td><td>0.71   </td><td>0.159</td><td>0.058</td><td>0.024   </td><td>0.028</td><td>0.022</td><td>0.029 </td><td>0.562 </td><td>0.058    </td><td>0.026</td><td>0.03</td><td>0.026 </td><td>0.018</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved', 'politioverbetjent'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['politibetjent', 'anne'], 'hyp_pos': ['NOUN', 'PROPN']}\n",
      "{'type': 'delete', 'ref': ['arne'], 'ref_pos': ['PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>##betjent</td><td>anne </td><td>anne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.006         </td><td>0.005      </td><td>0.006</td><td>0.007</td><td>0.007</td><td>0.007</td><td>0.008</td><td>0.012</td><td>0.017</td><td>0.019  </td><td>0.817  </td><td>0.708  </td><td>0.17 </td><td>0.088</td><td>0.049   </td><td>0.099</td><td>0.696</td><td>0.064 </td><td>0.573 </td><td>0.117    </td><td>0.629</td><td>0.52</td><td>0.101 </td><td>0.038</td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['reiser'], 'ref_pos': ['NOUN'], 'hyp': ['reise'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.06\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>og   </td><td>og   </td><td>og   </td><td>betale</td><td>for  </td><td>sånne</td><td>reiser</td><td>selv </td><td>jo   </td><td>men  </td><td>hvis </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>og   </td><td>og   </td><td>å    </td><td>betale</td><td>for  </td><td>sånne</td><td>reise </td><td>selv </td><td>jo   </td><td>men  </td><td>hvis </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.001</td><td>0.0  </td><td>0.001   </td><td>0.001 </td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.002</td><td>0.002</td><td>0.005</td><td>0.063</td><td>0.182</td><td>0.365</td><td>0.015 </td><td>0.018</td><td>0.073</td><td>0.143 </td><td>0.009</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['og', 'og'], 'ref_pos': ['CCONJ', 'CCONJ'], 'hyp': ['å', 'å'], 'hyp_pos': ['PART', 'PART']}\n",
      "{'type': 'substitute', 'ref': ['reiser'], 'ref_pos': ['NOUN'], 'hyp': ['reise'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['hvis'], 'ref_pos': ['SCONJ'], 'hyp': ['vi'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>og   </td><td>og   </td><td>betale</td><td>for  </td><td>sånne</td><td>reiser</td><td>selv </td><td>jo   </td><td>men  </td><td>hvis </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>å    </td><td>å    </td><td>betale</td><td>for  </td><td>sånne</td><td>reise </td><td>selv </td><td>jo   </td><td>men  </td><td>vi   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.085</td><td>0.006</td><td>0.002</td><td>0.003   </td><td>0.003 </td><td>0.004</td><td>0.009</td><td>0.006</td><td>0.017</td><td>0.015</td><td>0.043</td><td>0.413</td><td>0.405</td><td>0.026 </td><td>0.021</td><td>0.071</td><td>0.141 </td><td>0.012</td><td>0.008</td><td>0.037</td><td>0.595</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['guttedager'], 'ref_pos': ['NOUN'], 'hyp': ['gutedagar'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['balder'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['baldersteinen'], 'ref_pos': ['NOUN'], 'hyp': ['steinen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['nord'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['nordeuropas'], 'ref_pos': ['NOUN'], 'hyp': ['europas'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['den'], 'ref_pos': ['PRON'], 'hyp': ['denne'], 'hyp_pos': ['DET']}\n",
      "ASD-NorBERT ref len: 34, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hår  </td><td>##fag</td><td>##re </td><td>sprang</td><td>sine </td><td>gutte</td><td>##dager</td><td>##dager</td><td>og   </td><td>bal  </td><td>##ders</td><td>##teinen</td><td>som  </td><td>er   </td><td>nord </td><td>##euro</td><td>##pas</td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>den  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hår  </td><td>##fag</td><td>##re </td><td>sprang</td><td>sine </td><td>gut  </td><td>##edag </td><td>##ar   </td><td>og   </td><td>bal  </td><td>##der </td><td>steinen </td><td>som  </td><td>er   </td><td>nord </td><td>europa</td><td>##s  </td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>denne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.03 </td><td>0.003       </td><td>0.004</td><td>0.021 </td><td>0.005    </td><td>0.018</td><td>0.029 </td><td>0.014</td><td>0.011</td><td>0.013</td><td>0.014</td><td>0.008</td><td>0.007</td><td>0.011</td><td>0.027 </td><td>0.032</td><td>0.269</td><td>0.305  </td><td>0.547  </td><td>0.064</td><td>0.097</td><td>0.395 </td><td>0.249   </td><td>0.028</td><td>0.017</td><td>0.113</td><td>0.349 </td><td>0.399</td><td>0.021</td><td>0.016</td><td>0.016</td><td>0.018</td><td>0.024  </td><td>0.25 </td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['hårfagre'], 'ref_pos': ['ADJ'], 'hyp': ['håfagresprang'], 'hyp_pos': ['X']}\n",
      "{'type': 'delete', 'ref': ['sprang'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['guttedager'], 'ref_pos': ['NOUN'], 'hyp': ['gutedagar'], 'hyp_pos': ['X']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['balder'], 'hyp_pos': ['X']}\n",
      "{'type': 'substitute', 'ref': ['baldersteinen'], 'ref_pos': ['NOUN'], 'hyp': ['steinen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['nord'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['nordeuropas'], 'ref_pos': ['NOUN'], 'hyp': ['europas'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['den'], 'ref_pos': ['PRON'], 'hyp': ['denne'], 'hyp_pos': ['DET']}\n",
      "ASD-NorBERT ref len: 34, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hår  </td><td>##fag</td><td>##re  </td><td>sprang</td><td>sine </td><td>gutte</td><td>##dager</td><td>##dager</td><td>og   </td><td>bal  </td><td>##ders</td><td>##teinen</td><td>som  </td><td>er   </td><td>nord </td><td>##euro</td><td>##pas</td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>den  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hå   </td><td>##fag</td><td>##resp</td><td>##rang</td><td>sine </td><td>gut  </td><td>##edag </td><td>##ar   </td><td>og   </td><td>bal  </td><td>##der </td><td>steinen </td><td>som  </td><td>er   </td><td>nord </td><td>europa</td><td>##s  </td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>denne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.032</td><td>0.005       </td><td>0.006</td><td>0.021 </td><td>0.007    </td><td>0.024</td><td>0.031 </td><td>0.018</td><td>0.029</td><td>0.039</td><td>0.076</td><td>0.525</td><td>0.275</td><td>0.548 </td><td>0.408 </td><td>0.083</td><td>0.274</td><td>0.309  </td><td>0.546  </td><td>0.091</td><td>0.104</td><td>0.411 </td><td>0.247   </td><td>0.027</td><td>0.018</td><td>0.111</td><td>0.349 </td><td>0.402</td><td>0.022</td><td>0.017</td><td>0.018</td><td>0.019</td><td>0.026  </td><td>0.249</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 30, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>natten</td><td>enn</td><td>om</td><td>dagen</td><td>derfor</td><td>har</td><td>natt</td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra</td><td>[UNK]</td><td>de</td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er</td><td>over</td><td>brus</td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er</td><td>der</td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>natten</td><td>enn</td><td>om</td><td>dagen</td><td>derfor</td><td>har</td><td>natt</td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra</td><td>[UNK]</td><td>de</td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er</td><td>over</td><td>brus</td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er</td><td>der</td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0     </td><td>0  </td><td>0 </td><td>0    </td><td>0     </td><td>0  </td><td>0   </td><td>0    </td><td>0      </td><td>0    </td><td>0  </td><td>0    </td><td>0 </td><td>0    </td><td>0    </td><td>0    </td><td>0       </td><td>0   </td><td>0   </td><td>0   </td><td>0     </td><td>0       </td><td>0     </td><td>0   </td><td>0  </td><td>0          </td><td>0     </td><td>0      </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['enn'], 'ref_pos': ['ADP'], 'hyp': ['endomdagen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['om', 'dagen'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 30, ASD=0.11\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>natten</td><td>enn  </td><td>om   </td><td>dagen  </td><td>derfor</td><td>har  </td><td>natt </td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra  </td><td>[UNK]</td><td>de   </td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er </td><td>over </td><td>brus </td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er </td><td>der  </td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>natten</td><td>end  </td><td>##om </td><td>##dagen</td><td>derfor</td><td>har  </td><td>natt </td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra  </td><td>[UNK]</td><td>de   </td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er </td><td>over </td><td>brus </td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er </td><td>der  </td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.101 </td><td>0.577</td><td>0.522</td><td>0.319  </td><td>0.024 </td><td>0.005</td><td>0.005</td><td>0.002</td><td>0.002  </td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002   </td><td>0.002</td><td>0.002</td><td>0.008</td><td>0.005 </td><td>0.002   </td><td>0.001 </td><td>0.002</td><td>0.007</td><td>0.001      </td><td>0.001 </td><td>0.002  </td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['i'], 'ref_pos': ['SCONJ'], 'hyp': ['så'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['å', 'se'], 'ref_pos': ['PART', 'VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['president'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.15\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>å    </td><td>se   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>ønske    </td><td>at  </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en </td><td>og   </td><td>hans </td><td>likesinnede</td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>så   </td><td>så   </td><td>så   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>president</td><td>at  </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en </td><td>og   </td><td>hans </td><td>likesinnede</td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.015</td><td>0.535</td><td>0.611</td><td>0.468</td><td>0.114</td><td>0.037</td><td>0.022</td><td>0.015</td><td>0.014 </td><td>0.049</td><td>0.708    </td><td>0.04</td><td>0.019</td><td>0.032 </td><td>0.014</td><td>0.015 </td><td>0.011</td><td>0.007</td><td>0.018</td><td>0.016      </td><td>0.007</td><td>0.007</td><td>0.011</td><td>0.008 </td><td>0.009</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['se'], 'ref_pos': ['VERB'], 'hyp': ['så'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['president'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['like', 'siden'], 'hyp_pos': ['ADJ', 'SCONJ']}\n",
      "{'type': 'substitute', 'ref': ['likesinnede'], 'ref_pos': ['ADJ'], 'hyp': ['de'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.26\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>å    </td><td>se   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>ønske    </td><td>at   </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en</td><td>og   </td><td>hans </td><td>likesinnede</td><td>likesinnede</td><td>likesinnede</td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>å    </td><td>å    </td><td>så   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>president</td><td>at   </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en</td><td>og   </td><td>hans </td><td>like       </td><td>siden      </td><td>de         </td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.446</td><td>0.246</td><td>0.302</td><td>0.075</td><td>0.026</td><td>0.023</td><td>0.017</td><td>0.017 </td><td>0.061</td><td>0.698    </td><td>0.081</td><td>0.014</td><td>0.019 </td><td>0.007</td><td>0.012 </td><td>0.02</td><td>0.048</td><td>0.066</td><td>0.555      </td><td>0.802      </td><td>0.676      </td><td>0.094</td><td>0.061</td><td>0.037</td><td>0.018 </td><td>0.022</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['tilfellet'], 'ref_pos': ['NOUN'], 'hyp': ['tilfelle'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.01\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>påstår</td><td>at </td><td>det</td><td>blir</td><td>billigere</td><td>varer</td><td>og </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfellet</td><td>så   </td><td>må   </td><td>jo </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>påstår</td><td>at </td><td>det</td><td>blir</td><td>billigere</td><td>varer</td><td>og </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfelle </td><td>så   </td><td>må   </td><td>jo </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.0  </td><td>0.001 </td><td>0.0</td><td>0.0</td><td>0.0 </td><td>0.0      </td><td>0.0  </td><td>0.0</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.003</td><td>0.009</td><td>0.014</td><td>0.1      </td><td>0.003</td><td>0.001</td><td>0.0</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['påstår'], 'ref_pos': ['VERB'], 'hyp': ['påse'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['på'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['tilfellet'], 'ref_pos': ['NOUN'], 'hyp': ['tilfelle'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>påstår</td><td>at   </td><td>det  </td><td>blir </td><td>billigere</td><td>varer</td><td>og   </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfellet</td><td>så   </td><td>må   </td><td>jo   </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>påse  </td><td>at   </td><td>det  </td><td>blir </td><td>billigere</td><td>varer</td><td>på   </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfelle </td><td>så   </td><td>må   </td><td>jo   </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.294 </td><td>0.072</td><td>0.017</td><td>0.024</td><td>0.02     </td><td>0.044</td><td>0.526</td><td>0.091</td><td>0.035</td><td>0.009</td><td>0.006</td><td>0.012</td><td>0.016</td><td>0.103    </td><td>0.005</td><td>0.004</td><td>0.003</td><td>0.005</td><td>0.004</td><td>0.005</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 20\n",
    "\n",
    "for row in higher_asd_df.itertuples():\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  word  lemma\n",
      "0       -abel           -abel\n",
      "1       -abels          -abel\n",
      "2       -abelt          -abel\n",
      "3       -abelts         -abel\n",
      "4       -able           -abel\n",
      "...       ...             ...\n",
      "744437  jordskokkpuré   -    \n",
      "744438  rotgrønnsakene  -    \n",
      "744439  albóndigas      -    \n",
      "744440  gelatinplatene  -    \n",
      "744441  kyllinglårene   -    \n",
      "\n",
      "[744442 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/talebase/data/lex/Sprakbanken/NLB/nlb_nob_20181129.lex\"\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines:\n",
    "        lex_list = line.split(\"\\t\")\n",
    "        lemma = lex_list[13].split(\":\")[1].split(\"|\")[0]\n",
    "        word_list.append(lex_list[0])\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "lex_dict = {\"word\":word_list, \"lemma\":lemma_list}\n",
    "lex_df = pd.DataFrame(lex_dict)\n",
    "print(lex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origloss_NBTale = pd.read_csv(\"./logs/OrigLoss_V5/NBTale_results_wav2vec2_NO_origLossV5_2023-09-18.csv\")\n",
    "# origloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "# origloss_NBTale.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/OrigLoss_V5/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "origloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)\n",
    "    origloss_dfs.append(df)\n",
    "\n",
    "origloss = pd.concat(origloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customloss_NBTale = pd.read_csv(\"./logs/CustomLoss_V7/NBTale_results_wav2vec2_NO_customLossV7_2023-10-02.csv\")\n",
    "# customloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "# customloss_NBTale.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/CustomLoss_V7/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "customloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)\n",
    "    customloss_dfs.append(df)\n",
    "\n",
    "customloss = pd.concat(customloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged results of orig loss & custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pd.merge(origloss, customloss, on=\"ref_str\")\n",
    "merged_results = merged_results.reindex(columns=[\"segment_id\", \"ref_str\", \"asr_origloss\", \"wer_origloss\", \"asd_origloss\", \"asr_customloss\", \"wer_customloss\", \"asd_customloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer = merged_results[merged_results[\"wer_origloss\"] > merged_results[\"wer_customloss\"]]\n",
    "worse_wer = merged_results[merged_results[\"wer_origloss\"] < merged_results[\"wer_customloss\"]]\n",
    "better_asd = merged_results[merged_results[\"asd_origloss\"] > merged_results[\"asd_customloss\"]]\n",
    "worse_asd = merged_results[merged_results[\"asd_origloss\"] < merged_results[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better WER but *worse* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer[better_wer[\"asd_origloss\"] < better_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worse WER but *better* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_wer[worse_wer[\"asd_origloss\"] > worse_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_improved_percent = (len(better_wer)/len(merged_results)) * 100\n",
    "asd_improved_percent = (len(better_asd)/len(merged_results)) * 100\n",
    "\n",
    "wer_worsened_percent = (len(worse_wer)/len(merged_results)) * 100\n",
    "asd_worsened_percent = (len(worse_asd)/len(merged_results)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WER improved: {wer_improved_percent:.2f}%\")\n",
    "print(f\"ASD improved: {asd_improved_percent:.2f}%\")\n",
    "\n",
    "print(f\"WER worsened: {wer_worsened_percent:.2f}%\")\n",
    "print(f\"ASD worsened: {asd_worsened_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC LOSS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3535249/3756479965.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, \"Container\"):\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "import types\n",
    "from tabulate import tabulate\n",
    "from dtw import *\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1868.49it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank: 31\n",
      "reduction: mean\n",
      "zero inf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"blank:\", model.config.pad_token_id)\n",
    "print(\"reduction:\", model.config.ctc_loss_reduction)\n",
    "print(\"zero inf:\", model.config.ctc_zero_infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.04, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.047, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.047, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.055, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.047, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n",
      "32 <s>\n",
      "33 </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1688 [00:00<?, ?ex/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   3%|▎         | 50/1688 [00:00<00:03, 494.87ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   8%|▊         | 140/1688 [00:00<00:02, 729.49ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  15%|█▌        | 256/1688 [00:00<00:01, 923.91ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  22%|██▏       | 374/1688 [00:00<00:01, 1022.37ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  29%|██▉       | 492/1688 [00:00<00:01, 1078.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  36%|███▌      | 607/1688 [00:00<00:00, 1102.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  43%|████▎     | 725/1688 [00:00<00:00, 1125.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|████▉     | 841/1688 [00:00<00:00, 1134.06ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  57%|█████▋    | 959/1688 [00:00<00:00, 1147.61ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  64%|██████▎   | 1074/1688 [00:01<00:01, 565.97ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  72%|███████▏  | 1209/1688 [00:01<00:00, 705.40ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|███████▉  | 1343/1688 [00:01<00:00, 834.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  87%|████████▋ | 1476/1688 [00:01<00:00, 945.22ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 1688/1688 [00:01<00:00, 974.09ex/s] \n",
      "#3: 100%|██████████| 1687/1687 [00:01<00:00, 973.48ex/s] \n",
      "\n",
      "#2: 100%|██████████| 1687/1687 [00:01<00:00, 960.13ex/s] \n",
      "#0: 100%|██████████| 1688/1688 [00:01<00:00, 937.21ex/s] \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../model_ckpts/fine-tuning_wav2vec2_expectedASD2_3ep_0p01_Rundkast_aulus6/runs/train_set.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.cast_column(\"path\", Audio())\n",
    "train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_feature = [{\"input_values\": train_dataset[73][\"input_values\"]}]\n",
    "label_feature = [{\"input_ids\": train_dataset[73][\"labels\"]}]\n",
    "\n",
    "# input_feature = [{\"input_values\": input_values} for input_values in train_dataset[34:41][\"input_values\"]]\n",
    "# label_feature = [{\"input_ids\": labels} for labels in train_dataset[34:41][\"labels\"]]\n",
    "\n",
    "batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "batch[\"labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch[\"labels\"][0]\n",
    "labels_mask = labels >= 0\n",
    "flattened_targets = labels.masked_select(labels_mask)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166, 34])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_output.pkl', 'wb') as file:\n",
    "    pickle.dump(output, file)\n",
    "\n",
    "with open('sample_input.pkl', 'wb') as file:\n",
    "    pickle.dump(batch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis):\n",
    "    ref_text = reference.replace(\"[UNK]\", \"\")  # removes the [UNK] token in the reference text, observed during training\n",
    "    hyp_text = hypothesis.replace(\"[UNK]\", \"\")\n",
    "    tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                                hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                                hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(output_mean_reference)\n",
    "    asd_score = alignment.distance / num_tokens\n",
    "    return asd_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import cuda_ctc_decoder, ctc_decoder, download_pretrained_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model.wav2vec2.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'å', 'æ', 'ø', '[UNK]', '[PAD]', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = [None] * 34\n",
    "\n",
    "for key in processor.tokenizer.vocab:\n",
    "    tokens[processor.tokenizer.vocab[key]] = key\n",
    "tokens[32] = \"</s>\"\n",
    "tokens[33] = \"<s>\"\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# f = open(\"tokens.txt\", \"w\")\n",
    "# for i, item in enumerate(tokens):\n",
    "#     if i == (len(tokens) - 1):\n",
    "#         f.write(item)\n",
    "#     else:\n",
    "#         f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 34])\n",
      "torch.Size([1, 34])\n",
      "tensor([[ 5, 18,  0, 22,  9,  0, 19, 20,  1,  4,  9,  7,  0, 14,  5,  4,  5,  0,\n",
      "         15,  7,  0, 11, 21, 18, 19,  5, 18,  0,  4,  9, 19, 19,  5,  0]])\n",
      "['er vi stadig nede og kurser dise']\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# output = model(**batch)\n",
    "# logits = output[\"logits\"]\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# print(\"LABEL:\", processor_woLM.batch_decode(batch[\"labels\"]), \"\\n\")\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# log_probs = F.log_softmax(logits[0], dim=-1, dtype=torch.float32).to(device)\n",
    "# encoder_out_lens = torch.tensor(log_probs.shape[0], device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "# cuda_decoder = cuda_ctc_decoder(tokens, nbest=10, beam_size=10, blank_skip_threshold=0.95)\n",
    "# results = cuda_decoder(log_probs, encoder_out_lens)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: ikke minst er det viktig i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig\n",
      "idx: 6 \thyp:  ikke minst er det viktige i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n",
      "idx: 0 \thyp:  ikke minst er det viktige næringe som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# characters = model_tokenizer.convert_ids_to_tokens(batch[\"labels\"][0])\n",
    "# ref_text = re.sub(\" +\", \" \", \"\".join(characters).replace(\"|\", \" \"))\n",
    "# print(\"LABEL:\", ref_text, \"\\n\")\n",
    "\n",
    "# lm=\"./language_model/\"\n",
    "decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=50, blank_token=\"[PAD]\",\n",
    "                      sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "results = decoder(logits)\n",
    "\n",
    "asd_score_list = [0] * len(results[0])\n",
    "beam_score_list = [0] * len(results[0])\n",
    "hyp_list = []\n",
    "for i, item in enumerate(results[0]):\n",
    "    # print(item.tokens.shape, item.tokens)\n",
    "    chars = decoder.idxs_to_tokens(item.tokens)\n",
    "    hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "    hyp_list.append(hyp_text)\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    asd_score_list[i] = asd_score\n",
    "    beam_score_list[i] = item.score\n",
    "\n",
    "lowest_asd_idx = np.argmin(asd_score_list)\n",
    "highest_beam_score_idx = np.argmax(beam_score_list)\n",
    "\n",
    "print(\"idx:\", lowest_asd_idx, \"\\thyp:\", hyp_list[lowest_asd_idx])\n",
    "print(\"idx:\", highest_beam_score_idx, \"\\thyp:\", hyp_list[highest_beam_score_idx])\n",
    "\n",
    "nbest_loss = torch.tensor([beam_score_list[lowest_asd_idx]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4052.2563], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=nbest_loss, inputs=logits, grad_outputs=nbest_loss, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of log_prob: torch.Size([1, 175, 500]), the shape of encoder_out_lens: torch.Size([1])\n",
      "tensor([175], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "from torchaudio.models.decoder import cuda_ctc_decoder\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio\n",
    "\n",
    "def download_asset_external(url, key):\n",
    "    path = Path(torch.hub.get_dir()) / \"torchaudio\" / Path(key)\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.hub.download_url_to_file(url, path)\n",
    "    return str(path)\n",
    "\n",
    "url_prefix = \"https://huggingface.co/Zengwei/icefall-asr-librispeech-pruned-transducer-stateless7-ctc-2022-12-01\"\n",
    "model_link = f\"{url_prefix}/resolve/main/exp/cpu_jit.pt\"\n",
    "model_path = download_asset_external(model_link, \"cuda_ctc_decoder/cpu_jit.pt\")\n",
    "\n",
    "speech_file = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n",
    "waveform, sample_rate = torchaudio.load(speech_file)\n",
    "\n",
    "actual_transcript = \"i really was very much afraid of showing him how much shocked i was at some parts of what he said\"\n",
    "actual_transcript = actual_transcript.split()\n",
    "\n",
    "device = torch.device(\"cuda\", 0)\n",
    "acoustic_model = torch.jit.load(model_path)\n",
    "acoustic_model.to(device)\n",
    "acoustic_model.eval()\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "feat = torchaudio.compliance.kaldi.fbank(waveform, num_mel_bins=80, snip_edges=False)\n",
    "feat = feat.unsqueeze(0)\n",
    "feat_lens = torch.tensor(feat.size(1), device=device).unsqueeze(0)\n",
    "\n",
    "encoder_out, encoder_out_lens = acoustic_model.encoder(feat, feat_lens)\n",
    "nnet_output = acoustic_model.ctc_output(encoder_out)\n",
    "log_prob = torch.nn.functional.log_softmax(nnet_output, -1)\n",
    "\n",
    "print(f\"The shape of log_prob: {log_prob.shape}, the shape of encoder_out_lens: {encoder_out_lens.shape}\")\n",
    "print(encoder_out_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimum ASD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvrtc.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mk2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec/lib/python3.9/site-packages/k2/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     k2_torch_cuda_version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m!=\u001b[39m k2_torch_cuda_version\n\u001b[1;32m     17\u001b[0m ):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk2 was built using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk2_torch_cuda_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut you are using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to run it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeterminizeWeightPushingType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_ragged_index_select\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swoosh_l\n",
      "\u001b[0;31mImportError\u001b[0m: libnvrtc.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import k2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lattice(\n",
    "    nnet_output: torch.Tensor,\n",
    "    decoding_graph: k2.Fsa,\n",
    "    supervision_segments: torch.Tensor,\n",
    "    search_beam: float,\n",
    "    output_beam: float,\n",
    "    min_active_states: int,\n",
    "    max_active_states: int,\n",
    "    subsampling_factor: int = 1,\n",
    ") -> k2.Fsa:\n",
    "    \"\"\"Get the decoding lattice from a decoding graph and neural\n",
    "    network output.\n",
    "    Args:\n",
    "      nnet_output:\n",
    "        It is the output of a neural model of shape `(N, T, C)`.\n",
    "      decoding_graph:\n",
    "        An Fsa, the decoding graph. It can be either an HLG\n",
    "        (see `compile_HLG.py`) or an H (see `k2.ctc_topo`).\n",
    "      supervision_segments:\n",
    "        A 2-D **CPU** tensor of dtype `torch.int32` with 3 columns.\n",
    "        Each row contains information for a supervision segment. Column 0\n",
    "        is the `sequence_index` indicating which sequence this segment\n",
    "        comes from; column 1 specifies the `start_frame` of this segment\n",
    "        within the sequence; column 2 contains the `duration` of this\n",
    "        segment.\n",
    "      search_beam:\n",
    "        Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "        (less pruning). This is the default value; it may be modified by\n",
    "        `min_active_states` and `max_active_states`.\n",
    "      output_beam:\n",
    "         Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "         to best path of output.\n",
    "      min_active_states:\n",
    "        Minimum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to have fewer than this number active.\n",
    "        Set it to zero if there is no constraint.\n",
    "      max_active_states:\n",
    "        Maximum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to exceed that but may not always succeed.\n",
    "        You can use a very large number if no constraint is needed.\n",
    "      subsampling_factor:\n",
    "        The subsampling factor of the model.\n",
    "    Returns:\n",
    "      An FsaVec containing the decoding result. It has axes [utt][state][arc].\n",
    "    \"\"\"\n",
    "    dense_fsa_vec = k2.DenseFsaVec(\n",
    "        nnet_output,\n",
    "        supervision_segments,\n",
    "        allow_truncate=subsampling_factor - 1,\n",
    "    )\n",
    "\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        decoding_graph,\n",
    "        dense_fsa_vec,\n",
    "        search_beam=search_beam,\n",
    "        output_beam=output_beam,\n",
    "        min_active_states=min_active_states,\n",
    "        max_active_states=max_active_states,\n",
    "    )\n",
    "\n",
    "    return lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWERLoss(torch.nn.Module):\n",
    "    '''Minimum Word Error Rate Loss compuration in k2.\n",
    "\n",
    "    See equation 2 of https://arxiv.org/pdf/2106.02302.pdf about its definition.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        subsampling_factor: int,\n",
    "        search_beam: int = 20,\n",
    "        output_beam: int = 8,\n",
    "        min_active_states: int = 30,\n",
    "        max_active_states: int = 10000,\n",
    "        temperature: float = 1.0,\n",
    "        num_paths: int = 100,\n",
    "        use_double_scores: bool = True,\n",
    "        nbest_scale: float = 0.5,\n",
    "        reduction: Literal['none', 'mean', 'sum'] = 'sum'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          search_beam:\n",
    "            Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "            (less pruning). This is the default value; it may be modified by\n",
    "            `min_active_states` and `max_active_states`.\n",
    "          output_beam:\n",
    "             Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "             to best path of output.\n",
    "          min_active_states:\n",
    "            Minimum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to have fewer than this number active.\n",
    "            Set it to zero if there is no constraint.\n",
    "          max_active_states:\n",
    "            Maximum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to exceed that but may not always succeed.\n",
    "            You can use a very large number if no constraint is needed.\n",
    "          subsampling_factor:\n",
    "            The subsampling factor of the model.\n",
    "          temperature:\n",
    "            For long utterances, the dynamic range of scores will be too large\n",
    "            and the posteriors will be mostly 0 or 1.\n",
    "            To prevent this it might be a good idea to have an extra argument\n",
    "            that functions like a temperature.\n",
    "            We scale the logprobs by before doing the normalization.\n",
    "          use_double_scores:\n",
    "            True to use double precision floating point.\n",
    "            False to use single precision.\n",
    "          reduction:\n",
    "            Specifies the reduction to apply to the output:\n",
    "            'none' | 'sum' | 'mean'.\n",
    "            'none': no reduction will be applied.\n",
    "                    The returned 'loss' is a k2.RaggedTensor, with\n",
    "                    loss.tot_size(0) == batch_size.\n",
    "                    loss.tot_size(1) == total_num_paths_of_current_batch\n",
    "                    If you want the MWER loss for each utterance, just do:\n",
    "                    `loss_per_utt = loss.sum()`\n",
    "                    Then loss_per_utt.shape[0] should be batch_size.\n",
    "                    See more example usages in 'k2/python/tests/mwer_test.py'\n",
    "            'sum': sum loss of each path over the whole batch together.\n",
    "            'mean': divide above 'sum' by total num paths over the whole batch.\n",
    "          nbest_scale:\n",
    "            Scale `lattice.score` before passing it to :func:`k2.random_paths`.\n",
    "            A smaller value leads to more unique paths at the risk of being not\n",
    "            to sample the path with the best score.\n",
    "          num_paths:\n",
    "            Number of paths to **sample** from the lattice\n",
    "            using :func:`k2.random_paths`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.search_beam = search_beam\n",
    "        self.output_beam = output_beam\n",
    "        self.min_active_states = min_active_states\n",
    "        self.max_active_states = max_active_states\n",
    "\n",
    "        self.num_paths = num_paths\n",
    "        self.nbest_scale = nbest_scale\n",
    "        self.subsampling_factor = subsampling_factor\n",
    "\n",
    "        self.mwer_loss = k2.MWERLoss(\n",
    "            temperature=temperature,\n",
    "            use_double_scores=use_double_scores,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        emissions_lengths: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_length: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
    "                probability distribution over labels; output of acoustic model.\n",
    "            labels (torch.FloatTensor): CPU tensor of shape `(batch, label_len)` storing labels.\n",
    "            emissions_lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                in time axis of the output Tensor in each batch.\n",
    "            labels_length (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                label in each batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor:\n",
    "                Minimum Word Error Rate loss.\n",
    "        \"\"\"\n",
    "        H = k2.ctc_topo(\n",
    "            max_token=self.vocab_size-1,\n",
    "            modified=False,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "\n",
    "        supervision_segments = torch.stack(\n",
    "            (\n",
    "                torch.tensor(range(emissions_lengths.shape[0])),\n",
    "                torch.zeros(emissions_lengths.shape[0]),\n",
    "                emissions_lengths.cpu(),\n",
    "            ),\n",
    "            1,\n",
    "        ).to(torch.int32)\n",
    "\n",
    "        lattice = get_lattice(\n",
    "            nnet_output=emissions,\n",
    "            decoding_graph=H,\n",
    "            supervision_segments=supervision_segments,\n",
    "            search_beam=self.search_beam,\n",
    "            output_beam=self.output_beam,\n",
    "            min_active_states=self.min_active_states,\n",
    "            max_active_states=self.max_active_states,\n",
    "            subsampling_factor=self.subsampling_factor,\n",
    "        )\n",
    "\n",
    "        token_ids = []\n",
    "        for i in range(labels_length.size(0)):\n",
    "            token_ids.append(labels[i, : labels_length[i]].cpu().tolist())\n",
    "\n",
    "        loss = self.mwer_loss(\n",
    "            lattice, token_ids,\n",
    "            nbest_scale=self.nbest_scale,\n",
    "            num_paths=self.num_paths\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Gumbel Softmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "asd_scores = [0] * 10\n",
    "for i in range(10):\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=10, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_scores[i] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    print(\"asd:\", asd_scores[i], \"hyp:\", hyp_text)\n",
    "\n",
    "mean_asd = np.mean(asd_scores)\n",
    "loss[0] = mean_asd\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "def get_mass_prob(paths: Tensor,\n",
    "                        softmax_ctc: Tensor,\n",
    "                        model_pred_length: Tensor,\n",
    "                        eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    compute the path probability mass\n",
    "    :param paths: ctc alignments\n",
    "    :param softmax_ctc: model logits after softmax\n",
    "    :param model_pred_length:  max length of all given paths\n",
    "    :return: avg of the paths probability\n",
    "    \"\"\"\n",
    "    log_indexes_probs = softmax_ctc.gather(dim=1, index=paths) + eps\n",
    "    if len(log_indexes_probs) > model_pred_length:\n",
    "        log_indexes_probs[model_pred_length:, :] = torch.zeros((log_indexes_probs.shape[0] - model_pred_length, 0))\n",
    "    return torch.sum(log_indexes_probs, dim=0) / (model_pred_length.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: pågripelsen av toska gjorde altså denne pågripelsen mulig\n",
      "[0.037175211785556765, 0.037175211785556765, 0.1520948636867659, 0.24277221896598888]\n",
      "['pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåskha gjorde altså denne pågripelsen mulig']\n",
      "0\n",
      "tensor([0.2708, 0.1336, 0.1375], grad_fn=<CopySlices>)\n",
      "tensor(0.5420, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "input_lengths = model._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(output[\"logits\"][0], dim=-1, dtype=torch.float32)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "num_samples = 10\n",
    "non_zero_logits = []\n",
    "non_zero_asd = []\n",
    "non_zero_hyp = []\n",
    "# for i in range(num_samples):\n",
    "while len(non_zero_asd) < 4:\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=100, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    if asd_score != 0:\n",
    "        non_zero_logits.append(sampled_logits)\n",
    "        non_zero_asd.append(asd_score)\n",
    "        non_zero_hyp.append(hyp_text)\n",
    "\n",
    "print(non_zero_asd)\n",
    "print(non_zero_hyp)\n",
    "\n",
    "lowest_asd_idx = np.argmin(np.array(non_zero_asd))\n",
    "print(lowest_asd_idx)\n",
    "\n",
    "mass_prob_list = []\n",
    "for i in range(len(non_zero_asd)):\n",
    "    logits_selected = non_zero_logits[i].type(torch.LongTensor)\n",
    "    mass_prob_list.append(get_mass_prob(logits_selected, log_probs, input_lengths))\n",
    "\n",
    "loss = torch.zeros((len(mass_prob_list)-1))\n",
    "j=0\n",
    "for i in range(len(mass_prob_list)):\n",
    "    if i != lowest_asd_idx:\n",
    "        subtract_path_probs = mass_prob_list[i] - mass_prob_list[lowest_asd_idx]\n",
    "        loss[j] = torch.sum(torch.clamp((subtract_path_probs), min=0))\n",
    "        j += 1\n",
    "print(loss)\n",
    "print(torch.sum(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating aligned cosdist (1 batch, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    ref_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "\n",
    "    return ref_alignments\n",
    "\n",
    "\n",
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed\n",
    "\n",
    "\n",
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    return cosdist_for_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_register = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    input_feature = [{\"input_values\": input_values} for input_values in train_dataset[i*8:(i+1)*8][\"input_values\"]]\n",
    "    label_feature = [{\"input_ids\": labels} for labels in train_dataset[i*8:(i+1)*8][\"labels\"]]\n",
    "\n",
    "    batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "    label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = label\n",
    "\n",
    "    output = model(**batch)\n",
    "    output_logits = output[\"logits\"]\n",
    "    pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "    labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "    for i in range(len(pred_str)):\n",
    "        ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "        pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "        label_ids = labels[i]\n",
    "        labels_mask = label_ids >= 0\n",
    "        flattened_labels = label_ids.masked_select(labels_mask)\n",
    "        ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "        tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "        cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "        cosdist_register.append(cosdist_for_ctc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_976418/112888293.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cosdist_register_arr = np.asarray(cosdist_register)\n"
     ]
    }
   ],
   "source": [
    "cosdist_register_arr = np.asarray(cosdist_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(cosdist_register_arr.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    numphones = params.shape[0]  # number of labels, not even used in the code\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    # print(seqLen, numphones, L, T)\n",
    "\n",
    "    alphas = np.zeros((L,T))\n",
    "    betas = np.zeros((L,T))\n",
    "\n",
    "    # convert logits to log softmax\n",
    "    params = params - np.max(params,axis=0)\n",
    "    params = np.exp(params)\n",
    "    params = params / np.sum(params,axis=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = np.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = np.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = np.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward += np.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = np.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = np.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = np.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward += np.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = np.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = np.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = np.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or np.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(np.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_tensor(logits, seq, cosdist_for_ctc, blank=0):\n",
    "\n",
    "    cosdist_contribution_alphas = []\n",
    "    cosdist_contribution_betas = []\n",
    "\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    # print(\"label seq:\", seqLen)\n",
    "    # print(\"label seq length with blanks:\", L)\n",
    "    # print(\"utterance length:\", T)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # print(0, L-2*(T-t), \"time:\", t, \"start:\", start, \"L:\", L, \"s:\", s, \"l:\", l)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    # llDiff = torch.abs(llForward-llBackward)\n",
    "    # if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "    #     print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "    #     print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "    #     return (-llForward, grad)\n",
    "    # else:\n",
    "    #     grad = params - grad / (params * absum)\n",
    "    #     return (-llForward, grad)\n",
    "\n",
    "    for t in range(T):\n",
    "        for s in range(numphones):\n",
    "            tmp = (params[s,t]*absum[t])\n",
    "            if tmp > 0:\n",
    "                grad[s,t] = params[s,t] - grad[s,t] / tmp\n",
    "            else:\n",
    "                grad[s,t] = params[s,t]\n",
    "\n",
    "    return (-llForward, grad, sum(cosdist_contribution_alphas), sum(cosdist_contribution_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_newgrad(logits, seq, blank=0):\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient according to https://github.com/yehudabab/NumpyCTC/blob/main/ctc.py\n",
    "    padded_labels = torch.zeros((L))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(L):\n",
    "        if i%2 == 0:\n",
    "            padded_labels[i] = 0\n",
    "        else:\n",
    "            padded_labels[i] = seq[j]\n",
    "            j += 1\n",
    "\n",
    "    # print(len(seq), seq)\n",
    "    # print(len(padded_labels), padded_labels)\n",
    "\n",
    "    grad = torch.zeros(params.shape)\n",
    "\n",
    "    score_last = alphas[L-1, T-1]\n",
    "    score_before_last = betas[L-2, T-1]\n",
    "    p_l_given_ctc = score_last + score_before_last\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(numphones):\n",
    "            d_p_d_ytk = 0\n",
    "            lb_lk = np.nonzero(list(map(lambda x: 1 if k in x else 0, padded_labels)))[0]\n",
    "            for s in lb_lk:\n",
    "                d_p_d_ytk += alphas[s, t] * betas[s, t]\n",
    "\n",
    "            d_p_d_ytk /= (params[k, t] ** 2)\n",
    "            d_lnp_d_ytk = (1. / p_l_given_ctc) * d_p_d_ytk\n",
    "            grad[k, t] = d_lnp_d_ytk\n",
    "\n",
    "    return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Stanford CTC Loss script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].detach().cpu().numpy().transpose()\n",
    "seq = batch[\"labels\"][0].detach().cpu().numpy()\n",
    "\n",
    "loss, grad = ctc_loss(logits, seq.squeeze(), blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(output[\"logits\"][0], dim=1)\n",
    "print(token_ids)\n",
    "for token in token_ids:\n",
    "    if token == 33:\n",
    "        print(\"HUY\")\n",
    "\n",
    "decoded_tokens = []\n",
    "for token in token_ids:\n",
    "    decoded_tokens.append(model_tokenizer.decode(token))\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "targets = batch[\"labels\"][0]\n",
    "input_lengths = torch.tensor(output[\"logits\"][0].shape[0])\n",
    "target_lengths = torch.tensor(batch[\"labels\"][0].shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=targets, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].transpose(1,0)\n",
    "seq = batch[\"labels\"][0]\n",
    "\n",
    "loss, grad = ctc_loss_tensor(logits, seq, blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad.shape)\n",
    "transposed_grad = grad.transpose(1,0)\n",
    "print(transposed_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad.shape[0]):\n",
    "    print(i, grad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosdist alphas: 845.0103612840176\n",
      "cosdist betas: 972.7007383406162\n",
      "torch.Size([201, 34]) tensor(608.0905, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1854.4355, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 606.3643108587712\n",
      "cosdist betas: 550.9175247717649\n",
      "torch.Size([201, 34]) tensor(680.7924, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1999.4266, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1113.2689, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 307.15736174583435\n",
      "cosdist betas: 318.57188880443573\n",
      "torch.Size([201, 34]) tensor(2163.9688, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 2056.844472726263\n",
      "cosdist betas: 1902.4644071857585\n",
      "torch.Size([201, 34]) tensor(860.0428, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_to_inspect = []\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    loss, grad, cosdist_contribution_alphas, cosdist_contribution_betas = ctc_loss_tensor(logits, flattened_labels, cosdist_for_ctc)\n",
    "    grad_to_inspect.append(grad)\n",
    "\n",
    "    print(\"cosdist alphas:\", cosdist_contribution_alphas)\n",
    "    print(\"cosdist betas:\", cosdist_contribution_betas)\n",
    "    print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for logits in output_logits:\n",
    "    token_ids = torch.argmax(logits, dim=1)\n",
    "    print(token_ids)\n",
    "\n",
    "    decoded_tokens = []\n",
    "    for token in token_ids:\n",
    "        decoded_tokens.append(model_tokenizer.decode(token))\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing only the first utterance\n",
    "model.train()\n",
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "logits = output_logits[0]\n",
    "label_ids = labels[0]\n",
    "labels_mask = label_ids >= 0\n",
    "flattened_labels = label_ids.masked_select(labels_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([235, 34])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "# python-implemented CTC loss\n",
    "loss, grad = ctc_loss_tensor(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "loss, grad = ctc_loss_newgrad(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6440, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pytorch CTC loss\n",
    "import torch.nn.functional as F\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = labels_mask.sum(-1)\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=flattened_labels, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.grad)\n",
    "#         print(param.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating ASD into CTC Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: start running here! (when loading from pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOV-8 DATA\n",
    "# ref = \"wozniackis drøm om seier knust av russiske jekaterina\"\n",
    "# hyp = \"asnake drøm om sier knut av russiske karine\"\n",
    "\n",
    "# logit_frames_decoded = pickle.load(open(\"logit_frames_decoded_8Nov.pkl\", \"rb\"))\n",
    "\n",
    "# alignment_table = [logit_frames_decoded]\n",
    "# table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "# print(\"logits decoded\")\n",
    "# display(HTML(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    cosdist_alignment_tokens = []  # FOR PRINTING\n",
    "    ref_alignments = []\n",
    "    # hyp_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        cosdist_alignment_tokens.append(round(cosdist, 3))  # FOR PRINTING\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        # hyp_token = asd_tokenizer.convert_ids_to_tokens(hyp_input_ids[hyp_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "        # hyp_alignments.append((hyp_alignment_idxs[i], hyp_token, cosdist))\n",
    "\n",
    "    # FOR PRINTING\n",
    "    # ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(ref_alignment_idxs):\n",
    "    #     ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    # ref_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(ref_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(hyp_alignment_idxs):\n",
    "    #     hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    # hyp_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(hyp_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    # ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    # cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    # alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    # table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    # print(\"Token alignment table:\")\n",
    "    # display(HTML(table))\n",
    "\n",
    "    return ref_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    # print(tokens_compressed)\n",
    "    # if len(label_ids) == len(cosdist_for_ctc):\n",
    "    #     print(\"SAME\")\n",
    "    # else:\n",
    "    #     print(\"DIFF!\")\n",
    "    # for i, label in enumerate(label_ids):\n",
    "    #     print(label, cosdist_for_ctc[i])\n",
    "\n",
    "    # try normalizing\n",
    "    x = np.array(cosdist_for_ctc)\n",
    "    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    if np.isnan(np.sum(x_norm)):\n",
    "        print(\"boo!\")\n",
    "\n",
    "    return cosdist_for_ctc, x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07505041360855103, 0.07505041360855103, 0.07505041360855103, 0, 0.12715423107147217, 0.12715423107147217, 0.12715423107147217, 0, 0.029700636863708496, 0, 0.13597166538238525, 0.13597166538238525, 0, 0.5323134958744049, 0.5323134958744049, 0, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0, 0.6916015446186066, 0.6916015446186066, 0, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0, 0, 0.032095909118652344, 0.032095909118652344, 0, 0.018728435039520264, 0.018728435039520264, 0.018728435039520264, 0, 0.016046226024627686, 0.016046226024627686, 0, 0.016792714595794678, 0.016792714595794678, 0, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414]\n",
      "[0.10851684 0.10851684 0.10851684 0.         0.18385475 0.18385475\n",
      " 0.18385475 0.         0.04294472 0.         0.19660405 0.19660405\n",
      " 0.         0.76968234 0.76968234 0.         0.30526938 0.30526938\n",
      " 0.30526938 0.30526938 0.         0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.0306659  0.0306659  0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.         0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.         0.03278869 0.03278869 0.03278869\n",
      " 0.03278869 0.         1.         1.         0.         0.22142421\n",
      " 0.22142421 0.22142421 0.22142421 0.         0.         0.04640809\n",
      " 0.04640809 0.         0.02707981 0.02707981 0.02707981 0.\n",
      " 0.02320155 0.02320155 0.         0.02428091 0.02428091 0.\n",
      " 0.02478112 0.02478112 0.02478112 0.02478112 0.         0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4022911/3475542975.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007969975471496582, 0.007969975471496582, 0.007969975471496582, 0, 0.006086826324462891, 0.006086826324462891, 0, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0, 0.015287041664123535, 0.015287041664123535, 0, 0.0193021297454834, 0.0193021297454834, 0, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144]\n",
      "[0.02681489 0.02681489 0.02681489 0.         0.02047905 0.02047905\n",
      " 0.         0.02253498 0.02253498 0.02253498 0.02253498 0.02253498\n",
      " 0.         0.05143307 0.05143307 0.         0.06494178 0.06494178\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "[0.03979825973510742, 0.03979825973510742, 0.03979825973510742, 0, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0, 0.01816505193710327, 0.01816505193710327, 0.01816505193710327, 0, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0]\n",
      "[0.22152116 0.22152116 0.22152116 0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.         0.10110853 0.10110853 0.10110853 0.         0.04921078\n",
      " 0.04921078 0.04921078 0.04921078 0.04921078 0.        ]\n",
      "[0.1013750433921814, 0.1013750433921814, 0, 0.3244396448135376, 0.3244396448135376, 0.3244396448135376, 0, 0.03615701198577881, 0, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0, 0.018376052379608154, 0.018376052379608154, 0.018376052379608154, 0, 0.032556354999542236, 0, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0, 0.6256595551967621, 0.6256595551967621, 0, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566]\n",
      "[0.16202908 0.16202908 0.         0.51855621 0.51855621 0.51855621\n",
      " 0.         0.05779023 0.         0.03629764 0.03629764 0.03629764\n",
      " 0.03629764 0.03629764 0.         0.02937069 0.02937069 0.02937069\n",
      " 0.         0.05203526 0.         0.03731947 0.03731947 0.03731947\n",
      " 0.03731947 0.         1.         1.         0.         0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.         0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449]\n"
     ]
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    max_per_frame = torch.argmax(logits, dim=1)\n",
    "    relevant_frames = []\n",
    "    for max_id in max_per_frame:\n",
    "        if max_id in flattened_labels:\n",
    "            relevant_frames.append(1)\n",
    "        else:\n",
    "            relevant_frames.append(0)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc, x_norm = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    print(cosdist_for_ctc)\n",
    "    print(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CTC with ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCORPORATING ASD COSDIST VALUES TO THE CTC CALCULATION\n",
    "\n",
    "def ctc_loss_with_ASD(params, seq, cosdist_for_ctc, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0].clone() / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])  # scale 0 to 1\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone() + alphas[s-2,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t].clone() / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1].clone() / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone() + betas[s+2,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t].clone() / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = torch.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out custom ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import gradcheck\n",
    "\n",
    "# output = model(**batch)\n",
    "# ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "# label_ids = batch[\"labels\"][0]\n",
    "# logits = output[\"logits\"][0]\n",
    "# hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "# ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "# tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "# cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "# inputs = (logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "\n",
    "# test = gradcheck(ctc_loss_with_ASD, inputs)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "print(tokens_compressed)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "loss, grad = ctc_loss_with_ASD(logits.transpose(1,0), label_ids, cosdist_for_ctc, blank=0)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = torch.tensor(label_ids.shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=label_ids, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extending torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCTC(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, seq, cosdist_for_ctc, blank=0):\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "        T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "        alphas = torch.zeros((L,T)).double()\n",
    "        betas = torch.zeros((L,T)).double()\n",
    "\n",
    "        # convert logits to log probs\n",
    "        params = params - (torch.max(params, dim=0)[0])\n",
    "        params = torch.exp(params)\n",
    "        params = params / torch.sum(params, dim=0)\n",
    "\n",
    "        # initialize alphas and forward pass\n",
    "        alphas[0,0] = params[blank,0]\n",
    "        alphas[1,0] = params[seq[0],0]\n",
    "        c = torch.sum(alphas[:,0])\n",
    "        alphas[:,0] = alphas[:,0] / c\n",
    "        llForward = torch.log(c)\n",
    "\n",
    "        for t in range(1,T):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(start,L):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s==0:\n",
    "                        alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                    else:\n",
    "                        alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == 1 or seq[l] == seq[l-1]:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time (prevent underflow)\n",
    "            c = torch.sum(alphas[start:end,t])\n",
    "            alphas[start:end,t] = alphas[start:end,t] / c\n",
    "            llForward = llForward + torch.log(c)\n",
    "\n",
    "        # initialize betas and backwards pass\n",
    "        betas[-1,-1] = params[blank,-1]\n",
    "        betas[-2,-1] = params[seq[-1],-1]\n",
    "        c = torch.sum(betas[:,-1])\n",
    "        betas[:,-1] = betas[:,-1] / c\n",
    "        llBackward = torch.log(c)\n",
    "\n",
    "        for t in range(T-2,-1,-1):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(end-1,-1,-1):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s == L-1:\n",
    "                        betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                    else:\n",
    "                        betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time\n",
    "            c = torch.sum(betas[start:end,t])\n",
    "            betas[start:end,t] = betas[start:end,t] / c\n",
    "            llBackward = llBackward + torch.log(c)\n",
    "\n",
    "        ctx.save_for_backward(params, seq, alphas, betas, llForward, llBackward)\n",
    "\n",
    "        return -llForward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        params, seq, alphas, betas, llForward, llBackward = ctx.saved_tensors\n",
    "        blank = 0\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "\n",
    "        # Compute gradient with respect to unnormalized input parameters\n",
    "        grad = torch.zeros(params.shape)\n",
    "        ab = alphas*betas\n",
    "        for s in range(L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/params[blank,:]\n",
    "            else:\n",
    "                grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "        absum = torch.sum(ab,axis=0)\n",
    "\n",
    "        llDiff = torch.abs(llForward-llBackward)\n",
    "        if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "            return (grad, None, None)\n",
    "        else:\n",
    "            grad = params - grad / (params * absum)\n",
    "            return (grad, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "print(tokens_compressed)\n",
    "\n",
    "myctcloss = MyCTC.apply\n",
    "loss = myctcloss(logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS w/ FRAME NOS. & DISREGARDING ALL THE PADS\n",
    "\n",
    "clean_logit_frames = []\n",
    "for i, item in enumerate(logit_frames_decoded):\n",
    "    if item != \"[PAD]\":\n",
    "        clean_logit_frames.append((i, item))\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(clean_logit_frames):\n",
    "    if i == 0:\n",
    "        if char[1] == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if char[1] != \"\":\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if clean_logit_frames[i-1][1] == \"\" and char[1] != \"\":  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "            elif char[1] != \"\":\n",
    "                new_word.append(char)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(\" \".join([char[1] for char in item]))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS MATRIX INTO THEIR CORRESPONDING WORDS, WHILE RETAINING ROW NOS. OR FRAME NOS.\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(logit_frames_decoded):\n",
    "    if i == 0:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if logit_frames_decoded[i-1] in [\"\", \"[PAD]\"] and char not in [\"\", \"[PAD]\"]:\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if \"\" in logit_frames_decoded[i-2:i-1] and char not in [\"\", \"[PAD]\"]:  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "            elif logit_frames_decoded[i-1] == char and char != \"\":  # repeating characters\n",
    "                new_word.append((i, char))\n",
    "            elif logit_frames_decoded[i-1] != char and char != \"\":  # new character detected\n",
    "                new_word.append((i, char))\n",
    "\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # detecting applicable frames & caclulating average cosdist\n",
    "        # frame_group = []\n",
    "        # for token_char in token_char_list:\n",
    "        #     detect_count = 0\n",
    "        #     if token_char in [x[1] for x in char_group_copy]:\n",
    "        #         for i, char in enumerate(char_group_copy):\n",
    "        #             # print(char[1], token_char, [x[1] for x in char_group_copy])\n",
    "        #             if char[1] == token_char:\n",
    "        #                 frame_group.append(char)\n",
    "        #                 detect_count += 1\n",
    "        #             else:\n",
    "        #                 if detect_count > 0:\n",
    "        #                     for count in range(detect_count):\n",
    "        #                         char_group_copy.pop(0)\n",
    "        #                     break\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # cosdist_divided = token_cosdist / len(frame_group)  # divide ASD token cosdist by number of frames for word\n",
    "        # assign divided ASD token cosdist\n",
    "        # frame_cosdist = []\n",
    "        # for char in frame_group:\n",
    "        #     if char[1] in token_char_list:\n",
    "        #         frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING THE ASD COSDIST PER NO. FRAMES\n",
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
