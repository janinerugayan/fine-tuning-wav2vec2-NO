{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbest_log_distribution: torch.Size([3, 8]) tensor([[ -86.1507,  -34.0924, -141.7790,  -31.1852,  -26.1941, -140.0957,\n",
      "          -73.2255, -100.2174],\n",
      "        [-124.9425,  -75.4899, -182.2957,  -34.4313,  -91.5705, -142.6367,\n",
      "          -81.5478, -147.0352],\n",
      "        [-116.9663,  -67.3523, -177.0863,  -40.7054,  -70.1901, -145.5527,\n",
      "          -85.9843, -179.9039]], device='cuda:0', requires_grad=True)\n",
      "sum_nbest_log_distribution: torch.Size([8]) tensor([ -86.1507,  -34.0924, -141.7790,  -31.1470,  -26.1941, -140.0159,\n",
      "         -73.2253, -100.2174], device='cuda:0', grad_fn=<LogsumexpBackward0>)\n",
      "normal_nbest_distribution: torch.Size([3, 8]) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6247e-01, 1.0000e+00, 9.2331e-01,\n",
      "         9.9976e-01, 1.0000e+00],\n",
      "        [1.4221e-17, 1.0503e-18, 2.5340e-18, 3.7465e-02, 4.0492e-29, 7.2744e-02,\n",
      "         2.4298e-04, 4.6480e-21],\n",
      "        [4.1393e-14, 3.5927e-15, 4.6369e-16, 7.0606e-05, 7.8121e-20, 3.9390e-03,\n",
      "         2.8762e-06, 2.4694e-35]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "asd_loss: tensor([0.5059, 0.2090, 3.2362, 0.3981, 0.8621, 4.6285, 2.8125, 1.6432],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "asd_loss_mean: tensor(1.7869, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"nbest_log_dist.pkl\", \"rb\") as file:\n",
    "    nbest_log_distribution = pickle.load(file)\n",
    "\n",
    "with open(\"asd_scores.pkl\", \"rb\") as file:\n",
    "    asd_scores = pickle.load(file)\n",
    "\n",
    "def compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=True):\n",
    "    # Computes log distribution\n",
    "    # (n, b) -> (b,): log( p1+p2+...+pn ) = log( exp(log_p1)+exp(log_p2)+...+exp(log_pn) )\n",
    "    sum_nbest_log_distribution = torch.logsumexp(nbest_log_distribution, 0)\n",
    "    print(\"nbest_log_distribution:\", nbest_log_distribution.size(), nbest_log_distribution)\n",
    "    print(\"sum_nbest_log_distribution:\", sum_nbest_log_distribution.size(), sum_nbest_log_distribution)\n",
    "\n",
    "    # Re-normalized over just the N-best hypotheses.\n",
    "    # (n, b) - (b,) -> (n, b): exp(log_p)/exp(log_p_sum) = exp(log_p-log_p_sum)\n",
    "    normal_nbest_distribution = torch.exp(nbest_log_distribution - sum_nbest_log_distribution)\n",
    "    print(\"normal_nbest_distribution:\", normal_nbest_distribution.size(), normal_nbest_distribution)\n",
    "\n",
    "    if normalized_asd == True:\n",
    "        mean_asd = torch.mean(asd_scores, 0)\n",
    "        asd_norm = asd_scores - mean_asd\n",
    "        print(\"mean_asd:\", mean_asd.size(), mean_asd)\n",
    "        print(\"asd_norm:\", asd_norm.size(), asd_norm)\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_norm, 0)\n",
    "    else:\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_scores, 0)\n",
    "\n",
    "    return asd_loss\n",
    "\n",
    "asd_loss = compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=False)\n",
    "print(\"asd_loss:\", asd_loss)\n",
    "print(\"asd_loss_mean:\", torch.mean(asd_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - mASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import jiwer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_results(mod_loss_results, orig_loss_results):\n",
    "    merged_results = orig_loss_results.join(mod_loss_results.set_index(\"segment_id\"), on=\"segment_id\")\n",
    "    merged_results = merged_results[['segment_id', 'ref_str', 'orig_asr', 'orig_wer', 'orig_asd', 'mod_asr', 'mod_wer', 'mod_asd']]\n",
    "    return merged_results\n",
    "\n",
    "def load_csv_to_df(csv_file, mod_loss):\n",
    "    results_df = pd.read_csv(csv_file, index_col=0)\n",
    "    if mod_loss == True:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\", \"ref_str\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"mod_asr\", \"wer\":\"mod_wer\", \"asd\":\"mod_asd\"})\n",
    "    else:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"orig_asr\", \"wer\":\"orig_wer\", \"asd\":\"orig_asd\"})\n",
    "    return results_df\n",
    "\n",
    "def load_results_to_df(dir_path, mod_loss):\n",
    "    df_list = []\n",
    "    for (root, dirs, files) in os.walk(dir_path, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(dir_path, fn)\n",
    "                df_list.append(load_csv_to_df(csv_path, mod_loss))\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_dir = \"../../nlp_models/ner_pos/finetuned_bert_pos_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pos_model_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pos_model_dir)\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "def get_pos_tags(nlp_pipeline, text, label_list_pos):\n",
    "    pos_tags = nlp_pipeline(text)\n",
    "    labels = [label_list_pos[int(x[\"entity\"].split(\"_\")[1])] for x in pos_tags]\n",
    "    sub_words = [x[\"word\"] for x in pos_tags]\n",
    "    idx_for_labels = []\n",
    "    for i, item in enumerate(sub_words):\n",
    "        if item[0:2] != \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                idx_for_labels.append(i)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                idx_for_labels.append(i)\n",
    "        if item[0:2] == \"##\" and sub_words[i-1][0:2] != \"##\":\n",
    "            sub_word_combined = []\n",
    "            sub_word_combined.append(i-1)\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "        elif item[0:2] == \"##\" and sub_words[i-1][0:2] == \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "    new_word_list = []\n",
    "    new_label_list = []\n",
    "    for idx in idx_for_labels:\n",
    "        if type(idx) is int:\n",
    "            new_word_list.append(sub_words[idx])\n",
    "            new_label_list.append(labels[idx])\n",
    "        else:\n",
    "            new_word_list.append(\"\".join(sub_words[idx[0]:idx[1]]).replace(\"##\", \"\"))\n",
    "            new_label_list.append(labels[idx[0]])\n",
    "    return new_word_list, new_label_list\n",
    "\n",
    "\n",
    "def extract_errors_pos_tags(sub_count, ins_count, del_count, pos_tags, alignment, ref_labels, hyp_labels,\n",
    "                            ref_words, hyp_words):\n",
    "    sub = 0\n",
    "    ins = 0\n",
    "    dele = 0\n",
    "    pos = []\n",
    "    for i in range(len(alignment)):\n",
    "        ref_start = alignment[i].ref_start_idx\n",
    "        ref_end = alignment[i].ref_end_idx\n",
    "        hyp_start = alignment[i].hyp_start_idx\n",
    "        hyp_end = alignment[i].hyp_end_idx\n",
    "        if alignment[i].type == \"substitute\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                sub += 1\n",
    "            for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "                ref_words[ref_start+i] = word.upper()\n",
    "            for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "                hyp_words[ref_start+i] = word.upper()\n",
    "        elif alignment[i].type == \"insert\":\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "                ins += 1\n",
    "            for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "                hyp_words[ref_start+i] = word.upper()\n",
    "        elif alignment[i].type == \"delete\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                dele += 1\n",
    "            for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "                ref_words[ref_start+i] = word.upper()\n",
    "    sub_count.append(sub)\n",
    "    ins_count.append(ins)\n",
    "    del_count.append(dele)\n",
    "    pos_tags.append(pos)\n",
    "\n",
    "    error_ref_text = \" \".join(ref_words)\n",
    "    error_asr_text = \" \".join(hyp_words)\n",
    "\n",
    "    return error_ref_text, error_asr_text\n",
    "\n",
    "\n",
    "def merge_df_wer_pos_tags(df, nlp_pipeline, label_list_pos):\n",
    "    mod_error_ref = []\n",
    "    mod_error_asr = []\n",
    "    orig_error_ref = []\n",
    "    orig_error_asr = []\n",
    "\n",
    "    mod_sub_count = []\n",
    "    mod_ins_count = []\n",
    "    mod_del_count = []\n",
    "    mod_pos_tags = []\n",
    "\n",
    "    orig_sub_count = []\n",
    "    orig_ins_count = []\n",
    "    orig_del_count = []\n",
    "    orig_pos_tags = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Error Tagging\"):\n",
    "        ref_words, ref_labels = get_pos_tags(nlp_pipeline, row.ref_str, label_list_pos)\n",
    "        orig_words, orig_labels = get_pos_tags(nlp_pipeline, row.orig_asr, label_list_pos)\n",
    "        mod_words, mod_labels = get_pos_tags(nlp_pipeline, row.mod_asr, label_list_pos)\n",
    "\n",
    "        out = jiwer.process_words([row.ref_str, row.ref_str], [row.orig_asr, row.mod_asr])\n",
    "        orig_alignment = out.alignments[0]\n",
    "        mod_alignment = out.alignments[1]\n",
    "\n",
    "        # substitute, insert, delete\n",
    "        mod_ref_text, mod_asr_text = extract_errors_pos_tags(mod_sub_count, mod_ins_count, mod_del_count, mod_pos_tags,\n",
    "                                                             mod_alignment, ref_labels, mod_labels, ref_words, mod_words)\n",
    "        orig_ref_text, orig_asr_text = extract_errors_pos_tags(orig_sub_count, orig_ins_count, orig_del_count, orig_pos_tags,\n",
    "                                                               orig_alignment, ref_labels, orig_labels, ref_words, orig_words)\n",
    "\n",
    "        mod_error_ref.append(mod_ref_text)\n",
    "        mod_error_asr.append(mod_asr_text)\n",
    "        orig_error_ref.append(orig_ref_text)\n",
    "        orig_error_asr.append(orig_asr_text)\n",
    "\n",
    "    df.drop(labels=[\"ref_str\", \"orig_asr\", \"mod_asr\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    df[\"orig_ref\"] = orig_error_ref\n",
    "    df[\"orig_asr\"] = orig_error_asr\n",
    "\n",
    "    df[\"orig_sub\"] = orig_sub_count\n",
    "    df[\"orig_ins\"] = orig_ins_count\n",
    "    df[\"orig_del\"] = orig_del_count\n",
    "    df[\"orig_pos_tags\"] = orig_pos_tags\n",
    "\n",
    "    df[\"mod_ref\"] = mod_error_ref\n",
    "    df[\"mod_asr\"] = mod_error_asr\n",
    "\n",
    "    df[\"mod_sub\"] = mod_sub_count\n",
    "    df[\"mod_ins\"] = mod_ins_count\n",
    "    df[\"mod_del\"] = mod_del_count\n",
    "    df[\"mod_pos_tags\"] = mod_pos_tags\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD with mod loss: 1\n",
      "lower ASD with mod loss: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging:   0%|          | 0/465 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 465/465 [00:45<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod loss pos errors: 1166\n",
      "   mod_pos_tags  count\n",
      "0   ADJ          89   \n",
      "1   ADP          77   \n",
      "2   ADV          73   \n",
      "3   AUX          55   \n",
      "4   CCONJ        28   \n",
      "5   DET          56   \n",
      "6   INTJ         11   \n",
      "7   NOUN         272  \n",
      "8   NUM          8    \n",
      "9   PART         32   \n",
      "10  PRON         104  \n",
      "11  PROPN        184  \n",
      "12  PUNCT        1    \n",
      "13  SCONJ        24   \n",
      "14  VERB         127  \n",
      "15  X            25   \n",
      "orig loss pos errors: 1175\n",
      "   orig_pos_tags  count\n",
      "0   ADJ           89   \n",
      "1   ADP           83   \n",
      "2   ADV           69   \n",
      "3   AUX           54   \n",
      "4   CCONJ         25   \n",
      "5   DET           54   \n",
      "6   INTJ          12   \n",
      "7   NOUN          278  \n",
      "8   NUM           10   \n",
      "9   PART          34   \n",
      "10  PRON          104  \n",
      "11  PROPN         186  \n",
      "12  PUNCT         1    \n",
      "13  SCONJ         26   \n",
      "14  VERB          127  \n",
      "15  X             23   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "path = \"./logs/trial35_masd_RundkastOnly_aulus7\"\n",
    "mod_results = load_results_to_df(path, mod_loss=True)\n",
    "path = \"./logs/trial_origloss_RundkastOnly_aulus7\"\n",
    "orig_results = load_results_to_df(path, mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('mod_pos_tags').reset_index(name='count')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"count\"].sum(axis=0))\n",
    "print(df_mod_pos_count)\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('orig_pos_tags').reset_index(name='count')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"count\"].sum(axis=0))\n",
    "print(df_orig_pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD with mod loss: 0\n",
      "lower ASD with mod loss: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 67/67 [00:06<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod loss pos errors: 193\n",
      "   mod_pos_tags  count\n",
      "0   ADJ          18   \n",
      "1   ADP          25   \n",
      "2   ADV          10   \n",
      "3   AUX          7    \n",
      "4   CCONJ        9    \n",
      "5   DET          9    \n",
      "6   NOUN         36   \n",
      "7   NUM          5    \n",
      "8   PART         8    \n",
      "9   PRON         22   \n",
      "10  PROPN        3    \n",
      "11  SCONJ        8    \n",
      "12  VERB         31   \n",
      "13  X            2    \n",
      "orig loss pos errors: 198\n",
      "   orig_pos_tags  count\n",
      "0   ADJ           18   \n",
      "1   ADP           25   \n",
      "2   ADV           12   \n",
      "3   AUX           7    \n",
      "4   CCONJ         8    \n",
      "5   DET           11   \n",
      "6   NOUN          38   \n",
      "7   NUM           6    \n",
      "8   PART          9    \n",
      "9   PRON          25   \n",
      "10  PROPN         3    \n",
      "11  SCONJ         6    \n",
      "12  VERB          28   \n",
      "13  X             2    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "mod_results = load_results_to_df(\"./logs/asd_trial8_0p5_fulldata_aulus7/for_inspection\", mod_loss=True)\n",
    "orig_results = load_results_to_df(\"./logs/asd_origLoss_fulldata_aulus7/for_inspection\", mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('mod_pos_tags').reset_index(name='count')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"count\"].sum(axis=0))\n",
    "print(df_mod_pos_count)\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('orig_pos_tags').reset_index(name='count')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"count\"].sum(axis=0))\n",
    "print(df_orig_pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>orig_wer</th>\n",
       "      <th>orig_asd</th>\n",
       "      <th>mod_wer</th>\n",
       "      <th>mod_asd</th>\n",
       "      <th>orig_ref</th>\n",
       "      <th>orig_asr</th>\n",
       "      <th>orig_sub</th>\n",
       "      <th>orig_ins</th>\n",
       "      <th>orig_del</th>\n",
       "      <th>orig_pos_tags</th>\n",
       "      <th>mod_ref</th>\n",
       "      <th>mod_asr</th>\n",
       "      <th>mod_sub</th>\n",
       "      <th>mod_ins</th>\n",
       "      <th>mod_del</th>\n",
       "      <th>mod_pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.166859</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>JEG ER en førtifire år gammel mann som er veldig opptatt av sykkel JEG trener sykkel både for egen del</td>\n",
       "      <td>JA en førtifire år gammel mann som er veldig opptatt av sykkel er TRENER sykkel både for egen del</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PRON, AUX, PRON]</td>\n",
       "      <td>jeg er en førtifire år gammel mann som er veldig opptatt av sykkel JEG trener sykkel både for egen del</td>\n",
       "      <td>jeg er en førtifire år gammel mann som er veldig opptatt av sykkel ER trener sykkel både for egen del</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.102166</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.169534</td>\n",
       "      <td>jeg trener ungdommer og JEG er også spinninginstruktør på sats og bare det å få være ute sykle GIR MEG</td>\n",
       "      <td>jeg trener ungdommer og er også spinninginstruktør på sats og bare det å få være ute sykle gjer MEG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PRON, VERB]</td>\n",
       "      <td>jeg trener ungdommer og JEG er også spinninginstruktør på sats og bare det å få være ute sykle GIR MEG</td>\n",
       "      <td>jeg trener ungdommer og er også spinninginstruktør på sats og bare det å få være ute sykle hjem</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[PRON, VERB, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.121030</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.195588</td>\n",
       "      <td>EI ut av de DALSLUKTENE SÅ plutselig kom vi rundt en sving der og vind tok meg og blåste meg</td>\n",
       "      <td>ut av de dalslukten SÅ plutselig kom vi rundt en sving der og vind tok meg og blåste meg og</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[DET, NOUN, CCONJ]</td>\n",
       "      <td>EI ut av de DALSLUKTENE SÅ plutselig kom vi rundt en sving der og vind tok meg og blåste meg</td>\n",
       "      <td>ut av de dalslukteså PLUTSELIG kom vi rundt en sving der og vind tok meg og blåste meg og</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[DET, NOUN, CCONJ, CCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.120552</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.120552</td>\n",
       "      <td>over i motsatt kjørebane og JEG er ikke så liten og lett heller jeg klarte ikke holde meg I riktig</td>\n",
       "      <td>over i motsatt kjørebane og er ikke så liten og lett heller jeg klarte ikke å holde meg riktig</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[PRON, PART, ADP]</td>\n",
       "      <td>over i motsatt kjørebane og JEG er ikke så liten og lett heller jeg klarte ikke holde meg I riktig</td>\n",
       "      <td>over i motsatt kjørebane og er ikke så liten og lett heller jeg klarte ikke å holde meg riktig</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[PRON, PART, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det</td>\n",
       "      <td>kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det</td>\n",
       "      <td>kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.189802</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.189802</td>\n",
       "      <td>sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i oslo DER jobbet JEG særlig med å</td>\n",
       "      <td>sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i i OSLO de HAR jobbet det særlig med å</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADP, PRON, ADV, PRON]</td>\n",
       "      <td>sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i oslo DER jobbet JEG særlig med å</td>\n",
       "      <td>sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i i OSLO de HAR jobbet det særlig med å</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADP, PRON, ADV, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.195843</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.195843</td>\n",
       "      <td>PRØVE Å disaggregere KONFLIKTSTUDIENE fra å være de mer tradisjonelle land nivåbaserte STUDIE til å være STUDIET utført på et</td>\n",
       "      <td>PRØVER OM disaggregere KONFLIKTSTURIENE fra å være de mer tradisjonelle land nivåbaserte STUDIER til å være STUDIER utført på et</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[VERB, PART, NOUN, NOUN, NOUN]</td>\n",
       "      <td>PRØVE Å disaggregere KONFLIKTSTUDIENE fra å være de mer tradisjonelle land nivåbaserte STUDIE til å være STUDIET utført på et</td>\n",
       "      <td>PRØVER OM disaggregere KONFLIKTSTURIENE fra å være de mer tradisjonelle land nivåbaserte STUDIER til å være STUDIER utført på et</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[VERB, PART, NOUN, NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.054819</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.054819</td>\n",
       "      <td>analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne et eller per definisjon så skjer jo en borgerkrig</td>\n",
       "      <td>analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne innenfor et eller per definisjon så skjer jo en borgerkrig</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADP]</td>\n",
       "      <td>analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne et eller per definisjon så skjer jo en borgerkrig</td>\n",
       "      <td>analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne innenfor et eller per definisjon så skjer jo en borgerkrig</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.265584</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.319746</td>\n",
       "      <td>innenfor et land og gjerne ofte bare en veldig begrenset del av landet og da BRUKE AGGREGAT AGGREGERTE VARIABLER PÅ</td>\n",
       "      <td>innenfor et et land og gjerne ofte bare en veldig begrenset del av landet og DA BRUKER agregaagregerte variabler på</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[DET, VERB, NOUN, ADJ]</td>\n",
       "      <td>innenfor et land og gjerne ofte bare en veldig begrenset del av landet og da BRUKE AGGREGAT AGGREGERTE VARIABLER PÅ</td>\n",
       "      <td>innenfor et et land og gjerne ofte bare en veldig begrenset del av landet og DA BRUKER AGREGAAGREGERTE variable</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[DET, VERB, NOUN, ADJ, NOUN, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_9</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.044873</td>\n",
       "      <td>landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var BRUKEN AV GEOGRAFISKE INFORMASJONSSYSTEMER veldig</td>\n",
       "      <td>landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var BRUKERNE GEOGRAFISK INFORMASJONSSYSTEMET veldig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, ADP, ADJ, NOUN]</td>\n",
       "      <td>landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var bruken av GEOGRAFISKE INFORMASJONSSYSTEMER veldig</td>\n",
       "      <td>landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var bruken av GEOGRAFISK INFORMASJONSSYSTEMET veldig</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADJ, NOUN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   segment_id  orig_wer  orig_asd  mod_wer   mod_asd  \\\n",
       "0   p1_g02_m2_5_x-free_cut_0   0.15      0.166859  0.05     0.055100   \n",
       "1   p1_g02_m2_5_x-free_cut_1   0.10      0.102166  0.15     0.169534   \n",
       "2   p1_g02_m2_5_x-free_cut_10  0.15      0.121030  0.20     0.195588   \n",
       "3   p1_g02_m2_5_x-free_cut_11  0.15      0.120552  0.15     0.120552   \n",
       "4   p1_g02_m2_5_x-free_cut_12  0.00      0.000000  0.00     0.000000   \n",
       "..                        ...   ...           ...   ...          ...   \n",
       "62  p1_g08_m2_5_x-free_cut_5   0.20      0.189802  0.20     0.189802   \n",
       "63  p1_g08_m2_5_x-free_cut_6   0.25      0.195843  0.25     0.195843   \n",
       "64  p1_g08_m2_5_x-free_cut_7   0.05      0.054819  0.05     0.054819   \n",
       "65  p1_g08_m2_5_x-free_cut_8   0.20      0.265584  0.30     0.319746   \n",
       "66  p1_g08_m2_5_x-free_cut_9   0.20      0.121951  0.10     0.044873   \n",
       "\n",
       "                                                                                                                              orig_ref  \\\n",
       "0   JEG ER en førtifire år gammel mann som er veldig opptatt av sykkel JEG trener sykkel både for egen del                               \n",
       "1   jeg trener ungdommer og JEG er også spinninginstruktør på sats og bare det å få være ute sykle GIR MEG                               \n",
       "2   EI ut av de DALSLUKTENE SÅ plutselig kom vi rundt en sving der og vind tok meg og blåste meg                                         \n",
       "3   over i motsatt kjørebane og JEG er ikke så liten og lett heller jeg klarte ikke holde meg I riktig                                   \n",
       "4   kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det                             \n",
       "..                                                                                                       ...                             \n",
       "62  sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i oslo DER jobbet JEG særlig med å              \n",
       "63  PRØVE Å disaggregere KONFLIKTSTUDIENE fra å være de mer tradisjonelle land nivåbaserte STUDIE til å være STUDIET utført på et        \n",
       "64  analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne et eller per definisjon så skjer jo en borgerkrig       \n",
       "65  innenfor et land og gjerne ofte bare en veldig begrenset del av landet og da BRUKE AGGREGAT AGGREGERTE VARIABLER PÅ                  \n",
       "66  landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var BRUKEN AV GEOGRAFISKE INFORMASJONSSYSTEMER veldig   \n",
       "\n",
       "                                                                                                                                   orig_asr  \\\n",
       "0   JA en førtifire år gammel mann som er veldig opptatt av sykkel er TRENER sykkel både for egen del                                         \n",
       "1   jeg trener ungdommer og er også spinninginstruktør på sats og bare det å få være ute sykle gjer MEG                                       \n",
       "2   ut av de dalslukten SÅ plutselig kom vi rundt en sving der og vind tok meg og blåste meg og                                               \n",
       "3   over i motsatt kjørebane og er ikke så liten og lett heller jeg klarte ikke å holde meg riktig                                            \n",
       "4   kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det                                  \n",
       "..                                                                                                       ...                                  \n",
       "62  sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i i OSLO de HAR jobbet det særlig med å              \n",
       "63  PRØVER OM disaggregere KONFLIKTSTURIENE fra å være de mer tradisjonelle land nivåbaserte STUDIER til å være STUDIER utført på et          \n",
       "64  analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne innenfor et eller per definisjon så skjer jo en borgerkrig   \n",
       "65  innenfor et et land og gjerne ofte bare en veldig begrenset del av landet og DA BRUKER agregaagregerte variabler på                       \n",
       "66  landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var BRUKERNE GEOGRAFISK INFORMASJONSSYSTEMET veldig          \n",
       "\n",
       "    orig_sub  orig_ins  orig_del                   orig_pos_tags  \\\n",
       "0   2         0         1         [PRON, AUX, PRON]                \n",
       "1   1         0         1         [PRON, VERB]                     \n",
       "2   1         1         1         [DET, NOUN, CCONJ]               \n",
       "3   0         1         2         [PRON, PART, ADP]                \n",
       "4   0         0         0         []                               \n",
       ".. ..        ..        ..         ..                               \n",
       "62  2         2         0         [ADP, PRON, ADV, PRON]           \n",
       "63  5         0         0         [VERB, PART, NOUN, NOUN, NOUN]   \n",
       "64  0         1         0         [ADP]                            \n",
       "65  2         1         1         [DET, VERB, NOUN, ADJ]           \n",
       "66  3         0         1         [NOUN, ADP, ADJ, NOUN]           \n",
       "\n",
       "                                                                                                                               mod_ref  \\\n",
       "0   jeg er en førtifire år gammel mann som er veldig opptatt av sykkel JEG trener sykkel både for egen del                               \n",
       "1   jeg trener ungdommer og JEG er også spinninginstruktør på sats og bare det å få være ute sykle GIR MEG                               \n",
       "2   EI ut av de DALSLUKTENE SÅ plutselig kom vi rundt en sving der og vind tok meg og blåste meg                                         \n",
       "3   over i motsatt kjørebane og JEG er ikke så liten og lett heller jeg klarte ikke holde meg I riktig                                   \n",
       "4   kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det                             \n",
       "..                                                                                                       ...                             \n",
       "62  sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i oslo DER jobbet JEG særlig med å              \n",
       "63  PRØVE Å disaggregere KONFLIKTSTUDIENE fra å være de mer tradisjonelle land nivåbaserte STUDIE til å være STUDIET utført på et        \n",
       "64  analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne et eller per definisjon så skjer jo en borgerkrig       \n",
       "65  innenfor et land og gjerne ofte bare en veldig begrenset del av landet og da BRUKE AGGREGAT AGGREGERTE VARIABLER PÅ                  \n",
       "66  landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var bruken av GEOGRAFISKE INFORMASJONSSYSTEMER veldig   \n",
       "\n",
       "                                                                                                                                    mod_asr  \\\n",
       "0   jeg er en førtifire år gammel mann som er veldig opptatt av sykkel ER trener sykkel både for egen del                                     \n",
       "1   jeg trener ungdommer og er også spinninginstruktør på sats og bare det å få være ute sykle hjem                                           \n",
       "2   ut av de dalslukteså PLUTSELIG kom vi rundt en sving der og vind tok meg og blåste meg og                                                 \n",
       "3   over i motsatt kjørebane og er ikke så liten og lett heller jeg klarte ikke å holde meg riktig                                            \n",
       "4   kjørefelt sånn at jeg måtte gå av sykkelen legge ned sykkelen og nesten krype i veikanten og der var det                                  \n",
       "..                                                                                                       ...                                  \n",
       "62  sendt fremragende senter for forskning som ble ledet av institutt for fredsforskning i i OSLO de HAR jobbet det særlig med å              \n",
       "63  PRØVER OM disaggregere KONFLIKTSTURIENE fra å være de mer tradisjonelle land nivåbaserte STUDIER til å være STUDIER utført på et          \n",
       "64  analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne innenfor et eller per definisjon så skjer jo en borgerkrig   \n",
       "65  innenfor et et land og gjerne ofte bare en veldig begrenset del av landet og DA BRUKER AGREGAAGREGERTE variable                           \n",
       "66  landnivå vil være veldig feil for å studere et sånt lokalt fenomen så dermed var bruken av GEOGRAFISK INFORMASJONSSYSTEMET veldig         \n",
       "\n",
       "    mod_sub  mod_ins  mod_del                       mod_pos_tags  \n",
       "0   1        0        0        [PRON]                             \n",
       "1   1        0        2        [PRON, VERB, PRON]                 \n",
       "2   1        1        2        [DET, NOUN, CCONJ, CCONJ]          \n",
       "3   0        1        2        [PRON, PART, ADP]                  \n",
       "4   0        0        0        []                                 \n",
       ".. ..       ..       ..        ..                                 \n",
       "62  2        2        0        [ADP, PRON, ADV, PRON]             \n",
       "63  5        0        0        [VERB, PART, NOUN, NOUN, NOUN]     \n",
       "64  0        1        0        [ADP]                              \n",
       "65  3        1        2        [DET, VERB, NOUN, ADJ, NOUN, ADP]  \n",
       "66  2        0        0        [ADJ, NOUN]                        \n",
       "\n",
       "[67 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origloss_NBTale = pd.read_csv(\"./logs/OrigLoss_V5/NBTale_results_wav2vec2_NO_origLossV5_2023-09-18.csv\")\n",
    "# origloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "# origloss_NBTale.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/OrigLoss_V5/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "origloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)\n",
    "    origloss_dfs.append(df)\n",
    "\n",
    "origloss = pd.concat(origloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customloss_NBTale = pd.read_csv(\"./logs/CustomLoss_V7/NBTale_results_wav2vec2_NO_customLossV7_2023-10-02.csv\")\n",
    "# customloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "# customloss_NBTale.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/CustomLoss_V7/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "customloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)\n",
    "    customloss_dfs.append(df)\n",
    "\n",
    "customloss = pd.concat(customloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged results of orig loss & custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pd.merge(origloss, customloss, on=\"ref_str\")\n",
    "merged_results = merged_results.reindex(columns=[\"segment_id\", \"ref_str\", \"asr_origloss\", \"wer_origloss\", \"asd_origloss\", \"asr_customloss\", \"wer_customloss\", \"asd_customloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer = merged_results[merged_results[\"wer_origloss\"] > merged_results[\"wer_customloss\"]]\n",
    "worse_wer = merged_results[merged_results[\"wer_origloss\"] < merged_results[\"wer_customloss\"]]\n",
    "better_asd = merged_results[merged_results[\"asd_origloss\"] > merged_results[\"asd_customloss\"]]\n",
    "worse_asd = merged_results[merged_results[\"asd_origloss\"] < merged_results[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better WER but *worse* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer[better_wer[\"asd_origloss\"] < better_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worse WER but *better* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_wer[worse_wer[\"asd_origloss\"] > worse_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_improved_percent = (len(better_wer)/len(merged_results)) * 100\n",
    "asd_improved_percent = (len(better_asd)/len(merged_results)) * 100\n",
    "\n",
    "wer_worsened_percent = (len(worse_wer)/len(merged_results)) * 100\n",
    "asd_worsened_percent = (len(worse_asd)/len(merged_results)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WER improved: {wer_improved_percent:.2f}%\")\n",
    "print(f\"ASD improved: {asd_improved_percent:.2f}%\")\n",
    "\n",
    "print(f\"WER worsened: {wer_worsened_percent:.2f}%\")\n",
    "print(f\"ASD worsened: {asd_worsened_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC LOSS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2467856/3756479965.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, \"Container\"):\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "import types\n",
    "from tabulate import tabulate\n",
    "from dtw import *\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1828.98it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank: 31\n",
      "reduction: mean\n",
      "zero inf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"blank:\", model.config.pad_token_id)\n",
    "print(\"reduction:\", model.config.ctc_loss_reduction)\n",
    "print(\"zero inf:\", model.config.ctc_zero_infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.04, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.047, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.047, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.055, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.047, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n",
      "32 <s>\n",
      "33 </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1688 [00:00<?, ?ex/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   3%|▎         | 50/1688 [00:00<00:03, 494.87ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   8%|▊         | 140/1688 [00:00<00:02, 729.49ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  15%|█▌        | 256/1688 [00:00<00:01, 923.91ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  22%|██▏       | 374/1688 [00:00<00:01, 1022.37ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  29%|██▉       | 492/1688 [00:00<00:01, 1078.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  36%|███▌      | 607/1688 [00:00<00:00, 1102.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  43%|████▎     | 725/1688 [00:00<00:00, 1125.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|████▉     | 841/1688 [00:00<00:00, 1134.06ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  57%|█████▋    | 959/1688 [00:00<00:00, 1147.61ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  64%|██████▎   | 1074/1688 [00:01<00:01, 565.97ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  72%|███████▏  | 1209/1688 [00:01<00:00, 705.40ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|███████▉  | 1343/1688 [00:01<00:00, 834.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  87%|████████▋ | 1476/1688 [00:01<00:00, 945.22ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 1688/1688 [00:01<00:00, 974.09ex/s] \n",
      "#3: 100%|██████████| 1687/1687 [00:01<00:00, 973.48ex/s] \n",
      "\n",
      "#2: 100%|██████████| 1687/1687 [00:01<00:00, 960.13ex/s] \n",
      "#0: 100%|██████████| 1688/1688 [00:01<00:00, 937.21ex/s] \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../model_ckpts/fine-tuning_wav2vec2_expectedASD2_3ep_0p01_Rundkast_aulus6/runs/train_set.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.cast_column(\"path\", Audio())\n",
    "train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_feature = [{\"input_values\": train_dataset[73][\"input_values\"]}]\n",
    "label_feature = [{\"input_ids\": train_dataset[73][\"labels\"]}]\n",
    "\n",
    "# input_feature = [{\"input_values\": input_values} for input_values in train_dataset[34:41][\"input_values\"]]\n",
    "# label_feature = [{\"input_ids\": labels} for labels in train_dataset[34:41][\"labels\"]]\n",
    "\n",
    "batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "batch[\"labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch[\"labels\"][0]\n",
    "labels_mask = labels >= 0\n",
    "flattened_targets = labels.masked_select(labels_mask)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166, 34])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_output.pkl', 'wb') as file:\n",
    "    pickle.dump(output, file)\n",
    "\n",
    "with open('sample_input.pkl', 'wb') as file:\n",
    "    pickle.dump(batch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis):\n",
    "    ref_text = reference.replace(\"[UNK]\", \"\")  # removes the [UNK] token in the reference text, observed during training\n",
    "    hyp_text = hypothesis.replace(\"[UNK]\", \"\")\n",
    "    tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                                hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                                hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(output_mean_reference)\n",
    "    asd_score = alignment.distance / num_tokens\n",
    "    return asd_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import cuda_ctc_decoder, ctc_decoder, download_pretrained_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model.wav2vec2.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'å', 'æ', 'ø', '[UNK]', '[PAD]', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = [None] * 34\n",
    "\n",
    "for key in processor.tokenizer.vocab:\n",
    "    tokens[processor.tokenizer.vocab[key]] = key\n",
    "tokens[32] = \"</s>\"\n",
    "tokens[33] = \"<s>\"\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# f = open(\"tokens.txt\", \"w\")\n",
    "# for i, item in enumerate(tokens):\n",
    "#     if i == (len(tokens) - 1):\n",
    "#         f.write(item)\n",
    "#     else:\n",
    "#         f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 34])\n",
      "torch.Size([1, 34])\n",
      "tensor([[ 5, 18,  0, 22,  9,  0, 19, 20,  1,  4,  9,  7,  0, 14,  5,  4,  5,  0,\n",
      "         15,  7,  0, 11, 21, 18, 19,  5, 18,  0,  4,  9, 19, 19,  5,  0]])\n",
      "['er vi stadig nede og kurser dise']\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# output = model(**batch)\n",
    "# logits = output[\"logits\"]\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# print(\"LABEL:\", processor_woLM.batch_decode(batch[\"labels\"]), \"\\n\")\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# log_probs = F.log_softmax(logits[0], dim=-1, dtype=torch.float32).to(device)\n",
    "# encoder_out_lens = torch.tensor(log_probs.shape[0], device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "# cuda_decoder = cuda_ctc_decoder(tokens, nbest=10, beam_size=10, blank_skip_threshold=0.95)\n",
    "# results = cuda_decoder(log_probs, encoder_out_lens)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: ikke minst er det viktig i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig\n",
      "idx: 6 \thyp:  ikke minst er det viktige i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n",
      "idx: 0 \thyp:  ikke minst er det viktige næringe som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# characters = model_tokenizer.convert_ids_to_tokens(batch[\"labels\"][0])\n",
    "# ref_text = re.sub(\" +\", \" \", \"\".join(characters).replace(\"|\", \" \"))\n",
    "# print(\"LABEL:\", ref_text, \"\\n\")\n",
    "\n",
    "# lm=\"./language_model/\"\n",
    "decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=50, blank_token=\"[PAD]\",\n",
    "                      sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "results = decoder(logits)\n",
    "\n",
    "asd_score_list = [0] * len(results[0])\n",
    "beam_score_list = [0] * len(results[0])\n",
    "hyp_list = []\n",
    "for i, item in enumerate(results[0]):\n",
    "    # print(item.tokens.shape, item.tokens)\n",
    "    chars = decoder.idxs_to_tokens(item.tokens)\n",
    "    hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "    hyp_list.append(hyp_text)\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    asd_score_list[i] = asd_score\n",
    "    beam_score_list[i] = item.score\n",
    "\n",
    "lowest_asd_idx = np.argmin(asd_score_list)\n",
    "highest_beam_score_idx = np.argmax(beam_score_list)\n",
    "\n",
    "print(\"idx:\", lowest_asd_idx, \"\\thyp:\", hyp_list[lowest_asd_idx])\n",
    "print(\"idx:\", highest_beam_score_idx, \"\\thyp:\", hyp_list[highest_beam_score_idx])\n",
    "\n",
    "nbest_loss = torch.tensor([beam_score_list[lowest_asd_idx]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4052.2563], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=nbest_loss, inputs=logits, grad_outputs=nbest_loss, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of log_prob: torch.Size([1, 175, 500]), the shape of encoder_out_lens: torch.Size([1])\n",
      "tensor([175], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "from torchaudio.models.decoder import cuda_ctc_decoder\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio\n",
    "\n",
    "def download_asset_external(url, key):\n",
    "    path = Path(torch.hub.get_dir()) / \"torchaudio\" / Path(key)\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.hub.download_url_to_file(url, path)\n",
    "    return str(path)\n",
    "\n",
    "url_prefix = \"https://huggingface.co/Zengwei/icefall-asr-librispeech-pruned-transducer-stateless7-ctc-2022-12-01\"\n",
    "model_link = f\"{url_prefix}/resolve/main/exp/cpu_jit.pt\"\n",
    "model_path = download_asset_external(model_link, \"cuda_ctc_decoder/cpu_jit.pt\")\n",
    "\n",
    "speech_file = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n",
    "waveform, sample_rate = torchaudio.load(speech_file)\n",
    "\n",
    "actual_transcript = \"i really was very much afraid of showing him how much shocked i was at some parts of what he said\"\n",
    "actual_transcript = actual_transcript.split()\n",
    "\n",
    "device = torch.device(\"cuda\", 0)\n",
    "acoustic_model = torch.jit.load(model_path)\n",
    "acoustic_model.to(device)\n",
    "acoustic_model.eval()\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "feat = torchaudio.compliance.kaldi.fbank(waveform, num_mel_bins=80, snip_edges=False)\n",
    "feat = feat.unsqueeze(0)\n",
    "feat_lens = torch.tensor(feat.size(1), device=device).unsqueeze(0)\n",
    "\n",
    "encoder_out, encoder_out_lens = acoustic_model.encoder(feat, feat_lens)\n",
    "nnet_output = acoustic_model.ctc_output(encoder_out)\n",
    "log_prob = torch.nn.functional.log_softmax(nnet_output, -1)\n",
    "\n",
    "print(f\"The shape of log_prob: {log_prob.shape}, the shape of encoder_out_lens: {encoder_out_lens.shape}\")\n",
    "print(encoder_out_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimum ASD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvrtc.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mk2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec/lib/python3.9/site-packages/k2/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     k2_torch_cuda_version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m!=\u001b[39m k2_torch_cuda_version\n\u001b[1;32m     17\u001b[0m ):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk2 was built using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk2_torch_cuda_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut you are using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to run it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeterminizeWeightPushingType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_ragged_index_select\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swoosh_l\n",
      "\u001b[0;31mImportError\u001b[0m: libnvrtc.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import k2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lattice(\n",
    "    nnet_output: torch.Tensor,\n",
    "    decoding_graph: k2.Fsa,\n",
    "    supervision_segments: torch.Tensor,\n",
    "    search_beam: float,\n",
    "    output_beam: float,\n",
    "    min_active_states: int,\n",
    "    max_active_states: int,\n",
    "    subsampling_factor: int = 1,\n",
    ") -> k2.Fsa:\n",
    "    \"\"\"Get the decoding lattice from a decoding graph and neural\n",
    "    network output.\n",
    "    Args:\n",
    "      nnet_output:\n",
    "        It is the output of a neural model of shape `(N, T, C)`.\n",
    "      decoding_graph:\n",
    "        An Fsa, the decoding graph. It can be either an HLG\n",
    "        (see `compile_HLG.py`) or an H (see `k2.ctc_topo`).\n",
    "      supervision_segments:\n",
    "        A 2-D **CPU** tensor of dtype `torch.int32` with 3 columns.\n",
    "        Each row contains information for a supervision segment. Column 0\n",
    "        is the `sequence_index` indicating which sequence this segment\n",
    "        comes from; column 1 specifies the `start_frame` of this segment\n",
    "        within the sequence; column 2 contains the `duration` of this\n",
    "        segment.\n",
    "      search_beam:\n",
    "        Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "        (less pruning). This is the default value; it may be modified by\n",
    "        `min_active_states` and `max_active_states`.\n",
    "      output_beam:\n",
    "         Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "         to best path of output.\n",
    "      min_active_states:\n",
    "        Minimum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to have fewer than this number active.\n",
    "        Set it to zero if there is no constraint.\n",
    "      max_active_states:\n",
    "        Maximum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to exceed that but may not always succeed.\n",
    "        You can use a very large number if no constraint is needed.\n",
    "      subsampling_factor:\n",
    "        The subsampling factor of the model.\n",
    "    Returns:\n",
    "      An FsaVec containing the decoding result. It has axes [utt][state][arc].\n",
    "    \"\"\"\n",
    "    dense_fsa_vec = k2.DenseFsaVec(\n",
    "        nnet_output,\n",
    "        supervision_segments,\n",
    "        allow_truncate=subsampling_factor - 1,\n",
    "    )\n",
    "\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        decoding_graph,\n",
    "        dense_fsa_vec,\n",
    "        search_beam=search_beam,\n",
    "        output_beam=output_beam,\n",
    "        min_active_states=min_active_states,\n",
    "        max_active_states=max_active_states,\n",
    "    )\n",
    "\n",
    "    return lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWERLoss(torch.nn.Module):\n",
    "    '''Minimum Word Error Rate Loss compuration in k2.\n",
    "\n",
    "    See equation 2 of https://arxiv.org/pdf/2106.02302.pdf about its definition.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        subsampling_factor: int,\n",
    "        search_beam: int = 20,\n",
    "        output_beam: int = 8,\n",
    "        min_active_states: int = 30,\n",
    "        max_active_states: int = 10000,\n",
    "        temperature: float = 1.0,\n",
    "        num_paths: int = 100,\n",
    "        use_double_scores: bool = True,\n",
    "        nbest_scale: float = 0.5,\n",
    "        reduction: Literal['none', 'mean', 'sum'] = 'sum'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          search_beam:\n",
    "            Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "            (less pruning). This is the default value; it may be modified by\n",
    "            `min_active_states` and `max_active_states`.\n",
    "          output_beam:\n",
    "             Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "             to best path of output.\n",
    "          min_active_states:\n",
    "            Minimum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to have fewer than this number active.\n",
    "            Set it to zero if there is no constraint.\n",
    "          max_active_states:\n",
    "            Maximum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to exceed that but may not always succeed.\n",
    "            You can use a very large number if no constraint is needed.\n",
    "          subsampling_factor:\n",
    "            The subsampling factor of the model.\n",
    "          temperature:\n",
    "            For long utterances, the dynamic range of scores will be too large\n",
    "            and the posteriors will be mostly 0 or 1.\n",
    "            To prevent this it might be a good idea to have an extra argument\n",
    "            that functions like a temperature.\n",
    "            We scale the logprobs by before doing the normalization.\n",
    "          use_double_scores:\n",
    "            True to use double precision floating point.\n",
    "            False to use single precision.\n",
    "          reduction:\n",
    "            Specifies the reduction to apply to the output:\n",
    "            'none' | 'sum' | 'mean'.\n",
    "            'none': no reduction will be applied.\n",
    "                    The returned 'loss' is a k2.RaggedTensor, with\n",
    "                    loss.tot_size(0) == batch_size.\n",
    "                    loss.tot_size(1) == total_num_paths_of_current_batch\n",
    "                    If you want the MWER loss for each utterance, just do:\n",
    "                    `loss_per_utt = loss.sum()`\n",
    "                    Then loss_per_utt.shape[0] should be batch_size.\n",
    "                    See more example usages in 'k2/python/tests/mwer_test.py'\n",
    "            'sum': sum loss of each path over the whole batch together.\n",
    "            'mean': divide above 'sum' by total num paths over the whole batch.\n",
    "          nbest_scale:\n",
    "            Scale `lattice.score` before passing it to :func:`k2.random_paths`.\n",
    "            A smaller value leads to more unique paths at the risk of being not\n",
    "            to sample the path with the best score.\n",
    "          num_paths:\n",
    "            Number of paths to **sample** from the lattice\n",
    "            using :func:`k2.random_paths`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.search_beam = search_beam\n",
    "        self.output_beam = output_beam\n",
    "        self.min_active_states = min_active_states\n",
    "        self.max_active_states = max_active_states\n",
    "\n",
    "        self.num_paths = num_paths\n",
    "        self.nbest_scale = nbest_scale\n",
    "        self.subsampling_factor = subsampling_factor\n",
    "\n",
    "        self.mwer_loss = k2.MWERLoss(\n",
    "            temperature=temperature,\n",
    "            use_double_scores=use_double_scores,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        emissions_lengths: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_length: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
    "                probability distribution over labels; output of acoustic model.\n",
    "            labels (torch.FloatTensor): CPU tensor of shape `(batch, label_len)` storing labels.\n",
    "            emissions_lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                in time axis of the output Tensor in each batch.\n",
    "            labels_length (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                label in each batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor:\n",
    "                Minimum Word Error Rate loss.\n",
    "        \"\"\"\n",
    "        H = k2.ctc_topo(\n",
    "            max_token=self.vocab_size-1,\n",
    "            modified=False,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "\n",
    "        supervision_segments = torch.stack(\n",
    "            (\n",
    "                torch.tensor(range(emissions_lengths.shape[0])),\n",
    "                torch.zeros(emissions_lengths.shape[0]),\n",
    "                emissions_lengths.cpu(),\n",
    "            ),\n",
    "            1,\n",
    "        ).to(torch.int32)\n",
    "\n",
    "        lattice = get_lattice(\n",
    "            nnet_output=emissions,\n",
    "            decoding_graph=H,\n",
    "            supervision_segments=supervision_segments,\n",
    "            search_beam=self.search_beam,\n",
    "            output_beam=self.output_beam,\n",
    "            min_active_states=self.min_active_states,\n",
    "            max_active_states=self.max_active_states,\n",
    "            subsampling_factor=self.subsampling_factor,\n",
    "        )\n",
    "\n",
    "        token_ids = []\n",
    "        for i in range(labels_length.size(0)):\n",
    "            token_ids.append(labels[i, : labels_length[i]].cpu().tolist())\n",
    "\n",
    "        loss = self.mwer_loss(\n",
    "            lattice, token_ids,\n",
    "            nbest_scale=self.nbest_scale,\n",
    "            num_paths=self.num_paths\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Gumbel Softmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "asd_scores = [0] * 10\n",
    "for i in range(10):\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=10, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_scores[i] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    print(\"asd:\", asd_scores[i], \"hyp:\", hyp_text)\n",
    "\n",
    "mean_asd = np.mean(asd_scores)\n",
    "loss[0] = mean_asd\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "def get_mass_prob(paths: Tensor,\n",
    "                        softmax_ctc: Tensor,\n",
    "                        model_pred_length: Tensor,\n",
    "                        eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    compute the path probability mass\n",
    "    :param paths: ctc alignments\n",
    "    :param softmax_ctc: model logits after softmax\n",
    "    :param model_pred_length:  max length of all given paths\n",
    "    :return: avg of the paths probability\n",
    "    \"\"\"\n",
    "    log_indexes_probs = softmax_ctc.gather(dim=1, index=paths) + eps\n",
    "    if len(log_indexes_probs) > model_pred_length:\n",
    "        log_indexes_probs[model_pred_length:, :] = torch.zeros((log_indexes_probs.shape[0] - model_pred_length, 0))\n",
    "    return torch.sum(log_indexes_probs, dim=0) / (model_pred_length.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: pågripelsen av toska gjorde altså denne pågripelsen mulig\n",
      "[0.037175211785556765, 0.037175211785556765, 0.1520948636867659, 0.24277221896598888]\n",
      "['pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåskha gjorde altså denne pågripelsen mulig']\n",
      "0\n",
      "tensor([0.2708, 0.1336, 0.1375], grad_fn=<CopySlices>)\n",
      "tensor(0.5420, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "input_lengths = model._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(output[\"logits\"][0], dim=-1, dtype=torch.float32)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "num_samples = 10\n",
    "non_zero_logits = []\n",
    "non_zero_asd = []\n",
    "non_zero_hyp = []\n",
    "# for i in range(num_samples):\n",
    "while len(non_zero_asd) < 4:\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=100, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    if asd_score != 0:\n",
    "        non_zero_logits.append(sampled_logits)\n",
    "        non_zero_asd.append(asd_score)\n",
    "        non_zero_hyp.append(hyp_text)\n",
    "\n",
    "print(non_zero_asd)\n",
    "print(non_zero_hyp)\n",
    "\n",
    "lowest_asd_idx = np.argmin(np.array(non_zero_asd))\n",
    "print(lowest_asd_idx)\n",
    "\n",
    "mass_prob_list = []\n",
    "for i in range(len(non_zero_asd)):\n",
    "    logits_selected = non_zero_logits[i].type(torch.LongTensor)\n",
    "    mass_prob_list.append(get_mass_prob(logits_selected, log_probs, input_lengths))\n",
    "\n",
    "loss = torch.zeros((len(mass_prob_list)-1))\n",
    "j=0\n",
    "for i in range(len(mass_prob_list)):\n",
    "    if i != lowest_asd_idx:\n",
    "        subtract_path_probs = mass_prob_list[i] - mass_prob_list[lowest_asd_idx]\n",
    "        loss[j] = torch.sum(torch.clamp((subtract_path_probs), min=0))\n",
    "        j += 1\n",
    "print(loss)\n",
    "print(torch.sum(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating aligned cosdist (1 batch, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    ref_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "\n",
    "    return ref_alignments\n",
    "\n",
    "\n",
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed\n",
    "\n",
    "\n",
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    return cosdist_for_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_register = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    input_feature = [{\"input_values\": input_values} for input_values in train_dataset[i*8:(i+1)*8][\"input_values\"]]\n",
    "    label_feature = [{\"input_ids\": labels} for labels in train_dataset[i*8:(i+1)*8][\"labels\"]]\n",
    "\n",
    "    batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "    label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = label\n",
    "\n",
    "    output = model(**batch)\n",
    "    output_logits = output[\"logits\"]\n",
    "    pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "    labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "    for i in range(len(pred_str)):\n",
    "        ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "        pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "        label_ids = labels[i]\n",
    "        labels_mask = label_ids >= 0\n",
    "        flattened_labels = label_ids.masked_select(labels_mask)\n",
    "        ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "        tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "        cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "        cosdist_register.append(cosdist_for_ctc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_976418/112888293.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cosdist_register_arr = np.asarray(cosdist_register)\n"
     ]
    }
   ],
   "source": [
    "cosdist_register_arr = np.asarray(cosdist_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(cosdist_register_arr.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    numphones = params.shape[0]  # number of labels, not even used in the code\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    # print(seqLen, numphones, L, T)\n",
    "\n",
    "    alphas = np.zeros((L,T))\n",
    "    betas = np.zeros((L,T))\n",
    "\n",
    "    # convert logits to log softmax\n",
    "    params = params - np.max(params,axis=0)\n",
    "    params = np.exp(params)\n",
    "    params = params / np.sum(params,axis=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = np.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = np.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = np.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward += np.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = np.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = np.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = np.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward += np.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = np.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = np.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = np.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or np.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(np.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_tensor(logits, seq, cosdist_for_ctc, blank=0):\n",
    "\n",
    "    cosdist_contribution_alphas = []\n",
    "    cosdist_contribution_betas = []\n",
    "\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    # print(\"label seq:\", seqLen)\n",
    "    # print(\"label seq length with blanks:\", L)\n",
    "    # print(\"utterance length:\", T)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # print(0, L-2*(T-t), \"time:\", t, \"start:\", start, \"L:\", L, \"s:\", s, \"l:\", l)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    # llDiff = torch.abs(llForward-llBackward)\n",
    "    # if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "    #     print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "    #     print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "    #     return (-llForward, grad)\n",
    "    # else:\n",
    "    #     grad = params - grad / (params * absum)\n",
    "    #     return (-llForward, grad)\n",
    "\n",
    "    for t in range(T):\n",
    "        for s in range(numphones):\n",
    "            tmp = (params[s,t]*absum[t])\n",
    "            if tmp > 0:\n",
    "                grad[s,t] = params[s,t] - grad[s,t] / tmp\n",
    "            else:\n",
    "                grad[s,t] = params[s,t]\n",
    "\n",
    "    return (-llForward, grad, sum(cosdist_contribution_alphas), sum(cosdist_contribution_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_newgrad(logits, seq, blank=0):\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient according to https://github.com/yehudabab/NumpyCTC/blob/main/ctc.py\n",
    "    padded_labels = torch.zeros((L))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(L):\n",
    "        if i%2 == 0:\n",
    "            padded_labels[i] = 0\n",
    "        else:\n",
    "            padded_labels[i] = seq[j]\n",
    "            j += 1\n",
    "\n",
    "    # print(len(seq), seq)\n",
    "    # print(len(padded_labels), padded_labels)\n",
    "\n",
    "    grad = torch.zeros(params.shape)\n",
    "\n",
    "    score_last = alphas[L-1, T-1]\n",
    "    score_before_last = betas[L-2, T-1]\n",
    "    p_l_given_ctc = score_last + score_before_last\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(numphones):\n",
    "            d_p_d_ytk = 0\n",
    "            lb_lk = np.nonzero(list(map(lambda x: 1 if k in x else 0, padded_labels)))[0]\n",
    "            for s in lb_lk:\n",
    "                d_p_d_ytk += alphas[s, t] * betas[s, t]\n",
    "\n",
    "            d_p_d_ytk /= (params[k, t] ** 2)\n",
    "            d_lnp_d_ytk = (1. / p_l_given_ctc) * d_p_d_ytk\n",
    "            grad[k, t] = d_lnp_d_ytk\n",
    "\n",
    "    return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Stanford CTC Loss script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].detach().cpu().numpy().transpose()\n",
    "seq = batch[\"labels\"][0].detach().cpu().numpy()\n",
    "\n",
    "loss, grad = ctc_loss(logits, seq.squeeze(), blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(output[\"logits\"][0], dim=1)\n",
    "print(token_ids)\n",
    "for token in token_ids:\n",
    "    if token == 33:\n",
    "        print(\"HUY\")\n",
    "\n",
    "decoded_tokens = []\n",
    "for token in token_ids:\n",
    "    decoded_tokens.append(model_tokenizer.decode(token))\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "targets = batch[\"labels\"][0]\n",
    "input_lengths = torch.tensor(output[\"logits\"][0].shape[0])\n",
    "target_lengths = torch.tensor(batch[\"labels\"][0].shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=targets, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].transpose(1,0)\n",
    "seq = batch[\"labels\"][0]\n",
    "\n",
    "loss, grad = ctc_loss_tensor(logits, seq, blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad.shape)\n",
    "transposed_grad = grad.transpose(1,0)\n",
    "print(transposed_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad.shape[0]):\n",
    "    print(i, grad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosdist alphas: 845.0103612840176\n",
      "cosdist betas: 972.7007383406162\n",
      "torch.Size([201, 34]) tensor(608.0905, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1854.4355, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 606.3643108587712\n",
      "cosdist betas: 550.9175247717649\n",
      "torch.Size([201, 34]) tensor(680.7924, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1999.4266, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1113.2689, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 307.15736174583435\n",
      "cosdist betas: 318.57188880443573\n",
      "torch.Size([201, 34]) tensor(2163.9688, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 2056.844472726263\n",
      "cosdist betas: 1902.4644071857585\n",
      "torch.Size([201, 34]) tensor(860.0428, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_to_inspect = []\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    loss, grad, cosdist_contribution_alphas, cosdist_contribution_betas = ctc_loss_tensor(logits, flattened_labels, cosdist_for_ctc)\n",
    "    grad_to_inspect.append(grad)\n",
    "\n",
    "    print(\"cosdist alphas:\", cosdist_contribution_alphas)\n",
    "    print(\"cosdist betas:\", cosdist_contribution_betas)\n",
    "    print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for logits in output_logits:\n",
    "    token_ids = torch.argmax(logits, dim=1)\n",
    "    print(token_ids)\n",
    "\n",
    "    decoded_tokens = []\n",
    "    for token in token_ids:\n",
    "        decoded_tokens.append(model_tokenizer.decode(token))\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing only the first utterance\n",
    "model.train()\n",
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "logits = output_logits[0]\n",
    "label_ids = labels[0]\n",
    "labels_mask = label_ids >= 0\n",
    "flattened_labels = label_ids.masked_select(labels_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([235, 34])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "# python-implemented CTC loss\n",
    "loss, grad = ctc_loss_tensor(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "loss, grad = ctc_loss_newgrad(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6440, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pytorch CTC loss\n",
    "import torch.nn.functional as F\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = labels_mask.sum(-1)\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=flattened_labels, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.grad)\n",
    "#         print(param.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating ASD into CTC Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: start running here! (when loading from pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOV-8 DATA\n",
    "# ref = \"wozniackis drøm om seier knust av russiske jekaterina\"\n",
    "# hyp = \"asnake drøm om sier knut av russiske karine\"\n",
    "\n",
    "# logit_frames_decoded = pickle.load(open(\"logit_frames_decoded_8Nov.pkl\", \"rb\"))\n",
    "\n",
    "# alignment_table = [logit_frames_decoded]\n",
    "# table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "# print(\"logits decoded\")\n",
    "# display(HTML(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    cosdist_alignment_tokens = []  # FOR PRINTING\n",
    "    ref_alignments = []\n",
    "    # hyp_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        cosdist_alignment_tokens.append(round(cosdist, 3))  # FOR PRINTING\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        # hyp_token = asd_tokenizer.convert_ids_to_tokens(hyp_input_ids[hyp_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "        # hyp_alignments.append((hyp_alignment_idxs[i], hyp_token, cosdist))\n",
    "\n",
    "    # FOR PRINTING\n",
    "    # ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(ref_alignment_idxs):\n",
    "    #     ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    # ref_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(ref_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(hyp_alignment_idxs):\n",
    "    #     hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    # hyp_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(hyp_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    # ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    # cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    # alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    # table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    # print(\"Token alignment table:\")\n",
    "    # display(HTML(table))\n",
    "\n",
    "    return ref_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    # print(tokens_compressed)\n",
    "    # if len(label_ids) == len(cosdist_for_ctc):\n",
    "    #     print(\"SAME\")\n",
    "    # else:\n",
    "    #     print(\"DIFF!\")\n",
    "    # for i, label in enumerate(label_ids):\n",
    "    #     print(label, cosdist_for_ctc[i])\n",
    "\n",
    "    # try normalizing\n",
    "    x = np.array(cosdist_for_ctc)\n",
    "    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    if np.isnan(np.sum(x_norm)):\n",
    "        print(\"boo!\")\n",
    "\n",
    "    return cosdist_for_ctc, x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07505041360855103, 0.07505041360855103, 0.07505041360855103, 0, 0.12715423107147217, 0.12715423107147217, 0.12715423107147217, 0, 0.029700636863708496, 0, 0.13597166538238525, 0.13597166538238525, 0, 0.5323134958744049, 0.5323134958744049, 0, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0, 0.6916015446186066, 0.6916015446186066, 0, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0, 0, 0.032095909118652344, 0.032095909118652344, 0, 0.018728435039520264, 0.018728435039520264, 0.018728435039520264, 0, 0.016046226024627686, 0.016046226024627686, 0, 0.016792714595794678, 0.016792714595794678, 0, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414]\n",
      "[0.10851684 0.10851684 0.10851684 0.         0.18385475 0.18385475\n",
      " 0.18385475 0.         0.04294472 0.         0.19660405 0.19660405\n",
      " 0.         0.76968234 0.76968234 0.         0.30526938 0.30526938\n",
      " 0.30526938 0.30526938 0.         0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.0306659  0.0306659  0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.         0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.         0.03278869 0.03278869 0.03278869\n",
      " 0.03278869 0.         1.         1.         0.         0.22142421\n",
      " 0.22142421 0.22142421 0.22142421 0.         0.         0.04640809\n",
      " 0.04640809 0.         0.02707981 0.02707981 0.02707981 0.\n",
      " 0.02320155 0.02320155 0.         0.02428091 0.02428091 0.\n",
      " 0.02478112 0.02478112 0.02478112 0.02478112 0.         0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4022911/3475542975.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007969975471496582, 0.007969975471496582, 0.007969975471496582, 0, 0.006086826324462891, 0.006086826324462891, 0, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0, 0.015287041664123535, 0.015287041664123535, 0, 0.0193021297454834, 0.0193021297454834, 0, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144]\n",
      "[0.02681489 0.02681489 0.02681489 0.         0.02047905 0.02047905\n",
      " 0.         0.02253498 0.02253498 0.02253498 0.02253498 0.02253498\n",
      " 0.         0.05143307 0.05143307 0.         0.06494178 0.06494178\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "[0.03979825973510742, 0.03979825973510742, 0.03979825973510742, 0, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0, 0.01816505193710327, 0.01816505193710327, 0.01816505193710327, 0, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0]\n",
      "[0.22152116 0.22152116 0.22152116 0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.         0.10110853 0.10110853 0.10110853 0.         0.04921078\n",
      " 0.04921078 0.04921078 0.04921078 0.04921078 0.        ]\n",
      "[0.1013750433921814, 0.1013750433921814, 0, 0.3244396448135376, 0.3244396448135376, 0.3244396448135376, 0, 0.03615701198577881, 0, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0, 0.018376052379608154, 0.018376052379608154, 0.018376052379608154, 0, 0.032556354999542236, 0, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0, 0.6256595551967621, 0.6256595551967621, 0, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566]\n",
      "[0.16202908 0.16202908 0.         0.51855621 0.51855621 0.51855621\n",
      " 0.         0.05779023 0.         0.03629764 0.03629764 0.03629764\n",
      " 0.03629764 0.03629764 0.         0.02937069 0.02937069 0.02937069\n",
      " 0.         0.05203526 0.         0.03731947 0.03731947 0.03731947\n",
      " 0.03731947 0.         1.         1.         0.         0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.         0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449]\n"
     ]
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    max_per_frame = torch.argmax(logits, dim=1)\n",
    "    relevant_frames = []\n",
    "    for max_id in max_per_frame:\n",
    "        if max_id in flattened_labels:\n",
    "            relevant_frames.append(1)\n",
    "        else:\n",
    "            relevant_frames.append(0)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc, x_norm = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    print(cosdist_for_ctc)\n",
    "    print(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CTC with ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCORPORATING ASD COSDIST VALUES TO THE CTC CALCULATION\n",
    "\n",
    "def ctc_loss_with_ASD(params, seq, cosdist_for_ctc, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0].clone() / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])  # scale 0 to 1\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone() + alphas[s-2,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t].clone() / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1].clone() / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone() + betas[s+2,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t].clone() / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = torch.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out custom ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import gradcheck\n",
    "\n",
    "# output = model(**batch)\n",
    "# ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "# label_ids = batch[\"labels\"][0]\n",
    "# logits = output[\"logits\"][0]\n",
    "# hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "# ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "# tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "# cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "# inputs = (logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "\n",
    "# test = gradcheck(ctc_loss_with_ASD, inputs)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "print(tokens_compressed)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "loss, grad = ctc_loss_with_ASD(logits.transpose(1,0), label_ids, cosdist_for_ctc, blank=0)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = torch.tensor(label_ids.shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=label_ids, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extending torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCTC(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, seq, cosdist_for_ctc, blank=0):\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "        T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "        alphas = torch.zeros((L,T)).double()\n",
    "        betas = torch.zeros((L,T)).double()\n",
    "\n",
    "        # convert logits to log probs\n",
    "        params = params - (torch.max(params, dim=0)[0])\n",
    "        params = torch.exp(params)\n",
    "        params = params / torch.sum(params, dim=0)\n",
    "\n",
    "        # initialize alphas and forward pass\n",
    "        alphas[0,0] = params[blank,0]\n",
    "        alphas[1,0] = params[seq[0],0]\n",
    "        c = torch.sum(alphas[:,0])\n",
    "        alphas[:,0] = alphas[:,0] / c\n",
    "        llForward = torch.log(c)\n",
    "\n",
    "        for t in range(1,T):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(start,L):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s==0:\n",
    "                        alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                    else:\n",
    "                        alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == 1 or seq[l] == seq[l-1]:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time (prevent underflow)\n",
    "            c = torch.sum(alphas[start:end,t])\n",
    "            alphas[start:end,t] = alphas[start:end,t] / c\n",
    "            llForward = llForward + torch.log(c)\n",
    "\n",
    "        # initialize betas and backwards pass\n",
    "        betas[-1,-1] = params[blank,-1]\n",
    "        betas[-2,-1] = params[seq[-1],-1]\n",
    "        c = torch.sum(betas[:,-1])\n",
    "        betas[:,-1] = betas[:,-1] / c\n",
    "        llBackward = torch.log(c)\n",
    "\n",
    "        for t in range(T-2,-1,-1):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(end-1,-1,-1):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s == L-1:\n",
    "                        betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                    else:\n",
    "                        betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time\n",
    "            c = torch.sum(betas[start:end,t])\n",
    "            betas[start:end,t] = betas[start:end,t] / c\n",
    "            llBackward = llBackward + torch.log(c)\n",
    "\n",
    "        ctx.save_for_backward(params, seq, alphas, betas, llForward, llBackward)\n",
    "\n",
    "        return -llForward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        params, seq, alphas, betas, llForward, llBackward = ctx.saved_tensors\n",
    "        blank = 0\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "\n",
    "        # Compute gradient with respect to unnormalized input parameters\n",
    "        grad = torch.zeros(params.shape)\n",
    "        ab = alphas*betas\n",
    "        for s in range(L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/params[blank,:]\n",
    "            else:\n",
    "                grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "        absum = torch.sum(ab,axis=0)\n",
    "\n",
    "        llDiff = torch.abs(llForward-llBackward)\n",
    "        if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "            return (grad, None, None)\n",
    "        else:\n",
    "            grad = params - grad / (params * absum)\n",
    "            return (grad, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "print(tokens_compressed)\n",
    "\n",
    "myctcloss = MyCTC.apply\n",
    "loss = myctcloss(logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS w/ FRAME NOS. & DISREGARDING ALL THE PADS\n",
    "\n",
    "clean_logit_frames = []\n",
    "for i, item in enumerate(logit_frames_decoded):\n",
    "    if item != \"[PAD]\":\n",
    "        clean_logit_frames.append((i, item))\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(clean_logit_frames):\n",
    "    if i == 0:\n",
    "        if char[1] == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if char[1] != \"\":\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if clean_logit_frames[i-1][1] == \"\" and char[1] != \"\":  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "            elif char[1] != \"\":\n",
    "                new_word.append(char)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(\" \".join([char[1] for char in item]))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS MATRIX INTO THEIR CORRESPONDING WORDS, WHILE RETAINING ROW NOS. OR FRAME NOS.\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(logit_frames_decoded):\n",
    "    if i == 0:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if logit_frames_decoded[i-1] in [\"\", \"[PAD]\"] and char not in [\"\", \"[PAD]\"]:\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if \"\" in logit_frames_decoded[i-2:i-1] and char not in [\"\", \"[PAD]\"]:  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "            elif logit_frames_decoded[i-1] == char and char != \"\":  # repeating characters\n",
    "                new_word.append((i, char))\n",
    "            elif logit_frames_decoded[i-1] != char and char != \"\":  # new character detected\n",
    "                new_word.append((i, char))\n",
    "\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # detecting applicable frames & caclulating average cosdist\n",
    "        # frame_group = []\n",
    "        # for token_char in token_char_list:\n",
    "        #     detect_count = 0\n",
    "        #     if token_char in [x[1] for x in char_group_copy]:\n",
    "        #         for i, char in enumerate(char_group_copy):\n",
    "        #             # print(char[1], token_char, [x[1] for x in char_group_copy])\n",
    "        #             if char[1] == token_char:\n",
    "        #                 frame_group.append(char)\n",
    "        #                 detect_count += 1\n",
    "        #             else:\n",
    "        #                 if detect_count > 0:\n",
    "        #                     for count in range(detect_count):\n",
    "        #                         char_group_copy.pop(0)\n",
    "        #                     break\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # cosdist_divided = token_cosdist / len(frame_group)  # divide ASD token cosdist by number of frames for word\n",
    "        # assign divided ASD token cosdist\n",
    "        # frame_cosdist = []\n",
    "        # for char in frame_group:\n",
    "        #     if char[1] in token_char_list:\n",
    "        #         frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING THE ASD COSDIST PER NO. FRAMES\n",
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
