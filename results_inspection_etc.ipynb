{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbest_log_distribution: torch.Size([3, 8]) tensor([[ -86.1507,  -34.0924, -141.7790,  -31.1852,  -26.1941, -140.0957,\n",
      "          -73.2255, -100.2174],\n",
      "        [-124.9425,  -75.4899, -182.2957,  -34.4313,  -91.5705, -142.6367,\n",
      "          -81.5478, -147.0352],\n",
      "        [-116.9663,  -67.3523, -177.0863,  -40.7054,  -70.1901, -145.5527,\n",
      "          -85.9843, -179.9039]], device='cuda:0', requires_grad=True)\n",
      "sum_nbest_log_distribution: torch.Size([8]) tensor([ -86.1507,  -34.0924, -141.7790,  -31.1470,  -26.1941, -140.0159,\n",
      "         -73.2253, -100.2174], device='cuda:0', grad_fn=<LogsumexpBackward0>)\n",
      "normal_nbest_distribution: torch.Size([3, 8]) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6247e-01, 1.0000e+00, 9.2331e-01,\n",
      "         9.9976e-01, 1.0000e+00],\n",
      "        [1.4221e-17, 1.0503e-18, 2.5340e-18, 3.7465e-02, 4.0492e-29, 7.2744e-02,\n",
      "         2.4298e-04, 4.6480e-21],\n",
      "        [4.1393e-14, 3.5927e-15, 4.6369e-16, 7.0606e-05, 7.8121e-20, 3.9390e-03,\n",
      "         2.8762e-06, 2.4694e-35]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "asd_loss: tensor([0.5059, 0.2090, 3.2362, 0.3981, 0.8621, 4.6285, 2.8125, 1.6432],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "asd_loss_mean: tensor(1.7869, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"nbest_log_dist.pkl\", \"rb\") as file:\n",
    "    nbest_log_distribution = pickle.load(file)\n",
    "\n",
    "with open(\"asd_scores.pkl\", \"rb\") as file:\n",
    "    asd_scores = pickle.load(file)\n",
    "\n",
    "def compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=True):\n",
    "    # Computes log distribution\n",
    "    # (n, b) -> (b,): log( p1+p2+...+pn ) = log( exp(log_p1)+exp(log_p2)+...+exp(log_pn) )\n",
    "    sum_nbest_log_distribution = torch.logsumexp(nbest_log_distribution, 0)\n",
    "    print(\"nbest_log_distribution:\", nbest_log_distribution.size(), nbest_log_distribution)\n",
    "    print(\"sum_nbest_log_distribution:\", sum_nbest_log_distribution.size(), sum_nbest_log_distribution)\n",
    "\n",
    "    # Re-normalized over just the N-best hypotheses.\n",
    "    # (n, b) - (b,) -> (n, b): exp(log_p)/exp(log_p_sum) = exp(log_p-log_p_sum)\n",
    "    normal_nbest_distribution = torch.exp(nbest_log_distribution - sum_nbest_log_distribution)\n",
    "    print(\"normal_nbest_distribution:\", normal_nbest_distribution.size(), normal_nbest_distribution)\n",
    "\n",
    "    if normalized_asd == True:\n",
    "        mean_asd = torch.mean(asd_scores, 0)\n",
    "        asd_norm = asd_scores - mean_asd\n",
    "        print(\"mean_asd:\", mean_asd.size(), mean_asd)\n",
    "        print(\"asd_norm:\", asd_norm.size(), asd_norm)\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_norm, 0)\n",
    "    else:\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_scores, 0)\n",
    "\n",
    "    return asd_loss\n",
    "\n",
    "asd_loss = compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=False)\n",
    "print(\"asd_loss:\", asd_loss)\n",
    "print(\"asd_loss_mean:\", torch.mean(asd_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - mASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import jiwer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_results(mod_loss_results, orig_loss_results):\n",
    "    merged_results = orig_loss_results.join(mod_loss_results.set_index(\"segment_id\"), on=\"segment_id\")\n",
    "    merged_results = merged_results[['segment_id', 'ref_str', 'orig_asr', 'orig_cer', 'orig_wer', 'orig_asd', 'mod_asr', 'mod_cer', 'mod_wer', 'mod_asd']]\n",
    "    return merged_results\n",
    "\n",
    "def load_csv_to_df(csv_file, mod_loss):\n",
    "    results_df = pd.read_csv(csv_file, index_col=0)\n",
    "    if mod_loss == True:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\", \"ref_str\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"mod_asr\", \"wer\":\"mod_wer\", \"asd\":\"mod_asd\", \"cer\":\"mod_cer\"})\n",
    "    else:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"orig_asr\", \"wer\":\"orig_wer\", \"asd\":\"orig_asd\", \"cer\":\"orig_cer\"})\n",
    "    return results_df\n",
    "\n",
    "def load_results_to_df(dir_path, mod_loss):\n",
    "    df_list = []\n",
    "    for (root, dirs, files) in os.walk(dir_path, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(dir_path, fn)\n",
    "                df_list.append(load_csv_to_df(csv_path, mod_loss))\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_dir = \"../../nlp_models/ner_pos/finetuned_bert_pos_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pos_model_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pos_model_dir)\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "def get_pos_tags(nlp_pipeline, text, label_list_pos):\n",
    "    pos_tags = nlp_pipeline(text)\n",
    "    labels = [label_list_pos[int(x[\"entity\"].split(\"_\")[1])] for x in pos_tags]\n",
    "    sub_words = [x[\"word\"] for x in pos_tags]\n",
    "    idx_for_labels = []\n",
    "    for i, item in enumerate(sub_words):\n",
    "        if item[0:2] != \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                idx_for_labels.append(i)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                idx_for_labels.append(i)\n",
    "        if item[0:2] == \"##\" and sub_words[i-1][0:2] != \"##\":\n",
    "            sub_word_combined = []\n",
    "            sub_word_combined.append(i-1)\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "        elif item[0:2] == \"##\" and sub_words[i-1][0:2] == \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "    new_word_list = []\n",
    "    new_label_list = []\n",
    "    for idx in idx_for_labels:\n",
    "        if type(idx) is int:\n",
    "            new_word_list.append(sub_words[idx])\n",
    "            new_label_list.append(labels[idx])\n",
    "        else:\n",
    "            new_word_list.append(\"\".join(sub_words[idx[0]:idx[1]]).replace(\"##\", \"\"))\n",
    "            new_label_list.append(labels[idx[0]])\n",
    "    return new_word_list, new_label_list\n",
    "\n",
    "\n",
    "def extract_errors_pos_tags(sub_count, ins_count, del_count, pos_tags, alignment, ref_labels, hyp_labels,\n",
    "                            ref_words, hyp_words):\n",
    "    sub = 0\n",
    "    ins = 0\n",
    "    dele = 0\n",
    "    pos = []\n",
    "    ref_error_words = []\n",
    "    asr_error_words = []\n",
    "\n",
    "    for i in range(len(alignment)):\n",
    "        ref_start = alignment[i].ref_start_idx\n",
    "        ref_end = alignment[i].ref_end_idx\n",
    "        hyp_start = alignment[i].hyp_start_idx\n",
    "        hyp_end = alignment[i].hyp_end_idx\n",
    "        if alignment[i].type == \"substitute\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                sub += 1\n",
    "            for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "                ref_words[ref_start+i] = word.upper()\n",
    "                ref_error_words.append(word)\n",
    "            for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "                hyp_words[ref_start+i] = word.upper()\n",
    "                asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"insert\":\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "                ins += 1\n",
    "            for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "                hyp_words[ref_start+i] = word.upper()\n",
    "                asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"delete\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                dele += 1\n",
    "            for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "                ref_words[ref_start+i] = word.upper()\n",
    "                ref_error_words.append(word)\n",
    "\n",
    "\n",
    "    sub_count.append(sub)\n",
    "    ins_count.append(ins)\n",
    "    del_count.append(dele)\n",
    "    pos_tags.append(pos)\n",
    "\n",
    "    error_ref_text = \" \".join(ref_words)\n",
    "    error_asr_text = \" \".join(hyp_words)\n",
    "\n",
    "    return error_ref_text, error_asr_text, ref_error_words, asr_error_words\n",
    "\n",
    "\n",
    "def merge_df_wer_pos_tags(df, nlp_pipeline, label_list_pos):\n",
    "    mod_error_ref = []\n",
    "    mod_error_asr = []\n",
    "    mod_ref_error_words = []\n",
    "    mod_asr_error_words = []\n",
    "\n",
    "    orig_error_ref = []\n",
    "    orig_error_asr = []\n",
    "    orig_ref_error_words = []\n",
    "    orig_asr_error_words = []\n",
    "\n",
    "    mod_sub_count = []\n",
    "    mod_ins_count = []\n",
    "    mod_del_count = []\n",
    "    mod_pos_tags = []\n",
    "\n",
    "    orig_sub_count = []\n",
    "    orig_ins_count = []\n",
    "    orig_del_count = []\n",
    "    orig_pos_tags = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Error Tagging\"):\n",
    "        ref_words, ref_labels = get_pos_tags(nlp_pipeline, row.ref_str, label_list_pos)\n",
    "        orig_words, orig_labels = get_pos_tags(nlp_pipeline, row.orig_asr, label_list_pos)\n",
    "        mod_words, mod_labels = get_pos_tags(nlp_pipeline, row.mod_asr, label_list_pos)\n",
    "\n",
    "        out = jiwer.process_words([row.ref_str, row.ref_str], [row.orig_asr, row.mod_asr])\n",
    "        orig_alignment = out.alignments[0]\n",
    "        mod_alignment = out.alignments[1]\n",
    "\n",
    "        # substitute, insert, delete\n",
    "        mod_ref_text, mod_asr_text, mod_ref_error, mod_asr_error = extract_errors_pos_tags(mod_sub_count,\n",
    "            mod_ins_count, mod_del_count, mod_pos_tags, mod_alignment, ref_labels, mod_labels, ref_words, mod_words)\n",
    "        orig_ref_text, orig_asr_text, orig_ref_error, orig_asr_error = extract_errors_pos_tags(orig_sub_count,\n",
    "            orig_ins_count, orig_del_count, orig_pos_tags, orig_alignment, ref_labels, orig_labels, ref_words, orig_words)\n",
    "\n",
    "        mod_error_ref.append(mod_ref_text)\n",
    "        mod_error_asr.append(mod_asr_text)\n",
    "        orig_error_ref.append(orig_ref_text)\n",
    "        orig_error_asr.append(orig_asr_text)\n",
    "\n",
    "        mod_ref_error_words.append(mod_ref_error)\n",
    "        mod_asr_error_words.append(mod_asr_error)\n",
    "        orig_ref_error_words.append(orig_ref_error)\n",
    "        orig_asr_error_words.append(orig_asr_error)\n",
    "\n",
    "    df.drop(labels=[\"ref_str\", \"orig_asr\", \"mod_asr\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    # df[\"orig_ref\"] = orig_error_ref  # text/sentence form with uppercase errors\n",
    "    # df[\"orig_asr\"] = orig_error_asr  # text/sentence form with uppercase errors\n",
    "    df[\"orig_ref_error_words\"] = orig_ref_error_words\n",
    "    df[\"orig_asr_error_words\"] = orig_asr_error_words\n",
    "\n",
    "    df[\"orig_sub\"] = orig_sub_count\n",
    "    df[\"orig_ins\"] = orig_ins_count\n",
    "    df[\"orig_del\"] = orig_del_count\n",
    "    df[\"orig_pos_tags\"] = orig_pos_tags\n",
    "\n",
    "    # df[\"mod_ref\"] = mod_error_ref  # text/sentence form with uppercase errors\n",
    "    # df[\"mod_asr\"] = mod_error_asr  # text/sentence form with uppercase errors\n",
    "    df[\"mod_ref_error_words\"] = mod_ref_error_words\n",
    "    df[\"mod_asr_error_words\"] = mod_asr_error_words\n",
    "\n",
    "    df[\"mod_sub\"] = mod_sub_count\n",
    "    df[\"mod_ins\"] = mod_ins_count\n",
    "    df[\"mod_del\"] = mod_del_count\n",
    "    df[\"mod_pos_tags\"] = mod_pos_tags\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD with mod loss: 1\n",
      "lower ASD with mod loss: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging:   0%|          | 0/465 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 465/465 [00:45<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod loss pos errors: 1166\n",
      "   mod_pos_tags  count\n",
      "0   ADJ          89   \n",
      "1   ADP          77   \n",
      "2   ADV          73   \n",
      "3   AUX          55   \n",
      "4   CCONJ        28   \n",
      "5   DET          56   \n",
      "6   INTJ         11   \n",
      "7   NOUN         272  \n",
      "8   NUM          8    \n",
      "9   PART         32   \n",
      "10  PRON         104  \n",
      "11  PROPN        184  \n",
      "12  PUNCT        1    \n",
      "13  SCONJ        24   \n",
      "14  VERB         127  \n",
      "15  X            25   \n",
      "orig loss pos errors: 1175\n",
      "   orig_pos_tags  count\n",
      "0   ADJ           89   \n",
      "1   ADP           83   \n",
      "2   ADV           69   \n",
      "3   AUX           54   \n",
      "4   CCONJ         25   \n",
      "5   DET           54   \n",
      "6   INTJ          12   \n",
      "7   NOUN          278  \n",
      "8   NUM           10   \n",
      "9   PART          34   \n",
      "10  PRON          104  \n",
      "11  PROPN         186  \n",
      "12  PUNCT         1    \n",
      "13  SCONJ         26   \n",
      "14  VERB          127  \n",
      "15  X             23   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "path = \"./logs/trial35_masd_RundkastOnly_aulus7\"\n",
    "mod_results = load_results_to_df(path, mod_loss=True)\n",
    "path = \"./logs/trial_origloss_RundkastOnly_aulus7\"\n",
    "orig_results = load_results_to_df(path, mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('mod_pos_tags').reset_index(name='count')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"count\"].sum(axis=0))\n",
    "print(df_mod_pos_count)\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('orig_pos_tags').reset_index(name='count')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"count\"].sum(axis=0))\n",
    "print(df_orig_pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD with mod loss: 4\n",
      "lower ASD with mod loss: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 277/277 [00:23<00:00, 11.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod loss pos errors: 1426\n",
      "orig loss pos errors: 1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "# mod_results = load_results_to_df(\"./logs/asd_trial8_0p5_fulldata_aulus7/for_inspection\", mod_loss=True)\n",
    "# orig_results = load_results_to_df(\"./logs/asd_origLoss_fulldata_aulus7/for_inspection\", mod_loss=False)\n",
    "# mod_results = load_results_to_df(\"./logs/trial59_masd_3ep_allDataSmall_titan1/for_inspection\", mod_loss=True)\n",
    "# orig_results = load_results_to_df(\"./logs/trial_origloss_3ep_allDataSmall_titan1/for_inspection\", mod_loss=False)\n",
    "mod_results = load_results_to_df(\"./logs/trial59_masd_3ep_allDataSmall_titan1\", mod_loss=True)\n",
    "orig_results = load_results_to_df(\"./logs/trial_origloss_3ep_allDataSmall_titan1\", mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "merged_results_for_pos_inspection = merged_results[merged_results[\"mod_wer\"] != merged_results[\"orig_wer\"]]\n",
    "df_pos = merge_df_wer_pos_tags(merged_results_for_pos_inspection.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('pos_tags').reset_index(name='mod_loss')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"mod_loss\"].sum(axis=0))\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('pos_tags').reset_index(name='orig_loss')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"orig_loss\"].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "merged_pos_count = pd.merge(df_orig_pos_count, df_mod_pos_count, on=[\"pos_tags\"])\n",
    "merged_pos_count = pd.melt(merged_pos_count, id_vars=[\"pos_tags\"], value_vars=[\"orig_loss\", \"mod_loss\"])\n",
    "merged_pos_count = merged_pos_count.rename(columns={\"variable\":\"model\", \"value\":\"count\"})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax = sns.barplot(data=merged_pos_count, x=\"pos_tags\", y=\"count\", hue=\"model\", palette=\"dark\", alpha=0.7)\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.grid(axis='y')\n",
    "ax.set_title(\"POS Error Count\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"POS Tags\")\n",
    "sns.move_legend(ax, bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>model</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADP</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADV</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUX</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DET</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NUM</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PART</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PRON</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VERB</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>X</td>\n",
       "      <td>orig_loss</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ADP</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ADV</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AUX</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DET</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NUM</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PART</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PRON</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VERB</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>X</td>\n",
       "      <td>mod_loss</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos_tags      model  count\n",
       "0   ADJ      orig_loss  91   \n",
       "1   ADP      orig_loss  149  \n",
       "2   ADV      orig_loss  113  \n",
       "3   AUX      orig_loss  82   \n",
       "4   CCONJ    orig_loss  72   \n",
       "5   DET      orig_loss  76   \n",
       "6   INTJ     orig_loss  11   \n",
       "7   NOUN     orig_loss  325  \n",
       "8   NUM      orig_loss  48   \n",
       "9   PART     orig_loss  33   \n",
       "10  PRON     orig_loss  162  \n",
       "11  PROPN    orig_loss  97   \n",
       "12  SCONJ    orig_loss  48   \n",
       "13  VERB     orig_loss  129  \n",
       "14  X        orig_loss  13   \n",
       "15  ADJ      mod_loss   105  \n",
       "16  ADP      mod_loss   139  \n",
       "17  ADV      mod_loss   105  \n",
       "18  AUX      mod_loss   78   \n",
       "19  CCONJ    mod_loss   81   \n",
       "20  DET      mod_loss   68   \n",
       "21  INTJ     mod_loss   11   \n",
       "22  NOUN     mod_loss   316  \n",
       "23  NUM      mod_loss   55   \n",
       "24  PART     mod_loss   36   \n",
       "25  PRON     mod_loss   156  \n",
       "26  PROPN    mod_loss   91   \n",
       "27  SCONJ    mod_loss   44   \n",
       "28  VERB     mod_loss   130  \n",
       "29  X        mod_loss   10   "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  word  lemma\n",
      "0       -abel           -abel\n",
      "1       -abels          -abel\n",
      "2       -abelt          -abel\n",
      "3       -abelts         -abel\n",
      "4       -able           -abel\n",
      "...       ...             ...\n",
      "744437  jordskokkpuré   -    \n",
      "744438  rotgrønnsakene  -    \n",
      "744439  albóndigas      -    \n",
      "744440  gelatinplatene  -    \n",
      "744441  kyllinglårene   -    \n",
      "\n",
      "[744442 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/talebase/data/lex/Sprakbanken/NLB/nlb_nob_20181129.lex\"\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines:\n",
    "        lex_list = line.split(\"\\t\")\n",
    "        lemma = lex_list[13].split(\":\")[1].split(\"|\")[0]\n",
    "        word_list.append(lex_list[0])\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "lex_dict = {\"word\":word_list, \"lemma\":lemma_list}\n",
    "lex_df = pd.DataFrame(lex_dict)\n",
    "print(lex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origloss_NBTale = pd.read_csv(\"./logs/OrigLoss_V5/NBTale_results_wav2vec2_NO_origLossV5_2023-09-18.csv\")\n",
    "# origloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "# origloss_NBTale.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/OrigLoss_V5/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "origloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)\n",
    "    origloss_dfs.append(df)\n",
    "\n",
    "origloss = pd.concat(origloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customloss_NBTale = pd.read_csv(\"./logs/CustomLoss_V7/NBTale_results_wav2vec2_NO_customLossV7_2023-10-02.csv\")\n",
    "# customloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "# customloss_NBTale.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/CustomLoss_V7/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "customloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)\n",
    "    customloss_dfs.append(df)\n",
    "\n",
    "customloss = pd.concat(customloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged results of orig loss & custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pd.merge(origloss, customloss, on=\"ref_str\")\n",
    "merged_results = merged_results.reindex(columns=[\"segment_id\", \"ref_str\", \"asr_origloss\", \"wer_origloss\", \"asd_origloss\", \"asr_customloss\", \"wer_customloss\", \"asd_customloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer = merged_results[merged_results[\"wer_origloss\"] > merged_results[\"wer_customloss\"]]\n",
    "worse_wer = merged_results[merged_results[\"wer_origloss\"] < merged_results[\"wer_customloss\"]]\n",
    "better_asd = merged_results[merged_results[\"asd_origloss\"] > merged_results[\"asd_customloss\"]]\n",
    "worse_asd = merged_results[merged_results[\"asd_origloss\"] < merged_results[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better WER but *worse* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer[better_wer[\"asd_origloss\"] < better_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worse WER but *better* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_wer[worse_wer[\"asd_origloss\"] > worse_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_improved_percent = (len(better_wer)/len(merged_results)) * 100\n",
    "asd_improved_percent = (len(better_asd)/len(merged_results)) * 100\n",
    "\n",
    "wer_worsened_percent = (len(worse_wer)/len(merged_results)) * 100\n",
    "asd_worsened_percent = (len(worse_asd)/len(merged_results)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WER improved: {wer_improved_percent:.2f}%\")\n",
    "print(f\"ASD improved: {asd_improved_percent:.2f}%\")\n",
    "\n",
    "print(f\"WER worsened: {wer_worsened_percent:.2f}%\")\n",
    "print(f\"ASD worsened: {asd_worsened_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC LOSS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3535249/3756479965.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, \"Container\"):\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "import types\n",
    "from tabulate import tabulate\n",
    "from dtw import *\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1868.49it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank: 31\n",
      "reduction: mean\n",
      "zero inf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"blank:\", model.config.pad_token_id)\n",
    "print(\"reduction:\", model.config.ctc_loss_reduction)\n",
    "print(\"zero inf:\", model.config.ctc_zero_infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.04, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.047, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.047, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.055, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.047, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n",
      "32 <s>\n",
      "33 </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1688 [00:00<?, ?ex/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   3%|▎         | 50/1688 [00:00<00:03, 494.87ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   8%|▊         | 140/1688 [00:00<00:02, 729.49ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  15%|█▌        | 256/1688 [00:00<00:01, 923.91ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  22%|██▏       | 374/1688 [00:00<00:01, 1022.37ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  29%|██▉       | 492/1688 [00:00<00:01, 1078.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  36%|███▌      | 607/1688 [00:00<00:00, 1102.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  43%|████▎     | 725/1688 [00:00<00:00, 1125.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|████▉     | 841/1688 [00:00<00:00, 1134.06ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  57%|█████▋    | 959/1688 [00:00<00:00, 1147.61ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  64%|██████▎   | 1074/1688 [00:01<00:01, 565.97ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  72%|███████▏  | 1209/1688 [00:01<00:00, 705.40ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|███████▉  | 1343/1688 [00:01<00:00, 834.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  87%|████████▋ | 1476/1688 [00:01<00:00, 945.22ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 1688/1688 [00:01<00:00, 974.09ex/s] \n",
      "#3: 100%|██████████| 1687/1687 [00:01<00:00, 973.48ex/s] \n",
      "\n",
      "#2: 100%|██████████| 1687/1687 [00:01<00:00, 960.13ex/s] \n",
      "#0: 100%|██████████| 1688/1688 [00:01<00:00, 937.21ex/s] \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../model_ckpts/fine-tuning_wav2vec2_expectedASD2_3ep_0p01_Rundkast_aulus6/runs/train_set.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.cast_column(\"path\", Audio())\n",
    "train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_feature = [{\"input_values\": train_dataset[73][\"input_values\"]}]\n",
    "label_feature = [{\"input_ids\": train_dataset[73][\"labels\"]}]\n",
    "\n",
    "# input_feature = [{\"input_values\": input_values} for input_values in train_dataset[34:41][\"input_values\"]]\n",
    "# label_feature = [{\"input_ids\": labels} for labels in train_dataset[34:41][\"labels\"]]\n",
    "\n",
    "batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "batch[\"labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch[\"labels\"][0]\n",
    "labels_mask = labels >= 0\n",
    "flattened_targets = labels.masked_select(labels_mask)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166, 34])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_output.pkl', 'wb') as file:\n",
    "    pickle.dump(output, file)\n",
    "\n",
    "with open('sample_input.pkl', 'wb') as file:\n",
    "    pickle.dump(batch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis):\n",
    "    ref_text = reference.replace(\"[UNK]\", \"\")  # removes the [UNK] token in the reference text, observed during training\n",
    "    hyp_text = hypothesis.replace(\"[UNK]\", \"\")\n",
    "    tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                                hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                                hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(output_mean_reference)\n",
    "    asd_score = alignment.distance / num_tokens\n",
    "    return asd_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import cuda_ctc_decoder, ctc_decoder, download_pretrained_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model.wav2vec2.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'å', 'æ', 'ø', '[UNK]', '[PAD]', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = [None] * 34\n",
    "\n",
    "for key in processor.tokenizer.vocab:\n",
    "    tokens[processor.tokenizer.vocab[key]] = key\n",
    "tokens[32] = \"</s>\"\n",
    "tokens[33] = \"<s>\"\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# f = open(\"tokens.txt\", \"w\")\n",
    "# for i, item in enumerate(tokens):\n",
    "#     if i == (len(tokens) - 1):\n",
    "#         f.write(item)\n",
    "#     else:\n",
    "#         f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 34])\n",
      "torch.Size([1, 34])\n",
      "tensor([[ 5, 18,  0, 22,  9,  0, 19, 20,  1,  4,  9,  7,  0, 14,  5,  4,  5,  0,\n",
      "         15,  7,  0, 11, 21, 18, 19,  5, 18,  0,  4,  9, 19, 19,  5,  0]])\n",
      "['er vi stadig nede og kurser dise']\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# output = model(**batch)\n",
    "# logits = output[\"logits\"]\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# print(\"LABEL:\", processor_woLM.batch_decode(batch[\"labels\"]), \"\\n\")\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# log_probs = F.log_softmax(logits[0], dim=-1, dtype=torch.float32).to(device)\n",
    "# encoder_out_lens = torch.tensor(log_probs.shape[0], device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "# cuda_decoder = cuda_ctc_decoder(tokens, nbest=10, beam_size=10, blank_skip_threshold=0.95)\n",
    "# results = cuda_decoder(log_probs, encoder_out_lens)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: ikke minst er det viktig i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig\n",
      "idx: 6 \thyp:  ikke minst er det viktige i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n",
      "idx: 0 \thyp:  ikke minst er det viktige næringe som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# characters = model_tokenizer.convert_ids_to_tokens(batch[\"labels\"][0])\n",
    "# ref_text = re.sub(\" +\", \" \", \"\".join(characters).replace(\"|\", \" \"))\n",
    "# print(\"LABEL:\", ref_text, \"\\n\")\n",
    "\n",
    "# lm=\"./language_model/\"\n",
    "decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=50, blank_token=\"[PAD]\",\n",
    "                      sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "results = decoder(logits)\n",
    "\n",
    "asd_score_list = [0] * len(results[0])\n",
    "beam_score_list = [0] * len(results[0])\n",
    "hyp_list = []\n",
    "for i, item in enumerate(results[0]):\n",
    "    # print(item.tokens.shape, item.tokens)\n",
    "    chars = decoder.idxs_to_tokens(item.tokens)\n",
    "    hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "    hyp_list.append(hyp_text)\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    asd_score_list[i] = asd_score\n",
    "    beam_score_list[i] = item.score\n",
    "\n",
    "lowest_asd_idx = np.argmin(asd_score_list)\n",
    "highest_beam_score_idx = np.argmax(beam_score_list)\n",
    "\n",
    "print(\"idx:\", lowest_asd_idx, \"\\thyp:\", hyp_list[lowest_asd_idx])\n",
    "print(\"idx:\", highest_beam_score_idx, \"\\thyp:\", hyp_list[highest_beam_score_idx])\n",
    "\n",
    "nbest_loss = torch.tensor([beam_score_list[lowest_asd_idx]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4052.2563], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=nbest_loss, inputs=logits, grad_outputs=nbest_loss, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of log_prob: torch.Size([1, 175, 500]), the shape of encoder_out_lens: torch.Size([1])\n",
      "tensor([175], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "from torchaudio.models.decoder import cuda_ctc_decoder\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio\n",
    "\n",
    "def download_asset_external(url, key):\n",
    "    path = Path(torch.hub.get_dir()) / \"torchaudio\" / Path(key)\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.hub.download_url_to_file(url, path)\n",
    "    return str(path)\n",
    "\n",
    "url_prefix = \"https://huggingface.co/Zengwei/icefall-asr-librispeech-pruned-transducer-stateless7-ctc-2022-12-01\"\n",
    "model_link = f\"{url_prefix}/resolve/main/exp/cpu_jit.pt\"\n",
    "model_path = download_asset_external(model_link, \"cuda_ctc_decoder/cpu_jit.pt\")\n",
    "\n",
    "speech_file = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n",
    "waveform, sample_rate = torchaudio.load(speech_file)\n",
    "\n",
    "actual_transcript = \"i really was very much afraid of showing him how much shocked i was at some parts of what he said\"\n",
    "actual_transcript = actual_transcript.split()\n",
    "\n",
    "device = torch.device(\"cuda\", 0)\n",
    "acoustic_model = torch.jit.load(model_path)\n",
    "acoustic_model.to(device)\n",
    "acoustic_model.eval()\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "feat = torchaudio.compliance.kaldi.fbank(waveform, num_mel_bins=80, snip_edges=False)\n",
    "feat = feat.unsqueeze(0)\n",
    "feat_lens = torch.tensor(feat.size(1), device=device).unsqueeze(0)\n",
    "\n",
    "encoder_out, encoder_out_lens = acoustic_model.encoder(feat, feat_lens)\n",
    "nnet_output = acoustic_model.ctc_output(encoder_out)\n",
    "log_prob = torch.nn.functional.log_softmax(nnet_output, -1)\n",
    "\n",
    "print(f\"The shape of log_prob: {log_prob.shape}, the shape of encoder_out_lens: {encoder_out_lens.shape}\")\n",
    "print(encoder_out_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimum ASD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvrtc.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mk2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec/lib/python3.9/site-packages/k2/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     k2_torch_cuda_version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m!=\u001b[39m k2_torch_cuda_version\n\u001b[1;32m     17\u001b[0m ):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk2 was built using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk2_torch_cuda_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut you are using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to run it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeterminizeWeightPushingType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_ragged_index_select\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swoosh_l\n",
      "\u001b[0;31mImportError\u001b[0m: libnvrtc.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import k2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lattice(\n",
    "    nnet_output: torch.Tensor,\n",
    "    decoding_graph: k2.Fsa,\n",
    "    supervision_segments: torch.Tensor,\n",
    "    search_beam: float,\n",
    "    output_beam: float,\n",
    "    min_active_states: int,\n",
    "    max_active_states: int,\n",
    "    subsampling_factor: int = 1,\n",
    ") -> k2.Fsa:\n",
    "    \"\"\"Get the decoding lattice from a decoding graph and neural\n",
    "    network output.\n",
    "    Args:\n",
    "      nnet_output:\n",
    "        It is the output of a neural model of shape `(N, T, C)`.\n",
    "      decoding_graph:\n",
    "        An Fsa, the decoding graph. It can be either an HLG\n",
    "        (see `compile_HLG.py`) or an H (see `k2.ctc_topo`).\n",
    "      supervision_segments:\n",
    "        A 2-D **CPU** tensor of dtype `torch.int32` with 3 columns.\n",
    "        Each row contains information for a supervision segment. Column 0\n",
    "        is the `sequence_index` indicating which sequence this segment\n",
    "        comes from; column 1 specifies the `start_frame` of this segment\n",
    "        within the sequence; column 2 contains the `duration` of this\n",
    "        segment.\n",
    "      search_beam:\n",
    "        Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "        (less pruning). This is the default value; it may be modified by\n",
    "        `min_active_states` and `max_active_states`.\n",
    "      output_beam:\n",
    "         Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "         to best path of output.\n",
    "      min_active_states:\n",
    "        Minimum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to have fewer than this number active.\n",
    "        Set it to zero if there is no constraint.\n",
    "      max_active_states:\n",
    "        Maximum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to exceed that but may not always succeed.\n",
    "        You can use a very large number if no constraint is needed.\n",
    "      subsampling_factor:\n",
    "        The subsampling factor of the model.\n",
    "    Returns:\n",
    "      An FsaVec containing the decoding result. It has axes [utt][state][arc].\n",
    "    \"\"\"\n",
    "    dense_fsa_vec = k2.DenseFsaVec(\n",
    "        nnet_output,\n",
    "        supervision_segments,\n",
    "        allow_truncate=subsampling_factor - 1,\n",
    "    )\n",
    "\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        decoding_graph,\n",
    "        dense_fsa_vec,\n",
    "        search_beam=search_beam,\n",
    "        output_beam=output_beam,\n",
    "        min_active_states=min_active_states,\n",
    "        max_active_states=max_active_states,\n",
    "    )\n",
    "\n",
    "    return lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWERLoss(torch.nn.Module):\n",
    "    '''Minimum Word Error Rate Loss compuration in k2.\n",
    "\n",
    "    See equation 2 of https://arxiv.org/pdf/2106.02302.pdf about its definition.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        subsampling_factor: int,\n",
    "        search_beam: int = 20,\n",
    "        output_beam: int = 8,\n",
    "        min_active_states: int = 30,\n",
    "        max_active_states: int = 10000,\n",
    "        temperature: float = 1.0,\n",
    "        num_paths: int = 100,\n",
    "        use_double_scores: bool = True,\n",
    "        nbest_scale: float = 0.5,\n",
    "        reduction: Literal['none', 'mean', 'sum'] = 'sum'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          search_beam:\n",
    "            Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "            (less pruning). This is the default value; it may be modified by\n",
    "            `min_active_states` and `max_active_states`.\n",
    "          output_beam:\n",
    "             Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "             to best path of output.\n",
    "          min_active_states:\n",
    "            Minimum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to have fewer than this number active.\n",
    "            Set it to zero if there is no constraint.\n",
    "          max_active_states:\n",
    "            Maximum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to exceed that but may not always succeed.\n",
    "            You can use a very large number if no constraint is needed.\n",
    "          subsampling_factor:\n",
    "            The subsampling factor of the model.\n",
    "          temperature:\n",
    "            For long utterances, the dynamic range of scores will be too large\n",
    "            and the posteriors will be mostly 0 or 1.\n",
    "            To prevent this it might be a good idea to have an extra argument\n",
    "            that functions like a temperature.\n",
    "            We scale the logprobs by before doing the normalization.\n",
    "          use_double_scores:\n",
    "            True to use double precision floating point.\n",
    "            False to use single precision.\n",
    "          reduction:\n",
    "            Specifies the reduction to apply to the output:\n",
    "            'none' | 'sum' | 'mean'.\n",
    "            'none': no reduction will be applied.\n",
    "                    The returned 'loss' is a k2.RaggedTensor, with\n",
    "                    loss.tot_size(0) == batch_size.\n",
    "                    loss.tot_size(1) == total_num_paths_of_current_batch\n",
    "                    If you want the MWER loss for each utterance, just do:\n",
    "                    `loss_per_utt = loss.sum()`\n",
    "                    Then loss_per_utt.shape[0] should be batch_size.\n",
    "                    See more example usages in 'k2/python/tests/mwer_test.py'\n",
    "            'sum': sum loss of each path over the whole batch together.\n",
    "            'mean': divide above 'sum' by total num paths over the whole batch.\n",
    "          nbest_scale:\n",
    "            Scale `lattice.score` before passing it to :func:`k2.random_paths`.\n",
    "            A smaller value leads to more unique paths at the risk of being not\n",
    "            to sample the path with the best score.\n",
    "          num_paths:\n",
    "            Number of paths to **sample** from the lattice\n",
    "            using :func:`k2.random_paths`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.search_beam = search_beam\n",
    "        self.output_beam = output_beam\n",
    "        self.min_active_states = min_active_states\n",
    "        self.max_active_states = max_active_states\n",
    "\n",
    "        self.num_paths = num_paths\n",
    "        self.nbest_scale = nbest_scale\n",
    "        self.subsampling_factor = subsampling_factor\n",
    "\n",
    "        self.mwer_loss = k2.MWERLoss(\n",
    "            temperature=temperature,\n",
    "            use_double_scores=use_double_scores,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        emissions_lengths: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_length: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
    "                probability distribution over labels; output of acoustic model.\n",
    "            labels (torch.FloatTensor): CPU tensor of shape `(batch, label_len)` storing labels.\n",
    "            emissions_lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                in time axis of the output Tensor in each batch.\n",
    "            labels_length (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                label in each batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor:\n",
    "                Minimum Word Error Rate loss.\n",
    "        \"\"\"\n",
    "        H = k2.ctc_topo(\n",
    "            max_token=self.vocab_size-1,\n",
    "            modified=False,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "\n",
    "        supervision_segments = torch.stack(\n",
    "            (\n",
    "                torch.tensor(range(emissions_lengths.shape[0])),\n",
    "                torch.zeros(emissions_lengths.shape[0]),\n",
    "                emissions_lengths.cpu(),\n",
    "            ),\n",
    "            1,\n",
    "        ).to(torch.int32)\n",
    "\n",
    "        lattice = get_lattice(\n",
    "            nnet_output=emissions,\n",
    "            decoding_graph=H,\n",
    "            supervision_segments=supervision_segments,\n",
    "            search_beam=self.search_beam,\n",
    "            output_beam=self.output_beam,\n",
    "            min_active_states=self.min_active_states,\n",
    "            max_active_states=self.max_active_states,\n",
    "            subsampling_factor=self.subsampling_factor,\n",
    "        )\n",
    "\n",
    "        token_ids = []\n",
    "        for i in range(labels_length.size(0)):\n",
    "            token_ids.append(labels[i, : labels_length[i]].cpu().tolist())\n",
    "\n",
    "        loss = self.mwer_loss(\n",
    "            lattice, token_ids,\n",
    "            nbest_scale=self.nbest_scale,\n",
    "            num_paths=self.num_paths\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Gumbel Softmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "asd_scores = [0] * 10\n",
    "for i in range(10):\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=10, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_scores[i] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    print(\"asd:\", asd_scores[i], \"hyp:\", hyp_text)\n",
    "\n",
    "mean_asd = np.mean(asd_scores)\n",
    "loss[0] = mean_asd\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "def get_mass_prob(paths: Tensor,\n",
    "                        softmax_ctc: Tensor,\n",
    "                        model_pred_length: Tensor,\n",
    "                        eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    compute the path probability mass\n",
    "    :param paths: ctc alignments\n",
    "    :param softmax_ctc: model logits after softmax\n",
    "    :param model_pred_length:  max length of all given paths\n",
    "    :return: avg of the paths probability\n",
    "    \"\"\"\n",
    "    log_indexes_probs = softmax_ctc.gather(dim=1, index=paths) + eps\n",
    "    if len(log_indexes_probs) > model_pred_length:\n",
    "        log_indexes_probs[model_pred_length:, :] = torch.zeros((log_indexes_probs.shape[0] - model_pred_length, 0))\n",
    "    return torch.sum(log_indexes_probs, dim=0) / (model_pred_length.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: pågripelsen av toska gjorde altså denne pågripelsen mulig\n",
      "[0.037175211785556765, 0.037175211785556765, 0.1520948636867659, 0.24277221896598888]\n",
      "['pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåskha gjorde altså denne pågripelsen mulig']\n",
      "0\n",
      "tensor([0.2708, 0.1336, 0.1375], grad_fn=<CopySlices>)\n",
      "tensor(0.5420, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "input_lengths = model._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(output[\"logits\"][0], dim=-1, dtype=torch.float32)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "num_samples = 10\n",
    "non_zero_logits = []\n",
    "non_zero_asd = []\n",
    "non_zero_hyp = []\n",
    "# for i in range(num_samples):\n",
    "while len(non_zero_asd) < 4:\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=100, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    if asd_score != 0:\n",
    "        non_zero_logits.append(sampled_logits)\n",
    "        non_zero_asd.append(asd_score)\n",
    "        non_zero_hyp.append(hyp_text)\n",
    "\n",
    "print(non_zero_asd)\n",
    "print(non_zero_hyp)\n",
    "\n",
    "lowest_asd_idx = np.argmin(np.array(non_zero_asd))\n",
    "print(lowest_asd_idx)\n",
    "\n",
    "mass_prob_list = []\n",
    "for i in range(len(non_zero_asd)):\n",
    "    logits_selected = non_zero_logits[i].type(torch.LongTensor)\n",
    "    mass_prob_list.append(get_mass_prob(logits_selected, log_probs, input_lengths))\n",
    "\n",
    "loss = torch.zeros((len(mass_prob_list)-1))\n",
    "j=0\n",
    "for i in range(len(mass_prob_list)):\n",
    "    if i != lowest_asd_idx:\n",
    "        subtract_path_probs = mass_prob_list[i] - mass_prob_list[lowest_asd_idx]\n",
    "        loss[j] = torch.sum(torch.clamp((subtract_path_probs), min=0))\n",
    "        j += 1\n",
    "print(loss)\n",
    "print(torch.sum(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating aligned cosdist (1 batch, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    ref_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "\n",
    "    return ref_alignments\n",
    "\n",
    "\n",
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed\n",
    "\n",
    "\n",
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    return cosdist_for_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_register = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    input_feature = [{\"input_values\": input_values} for input_values in train_dataset[i*8:(i+1)*8][\"input_values\"]]\n",
    "    label_feature = [{\"input_ids\": labels} for labels in train_dataset[i*8:(i+1)*8][\"labels\"]]\n",
    "\n",
    "    batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "    label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = label\n",
    "\n",
    "    output = model(**batch)\n",
    "    output_logits = output[\"logits\"]\n",
    "    pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "    labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "    for i in range(len(pred_str)):\n",
    "        ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "        pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "        label_ids = labels[i]\n",
    "        labels_mask = label_ids >= 0\n",
    "        flattened_labels = label_ids.masked_select(labels_mask)\n",
    "        ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "        tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "        cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "        cosdist_register.append(cosdist_for_ctc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_976418/112888293.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cosdist_register_arr = np.asarray(cosdist_register)\n"
     ]
    }
   ],
   "source": [
    "cosdist_register_arr = np.asarray(cosdist_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAI/CAYAAAAGDwK6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhK0lEQVR4nO3dfYxl913f8c+3XohKQU7Ak9jyQ9esTOykCwamBpUGBdI0ThRhUhViFwWXhi5WEwQSf2STSjBqFSlqMbRVHSIDVhYJ5aE4EFcOD9auS4wghDUsjp3FxHkgWWLZmwQRxEOQnV//mGs8cWZ37s69d+7Mfl8vaTX3nnvOPd/d+OxO3nPPOTXGCAAAAAB9/KNlDwAAAADAzhKEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACa2bfsAZLkoosuGvv371/2GAAAAADnjfvvv/8zY4yVzV7bFUFo//79OX78+LLHAAAAADhvVNWfnek1p4wBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0s2UQqqrLq+reqjpZVQ9V1Y9Nln9tVd1TVR+ZfH3Ohm3eWFWPVNXDVfWyRf4GAAAAADg303xC6IkkPzHGuCbJtyd5XVW9IMnhJEfHGFclOTp5nslrNyZ5YZLrk7y1qi5YxPAAAAAAnLstg9AY49Exxh9OHv9VkpNJLk1yQ5Ijk9WOJPneyeMbkrxzjPGFMcbHkzyS5Lo5zw0AAADANp3TNYSqan+Sb07y+0meN8Z4NFmPRkmeO1nt0iSf2rDZqckyAAAAAHaBqYNQVX11kjuT/PgY4/NnW3WTZWOT9ztUVcer6vjp06enHQMAAACAGU0VhKrqK7Ieg355jPGeyeLHquqSyeuXJHl8svxUkss3bH5Zkk8/8z3HGLePMVbHGKsrKyvbnR8AAACAczTNXcYqyS8mOTnG+JkNL92V5ObJ45uTvHfD8hur6llVdWWSq5J8cH4jAwAAADCLfVOs8x1JXpPkQ1V1YrLsTUnekuTdVfXaJJ9M8n1JMsZ4qKreneTDWb9D2evGGE/Oe3AAAAAAtmfLIDTG+J1sfl2gJHnJGbZ5c5I3zzAXAAAAAAtyTncZAwAAAGDvE4QAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmtkyCFXVHVX1eFU9uGHZu6rqxOTXJ6rqxGT5/qr62w2vvW2BswMAAACwDfumWOftSf53kl96asEY49VPPa6qW5P85Yb1PzrGuHZO8wEAAAAwZ1sGoTHG+6tq/2avVVUl+f4k3z3nuQAAAABYkFmvIfSiJI+NMT6yYdmVVfVHVfXbVfWiGd8fAAAAgDmb5pSxs7kpyTs2PH80yRVjjM9W1bcm+bWqeuEY4/PP3LCqDiU5lCRXXHHFjGMAAAAAMK1tf0KoqvYl+TdJ3vXUsjHGF8YYn508vj/JR5N8w2bbjzFuH2OsjjFWV1ZWtjsGAAAAAOdollPG/lWSPxljnHpqQVWtVNUFk8dfn+SqJB+bbUQAAAAA5mma286/I8nvJXl+VZ2qqtdOXroxX3q6WJJ8Z5IHquqPk/xKklvGGJ+b58AAAAAAzGaau4zddIbl/36TZXcmuXP2sQAAAABYlFnvMgYAAADAHiMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0s2UQqqo7qurxqnpww7K1qvrzqjox+fWKDa+9saoeqaqHq+plixocAAAAgO2Z5hNCb09y/SbLf3aMce3k1/uSpKpekOTGJC+cbPPWqrpgXsMCAAAAMLstg9AY4/1JPjfl+92Q5J1jjC+MMT6e5JEk180wHwAAAABzNss1hF5fVQ9MTil7zmTZpUk+tWGdU5NlAAAAAOwS2w1CP5fkQJJrkzya5NbJ8tpk3bHZG1TVoao6XlXHT58+vc0xAAAAADhX2wpCY4zHxhhPjjG+mOTn8/RpYaeSXL5h1cuSfPoM73H7GGN1jLG6srKynTEAAAAA2IZtBaGqumTD01cleeoOZHclubGqnlVVVya5KskHZxsRAAAAgHnat9UKVfWOJC9OclFVnUryU0leXFXXZv10sE8k+ZEkGWM8VFXvTvLhJE8ked0Y48mFTA4AAADAttQYm17iZ0etrq6O48ePL3sMAAAAgPNGVd0/xljd7LVZ7jIGAAAAwB4kCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMI7XG33XJs2SMAAAAAe4wgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANDMlkGoqu6oqser6sENy/57Vf1JVT1QVb9aVc+eLN9fVX9bVScmv962wNkBAAAA2IZpPiH09iTXP2PZPUn+2RjjG5P8aZI3bnjto2OMaye/bpnPmAAAAADMy5ZBaIzx/iSfe8ay3xpjPDF5+oEkly1gNgAAAAAWYB7XEPoPSX59w/Mrq+qPquq3q+pFc3h/AAAAAOZo3ywbV9V/TvJEkl+eLHo0yRVjjM9W1bcm+bWqeuEY4/ObbHsoyaEkueKKK2YZAwAAAIBzsO1PCFXVzUlemeQHxhgjScYYXxhjfHby+P4kH03yDZttP8a4fYyxOsZYXVlZ2e4YAAAAAJyjbQWhqro+yRuSfM8Y4282LF+pqgsmj78+yVVJPjaPQQEAAACYjy1PGauqdyR5cZKLqupUkp/K+l3FnpXknqpKkg9M7ij2nUn+S1U9keTJJLeMMT636RsDAAAAsBRbBqExxk2bLP7FM6x7Z5I7Zx0KAAAAgMWZx13GAAAAANhDBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQhAAAAgGYEoeYuvvfEskcAAAAAdpggBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANDMlkGoqu6oqser6sENy762qu6pqo9Mvj5nw2tvrKpHqurhqnrZogYHAAAAYHum+YTQ25Nc/4xlh5McHWNcleTo5Hmq6gVJbkzywsk2b62qC+Y2LQAAAAAz2zIIjTHen+Rzz1h8Q5Ijk8dHknzvhuXvHGN8YYzx8SSPJLluPqMCAAAAMA/bvYbQ88YYjybJ5OtzJ8svTfKpDeudmiwDAAAAYJeY90Wla5NlY9MVqw5V1fGqOn769Ok5jwEAAADAmWw3CD1WVZckyeTr45Plp5JcvmG9y5J8erM3GGPcPsZYHWOsrqysbHMMAAAAAM7VdoPQXUlunjy+Ocl7Nyy/saqeVVVXJrkqyQdnGxEAAACAeZrmtvPvSPJ7SZ5fVaeq6rVJ3pLkpVX1kSQvnTzPGOOhJO9O8uEkv5HkdWOMJxc1/Plg/+G7lz0CAAAA0My+rVYYY9x0hpdecob135zkzbMMBQAAAMDizPui0gAAAADscoIQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzgtAudPTYgWWPAAAAAJzHBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgShhvYfvnvZIwAAAABLJAgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwgBAAAANCMIAQAAADQjCAEAAAA0IwixFBffe2LZIwAAAEBbghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzghAAAABAM4LQHnbrq1+57BEAAACAPUgQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaGbfdjesqucnedeGRV+f5CeTPDvJf0xyerL8TWOM9213PwAAAADM17aD0Bjj4STXJklVXZDkz5P8apIfSvKzY4yfnseAAAAAAMzXvE4Ze0mSj44x/mxO7wcAAADAgswrCN2Y5B0bnr++qh6oqjuq6jlz2gcAAAAAczBzEKqqr0zyPUn+z2TRzyU5kPXTyR5NcusZtjtUVcer6vjp06c3W2XPO3n1NcseAQAAAODLzOMTQi9P8odjjMeSZIzx2BjjyTHGF5P8fJLrNttojHH7GGN1jLG6srIyhzEAAAAAmMY8gtBN2XC6WFVdsuG1VyV5cA77AAAAAGBOtn2XsSSpqq9K8tIkP7Jh8X+rqmuTjCSfeMZrAAAAACzZTEFojPE3Sb7uGcteM9NEAAAAACzUvO4yRhO3vvqVyx4BAAAAmJEgBAAAANCMIAQAAADQjCDE1JwuBgAAAOcHQQgAAACgGUEIAAAAoBlBCAAAAKAZQQgAAACgGUEIAAAAoBlBCAAAAKAZQYgtnTp837JHAAAAAOZIEAIAAABoRhACAAAAaEYQWoCTV1+z7BEAAAAAzkgQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGEAIAAABoRhACAAAAaEYQAgAAAGhGENoFLr73xLJHAAAAABoRhAAAAACaEYQAAAAAmhGEzlMHjxxc9ggAAADALiUIAQAAADQjCLErrK2tLXsEAAAAaEMQAgAAAGhGEAIAAABoRhBiR+w/fPeyRwAAAAAmBCEAAACAZgQhAAAAgGYEoWVYu3DZEwAAAACNCUIAAAAAzQhCO8kngwAAAIBdQBACAAAAaEYQAgAAAGhGEAIAAABoZt8sG1fVJ5L8VZInkzwxxlitqq9N8q4k+5N8Isn3jzH+YrYxAQAAAJiXeXxC6LvGGNeOMVYnzw8nOTrGuCrJ0clzAAAAAHaJRZwydkOSI5PHR5J87wL2AQAAAMA2zRqERpLfqqr7q+rQZNnzxhiPJsnk63Nn3AcAAAAAczRrEPqOMca3JHl5ktdV1XdOu2FVHaqq41V1/PTp0zOOwZkcPHJw2SMAAAAAu8xMQWiM8enJ18eT/GqS65I8VlWXJMnk6+Nn2Pb2McbqGGN1ZWVlljEAAAAAOAfbDkJV9U+q6mueepzkXyd5MMldSW6erHZzkvfOOiQAAAAA8zPLJ4Sel+R3quqPk3wwyd1jjN9I8pYkL62qjyR56eQ555Hbbjm2sPdeW1tLkpw6fN/C9gEAAADd7dvuhmOMjyX5pk2WfzbJS2YZCgAAAIDFWcRt51myk1dfs+wRAAAAgF1MEAIAAABoRhACAAAAaEYQ4ss8dWFnAAAA4PwkCAEAAAA0IwgBAAAANCMIkSQ5euzAtrZzRzMAAADYewQhAAAAgGYEoTm77ZZjyx4BAAAA4KwEIQAAAIBmBCEAAACAZgQhzurU4fuWPQIAAAAwZ4IQAAAAQDOC0IIdPHJw2SMAAAAAfAlBCAAAAKAZQQgAAACgGUHoPHDbLcfm8j5Hjx2Yy/sAAAAAu5sgBAAAANCMIAQAAADQjCDUwdqFS939/sN3L3X/AAAAwJcShAAAAACaEYQAAAAAmhGEAAAAAJoRhAAAAACaEYQAAAAAmhGEAAAAAJoRhPaYU4fvW/YI52aLW94fPXZghwYBAAAAniIIAQAAADQjCAEAAAA0IwjtgINHDi57hE1dfO+JZY8AAAAALIEgBAAAANCMIAQAAADQjCDEjnKaGgAAACyfIAQAAADQjCDUxdqFy54AAAAA2CUEIQAAAIBmBCEAAACAZgQhAAAAgGYEIQAAAIBmBCEAAACAZgQh5urgkYPLHgEAAADYgiAEAAAA0IwgtET7D9+97BEAAACAhgQhAAAAgGYEIQAAAIBmBKFd5OJ7Tyx2B2sXLvb9p9zfmX6fa2tri5sFAAAA+AeCEAAAAEAzghAAAABAM4IQAAAAQDOCEAAAAEAzgtAudfTYgW1td/Lqa+Y8yTb3udMXsAYAAACmJggBAAAANCMIAQAAADQjCDEXJ6++JgePHDzrOvsP371D0wAAAABnIwgBAAAANCMI7WJra2s7sp+L7z2xI/sBAAAAdodtB6Gquryq7q2qk1X1UFX92GT5WlX9eVWdmPx6xfzGBQAAAGBW+2bY9okkPzHG+MOq+pok91fVPZPXfnaM8dOzjwcAAADAvG07CI0xHk3y6OTxX1XVySSXzmswlmttbS0//HcvWeg+jh47sND3BwAAADY3l2sIVdX+JN+c5Pcni15fVQ9U1R1V9Zx57AMAAACA+Zg5CFXVVye5M8mPjzE+n+TnkhxIcm3WP0F06xm2O1RVx6vq+OnTp2cdgznZqQtZAwAAAMszUxCqqq/Iegz65THGe5JkjPHYGOPJMcYXk/x8kus223aMcfsYY3WMsbqysjLLGAAAAACcg1nuMlZJfjHJyTHGz2xYfsmG1V6V5MHtjwcAAADAvM1yl7HvSPKaJB+qqhOTZW9KclNVXZtkJPlEkh+ZYR8AAAAAzNksdxn7nSS1yUvv2/44AAAAACzaXO4yBgAAAMDeIQixbbfdcmzZIwAAAADbIAgBAAAANCMInccOHjm47BEAAACAXUgQAgAAAGhGEAIAAABoRhA6j5y8+pqFvfetr37lwt4bAAAA2FmCEAAAAEAzghAAAABAM4LQgszr9K21tbUvW+b0LQAAAGAWghAAAABAM4LQLnHxvSfO+vqpw/ftzCDAUhw8cnDZIwAAAI0IQgAAAADNCEIAAAAAzQhCO2Ta00GOHjuwrfe/7ZZj29puuxa5P6fHAQAAwGIJQgAAAADNCEIAwHlj/+G7lz0CAMCeIAgBAAAANCMIAQAAADQjCAG739qFy56As3AheAAA2HsEIQAAAIBmBCEAAACAZgShJVn2XVCWvX8AAABgeQQhAAAAgGYEIWjGBYA5X9z66lfO7b0uvvfE3N4LAAD2AkEIAAAAoBlBCAAAAKAZQWinrV247AkAAIAGnBINnI0gBAAAANCMILRHzfNiqh0dPXbgnLc5efU1C5gEAAAAdp4gBAAAANCMIAQAAADQjCAEu4TTAPtw+iEAALBsghAAAABAM4IQAAAAQDOC0B6wtra27BGAbXIqIMzXwSMHlz0CAMB5QRACAAAAaEYQgm0600+p9x++e4cnYSunDt831/e7+N4Tc30/APaGo8cOLHsEpuCThADTEYQAAAAAmhGEAAAAAJoRhGCPm/YUNRcn3x6nALKV2245tuwR2GPmfRorAOuc1g/nRhACAAAAaEYQ2kP2yk8UXcgP2Em3vvqVyx4BoIW98r1oJ77vBmYhCAEAAAA0IwgBAAAANCMIwXmiy8WPu/w+z+bosQP/8NjFwtlrNv73C3DeWbtwrm+3V25ccD59f3by6muWPQLsGEEIAAAAoBlBCAAAAKAZQQia2813aJr2zhmznoLio8GNTPFRfqc0PW03//2wmfPplAVg99hNfxf6ngWYJ0EIAAAAoBlBCNjStJ/U2Yv8pA1gPlzkfuedOnzfluv4d25nnM/fKwHnL0EIAAAAoBlBCAAAAKAZQaiZ3XjBzT1/AdcpLlK71/br4+VbW/R/t9s5Vqc5dWDZFv2R+ttuOXZO62/nz/lsFxd17CyWP1/YPqc0sTDL+l4YmJkgBAAAANCMIAQAAADQjCAEmzjbKSF72fl+B5jddPrhXjh9ayecT6f4+N90sTr/+Z7rqY7d7MbT3feqg0cOnvPpPcv+3mGn/m7Y+Pvs/PfRXuPvB5jNwoJQVV1fVQ9X1SNVdXhR+wEAAADg3CwkCFXVBUluS/LyJC9IclNVvWAR+4LWXMQPmNLF955Y9gjADjqfPqEJwGIs6hNC1yV5ZIzxsTHG3yd5Z5IbFrQvAAAAAM7BooLQpUk+teH5qckyAAAAAJasxhjzf9Oq70vysjHGD0+evybJdWOMH92wzqEkhyZPn5/k4bkPMl8XJfnMsocApuJ4hb3FMQt7i2MW9hbHbG//dIyxstkL+xa0w1NJLt/w/LIkn964whjj9iS3L2j/c1dVx8cYq8ueA9ia4xX2Fscs7C2OWdhbHLOcyaJOGfuDJFdV1ZVV9ZVJbkxy14L2BQAAAMA5WMgnhMYYT1TV65P8ZpILktwxxnhoEfsCAAAA4Nws6pSxjDHel+R9i3r/Jdgzp7cBjlfYYxyzsLc4ZmFvccyyqYVcVBoAAACA3WtR1xACAAAAYJcShDaoquur6uGqeqSqDm/yelXV/5q8/kBVfcsy5gTWTXHM/sDkWH2gqn63qr5pGXMC67Y6Zjes98+r6smq+rc7OR/wpaY5ZqvqxVV1oqoeqqrf3ukZgadN8b3xhVX1f6vqjyfH7A8tY052D6eMTVTVBUn+NMlLk5zK+p3SbhpjfHjDOq9I8qNJXpHk25L8zzHGty1hXGhvymP2XyQ5Ocb4i6p6eZI1xywsxzTH7Ib17knyd1m/KcWv7PSswNT/zj47ye8muX6M8cmqeu4Y4/FlzAvdTXnMvinJhWOMN1TVSpKHk1w8xvj7ZczM8vmE0NOuS/LIGONjkwPinUlueMY6NyT5pbHuA0meXVWX7PSgQJIpjtkxxu+OMf5i8vQDSS7b4RmBp03z72yy/oOXO5P4P5WwXNMcs/8uyXvGGJ9MEjEIlmqaY3Yk+ZqqqiRfneRzSZ7Y2THZTQShp12a5FMbnp+aLDvXdYCdca7H42uT/PpCJwLOZstjtqouTfKqJG/bwbmAzU3z7+w3JHlOVf2/qrq/qn5wx6YDnmmaY/Z/J7kmyaeTfCjJj40xvrgz47EbLey283tQbbLsmefTTbMOsDOmPh6r6ruyHoT+5UInAs5mmmP2fyR5wxjjyfUfXgJLNM0xuy/JtyZ5SZJ/nOT3quoDY4w/XfRwwJeZ5ph9WZITSb47yYEk91TVfWOMzy94NnYpQehpp5JcvuH5ZVkvp+e6DrAzpjoeq+obk/xCkpePMT67Q7MBX26aY3Y1yTsnMeiiJK+oqifGGL+2IxMCG037vfFnxhh/neSvq+r9Sb4p69cxAXbWNMfsDyV5y1i/kPAjVfXxJFcn+eDOjMhu45Sxp/1Bkquq6sqq+sokNya56xnr3JXkByd3G/v2JH85xnh0pwcFkkxxzFbVFUnek+Q1floJS7flMTvGuHKMsX+MsT/JryT5T2IQLM003xu/N8mLqmpfVX1V1m+6cnKH5wTWTXPMfjLrn+hLVT0vyfOTfGxHp2RX8QmhiTHGE1X1+iS/meSCrN/Z5KGqumXy+tuSvC/rdxh7JMnfZL2wAksw5TH7k0m+LslbJ584eGKMsbqsmaGzKY9ZYJeY5pgdY5ysqt9I8kCSLyb5hTHGg8ubGvqa8t/Z/5rk7VX1oayfYvaGMcZnljY0S+e28wAAAADNOGUMAAAAoBlBCAAAAKAZQQgAAACgGUEIAAAAoBlBCAAAAKAZQQgAAACgGUEIAAAAoBlBCAAAAKCZ/w/RxuhXDKHp4QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(cosdist_register_arr.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    numphones = params.shape[0]  # number of labels, not even used in the code\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    # print(seqLen, numphones, L, T)\n",
    "\n",
    "    alphas = np.zeros((L,T))\n",
    "    betas = np.zeros((L,T))\n",
    "\n",
    "    # convert logits to log softmax\n",
    "    params = params - np.max(params,axis=0)\n",
    "    params = np.exp(params)\n",
    "    params = params / np.sum(params,axis=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = np.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = np.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = np.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward += np.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = np.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = np.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = np.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward += np.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = np.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = np.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = np.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or np.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(np.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_tensor(logits, seq, cosdist_for_ctc, blank=0):\n",
    "\n",
    "    cosdist_contribution_alphas = []\n",
    "    cosdist_contribution_betas = []\n",
    "\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    # print(\"label seq:\", seqLen)\n",
    "    # print(\"label seq length with blanks:\", L)\n",
    "    # print(\"utterance length:\", T)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # print(0, L-2*(T-t), \"time:\", t, \"start:\", start, \"L:\", L, \"s:\", s, \"l:\", l)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    # llDiff = torch.abs(llForward-llBackward)\n",
    "    # if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "    #     print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "    #     print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "    #     return (-llForward, grad)\n",
    "    # else:\n",
    "    #     grad = params - grad / (params * absum)\n",
    "    #     return (-llForward, grad)\n",
    "\n",
    "    for t in range(T):\n",
    "        for s in range(numphones):\n",
    "            tmp = (params[s,t]*absum[t])\n",
    "            if tmp > 0:\n",
    "                grad[s,t] = params[s,t] - grad[s,t] / tmp\n",
    "            else:\n",
    "                grad[s,t] = params[s,t]\n",
    "\n",
    "    return (-llForward, grad, sum(cosdist_contribution_alphas), sum(cosdist_contribution_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_newgrad(logits, seq, blank=0):\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient according to https://github.com/yehudabab/NumpyCTC/blob/main/ctc.py\n",
    "    padded_labels = torch.zeros((L))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(L):\n",
    "        if i%2 == 0:\n",
    "            padded_labels[i] = 0\n",
    "        else:\n",
    "            padded_labels[i] = seq[j]\n",
    "            j += 1\n",
    "\n",
    "    # print(len(seq), seq)\n",
    "    # print(len(padded_labels), padded_labels)\n",
    "\n",
    "    grad = torch.zeros(params.shape)\n",
    "\n",
    "    score_last = alphas[L-1, T-1]\n",
    "    score_before_last = betas[L-2, T-1]\n",
    "    p_l_given_ctc = score_last + score_before_last\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(numphones):\n",
    "            d_p_d_ytk = 0\n",
    "            lb_lk = np.nonzero(list(map(lambda x: 1 if k in x else 0, padded_labels)))[0]\n",
    "            for s in lb_lk:\n",
    "                d_p_d_ytk += alphas[s, t] * betas[s, t]\n",
    "\n",
    "            d_p_d_ytk /= (params[k, t] ** 2)\n",
    "            d_lnp_d_ytk = (1. / p_l_given_ctc) * d_p_d_ytk\n",
    "            grad[k, t] = d_lnp_d_ytk\n",
    "\n",
    "    return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Stanford CTC Loss script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].detach().cpu().numpy().transpose()\n",
    "seq = batch[\"labels\"][0].detach().cpu().numpy()\n",
    "\n",
    "loss, grad = ctc_loss(logits, seq.squeeze(), blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(output[\"logits\"][0], dim=1)\n",
    "print(token_ids)\n",
    "for token in token_ids:\n",
    "    if token == 33:\n",
    "        print(\"HUY\")\n",
    "\n",
    "decoded_tokens = []\n",
    "for token in token_ids:\n",
    "    decoded_tokens.append(model_tokenizer.decode(token))\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "targets = batch[\"labels\"][0]\n",
    "input_lengths = torch.tensor(output[\"logits\"][0].shape[0])\n",
    "target_lengths = torch.tensor(batch[\"labels\"][0].shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=targets, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].transpose(1,0)\n",
    "seq = batch[\"labels\"][0]\n",
    "\n",
    "loss, grad = ctc_loss_tensor(logits, seq, blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad.shape)\n",
    "transposed_grad = grad.transpose(1,0)\n",
    "print(transposed_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad.shape[0]):\n",
    "    print(i, grad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosdist alphas: 845.0103612840176\n",
      "cosdist betas: 972.7007383406162\n",
      "torch.Size([201, 34]) tensor(608.0905, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1854.4355, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 606.3643108587712\n",
      "cosdist betas: 550.9175247717649\n",
      "torch.Size([201, 34]) tensor(680.7924, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1999.4266, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1113.2689, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 307.15736174583435\n",
      "cosdist betas: 318.57188880443573\n",
      "torch.Size([201, 34]) tensor(2163.9688, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 2056.844472726263\n",
      "cosdist betas: 1902.4644071857585\n",
      "torch.Size([201, 34]) tensor(860.0428, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_to_inspect = []\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    loss, grad, cosdist_contribution_alphas, cosdist_contribution_betas = ctc_loss_tensor(logits, flattened_labels, cosdist_for_ctc)\n",
    "    grad_to_inspect.append(grad)\n",
    "\n",
    "    print(\"cosdist alphas:\", cosdist_contribution_alphas)\n",
    "    print(\"cosdist betas:\", cosdist_contribution_betas)\n",
    "    print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for logits in output_logits:\n",
    "    token_ids = torch.argmax(logits, dim=1)\n",
    "    print(token_ids)\n",
    "\n",
    "    decoded_tokens = []\n",
    "    for token in token_ids:\n",
    "        decoded_tokens.append(model_tokenizer.decode(token))\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing only the first utterance\n",
    "model.train()\n",
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "logits = output_logits[0]\n",
    "label_ids = labels[0]\n",
    "labels_mask = label_ids >= 0\n",
    "flattened_labels = label_ids.masked_select(labels_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([235, 34])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "# python-implemented CTC loss\n",
    "loss, grad = ctc_loss_tensor(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "loss, grad = ctc_loss_newgrad(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6440, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pytorch CTC loss\n",
    "import torch.nn.functional as F\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = labels_mask.sum(-1)\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=flattened_labels, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.grad)\n",
    "#         print(param.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating ASD into CTC Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: start running here! (when loading from pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOV-8 DATA\n",
    "# ref = \"wozniackis drøm om seier knust av russiske jekaterina\"\n",
    "# hyp = \"asnake drøm om sier knut av russiske karine\"\n",
    "\n",
    "# logit_frames_decoded = pickle.load(open(\"logit_frames_decoded_8Nov.pkl\", \"rb\"))\n",
    "\n",
    "# alignment_table = [logit_frames_decoded]\n",
    "# table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "# print(\"logits decoded\")\n",
    "# display(HTML(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    cosdist_alignment_tokens = []  # FOR PRINTING\n",
    "    ref_alignments = []\n",
    "    # hyp_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        cosdist_alignment_tokens.append(round(cosdist, 3))  # FOR PRINTING\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        # hyp_token = asd_tokenizer.convert_ids_to_tokens(hyp_input_ids[hyp_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "        # hyp_alignments.append((hyp_alignment_idxs[i], hyp_token, cosdist))\n",
    "\n",
    "    # FOR PRINTING\n",
    "    # ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(ref_alignment_idxs):\n",
    "    #     ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    # ref_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(ref_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(hyp_alignment_idxs):\n",
    "    #     hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    # hyp_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(hyp_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    # ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    # cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    # alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    # table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    # print(\"Token alignment table:\")\n",
    "    # display(HTML(table))\n",
    "\n",
    "    return ref_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    # print(tokens_compressed)\n",
    "    # if len(label_ids) == len(cosdist_for_ctc):\n",
    "    #     print(\"SAME\")\n",
    "    # else:\n",
    "    #     print(\"DIFF!\")\n",
    "    # for i, label in enumerate(label_ids):\n",
    "    #     print(label, cosdist_for_ctc[i])\n",
    "\n",
    "    # try normalizing\n",
    "    x = np.array(cosdist_for_ctc)\n",
    "    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    if np.isnan(np.sum(x_norm)):\n",
    "        print(\"boo!\")\n",
    "\n",
    "    return cosdist_for_ctc, x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07505041360855103, 0.07505041360855103, 0.07505041360855103, 0, 0.12715423107147217, 0.12715423107147217, 0.12715423107147217, 0, 0.029700636863708496, 0, 0.13597166538238525, 0.13597166538238525, 0, 0.5323134958744049, 0.5323134958744049, 0, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0, 0.6916015446186066, 0.6916015446186066, 0, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0, 0, 0.032095909118652344, 0.032095909118652344, 0, 0.018728435039520264, 0.018728435039520264, 0.018728435039520264, 0, 0.016046226024627686, 0.016046226024627686, 0, 0.016792714595794678, 0.016792714595794678, 0, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414]\n",
      "[0.10851684 0.10851684 0.10851684 0.         0.18385475 0.18385475\n",
      " 0.18385475 0.         0.04294472 0.         0.19660405 0.19660405\n",
      " 0.         0.76968234 0.76968234 0.         0.30526938 0.30526938\n",
      " 0.30526938 0.30526938 0.         0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.0306659  0.0306659  0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.         0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.         0.03278869 0.03278869 0.03278869\n",
      " 0.03278869 0.         1.         1.         0.         0.22142421\n",
      " 0.22142421 0.22142421 0.22142421 0.         0.         0.04640809\n",
      " 0.04640809 0.         0.02707981 0.02707981 0.02707981 0.\n",
      " 0.02320155 0.02320155 0.         0.02428091 0.02428091 0.\n",
      " 0.02478112 0.02478112 0.02478112 0.02478112 0.         0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4022911/3475542975.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007969975471496582, 0.007969975471496582, 0.007969975471496582, 0, 0.006086826324462891, 0.006086826324462891, 0, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0, 0.015287041664123535, 0.015287041664123535, 0, 0.0193021297454834, 0.0193021297454834, 0, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144]\n",
      "[0.02681489 0.02681489 0.02681489 0.         0.02047905 0.02047905\n",
      " 0.         0.02253498 0.02253498 0.02253498 0.02253498 0.02253498\n",
      " 0.         0.05143307 0.05143307 0.         0.06494178 0.06494178\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "[0.03979825973510742, 0.03979825973510742, 0.03979825973510742, 0, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0, 0.01816505193710327, 0.01816505193710327, 0.01816505193710327, 0, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0]\n",
      "[0.22152116 0.22152116 0.22152116 0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.         0.10110853 0.10110853 0.10110853 0.         0.04921078\n",
      " 0.04921078 0.04921078 0.04921078 0.04921078 0.        ]\n",
      "[0.1013750433921814, 0.1013750433921814, 0, 0.3244396448135376, 0.3244396448135376, 0.3244396448135376, 0, 0.03615701198577881, 0, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0, 0.018376052379608154, 0.018376052379608154, 0.018376052379608154, 0, 0.032556354999542236, 0, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0, 0.6256595551967621, 0.6256595551967621, 0, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566]\n",
      "[0.16202908 0.16202908 0.         0.51855621 0.51855621 0.51855621\n",
      " 0.         0.05779023 0.         0.03629764 0.03629764 0.03629764\n",
      " 0.03629764 0.03629764 0.         0.02937069 0.02937069 0.02937069\n",
      " 0.         0.05203526 0.         0.03731947 0.03731947 0.03731947\n",
      " 0.03731947 0.         1.         1.         0.         0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.         0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449]\n"
     ]
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    max_per_frame = torch.argmax(logits, dim=1)\n",
    "    relevant_frames = []\n",
    "    for max_id in max_per_frame:\n",
    "        if max_id in flattened_labels:\n",
    "            relevant_frames.append(1)\n",
    "        else:\n",
    "            relevant_frames.append(0)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc, x_norm = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    print(cosdist_for_ctc)\n",
    "    print(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CTC with ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCORPORATING ASD COSDIST VALUES TO THE CTC CALCULATION\n",
    "\n",
    "def ctc_loss_with_ASD(params, seq, cosdist_for_ctc, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0].clone() / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])  # scale 0 to 1\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone() + alphas[s-2,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t].clone() / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1].clone() / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone() + betas[s+2,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t].clone() / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = torch.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out custom ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import gradcheck\n",
    "\n",
    "# output = model(**batch)\n",
    "# ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "# label_ids = batch[\"labels\"][0]\n",
    "# logits = output[\"logits\"][0]\n",
    "# hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "# ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "# tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "# cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "# inputs = (logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "\n",
    "# test = gradcheck(ctc_loss_with_ASD, inputs)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "print(tokens_compressed)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "loss, grad = ctc_loss_with_ASD(logits.transpose(1,0), label_ids, cosdist_for_ctc, blank=0)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = torch.tensor(label_ids.shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=label_ids, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extending torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCTC(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, seq, cosdist_for_ctc, blank=0):\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "        T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "        alphas = torch.zeros((L,T)).double()\n",
    "        betas = torch.zeros((L,T)).double()\n",
    "\n",
    "        # convert logits to log probs\n",
    "        params = params - (torch.max(params, dim=0)[0])\n",
    "        params = torch.exp(params)\n",
    "        params = params / torch.sum(params, dim=0)\n",
    "\n",
    "        # initialize alphas and forward pass\n",
    "        alphas[0,0] = params[blank,0]\n",
    "        alphas[1,0] = params[seq[0],0]\n",
    "        c = torch.sum(alphas[:,0])\n",
    "        alphas[:,0] = alphas[:,0] / c\n",
    "        llForward = torch.log(c)\n",
    "\n",
    "        for t in range(1,T):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(start,L):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s==0:\n",
    "                        alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                    else:\n",
    "                        alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == 1 or seq[l] == seq[l-1]:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time (prevent underflow)\n",
    "            c = torch.sum(alphas[start:end,t])\n",
    "            alphas[start:end,t] = alphas[start:end,t] / c\n",
    "            llForward = llForward + torch.log(c)\n",
    "\n",
    "        # initialize betas and backwards pass\n",
    "        betas[-1,-1] = params[blank,-1]\n",
    "        betas[-2,-1] = params[seq[-1],-1]\n",
    "        c = torch.sum(betas[:,-1])\n",
    "        betas[:,-1] = betas[:,-1] / c\n",
    "        llBackward = torch.log(c)\n",
    "\n",
    "        for t in range(T-2,-1,-1):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(end-1,-1,-1):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s == L-1:\n",
    "                        betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                    else:\n",
    "                        betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time\n",
    "            c = torch.sum(betas[start:end,t])\n",
    "            betas[start:end,t] = betas[start:end,t] / c\n",
    "            llBackward = llBackward + torch.log(c)\n",
    "\n",
    "        ctx.save_for_backward(params, seq, alphas, betas, llForward, llBackward)\n",
    "\n",
    "        return -llForward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        params, seq, alphas, betas, llForward, llBackward = ctx.saved_tensors\n",
    "        blank = 0\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "\n",
    "        # Compute gradient with respect to unnormalized input parameters\n",
    "        grad = torch.zeros(params.shape)\n",
    "        ab = alphas*betas\n",
    "        for s in range(L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/params[blank,:]\n",
    "            else:\n",
    "                grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "        absum = torch.sum(ab,axis=0)\n",
    "\n",
    "        llDiff = torch.abs(llForward-llBackward)\n",
    "        if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "            return (grad, None, None)\n",
    "        else:\n",
    "            grad = params - grad / (params * absum)\n",
    "            return (grad, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "print(tokens_compressed)\n",
    "\n",
    "myctcloss = MyCTC.apply\n",
    "loss = myctcloss(logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS w/ FRAME NOS. & DISREGARDING ALL THE PADS\n",
    "\n",
    "clean_logit_frames = []\n",
    "for i, item in enumerate(logit_frames_decoded):\n",
    "    if item != \"[PAD]\":\n",
    "        clean_logit_frames.append((i, item))\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(clean_logit_frames):\n",
    "    if i == 0:\n",
    "        if char[1] == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if char[1] != \"\":\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if clean_logit_frames[i-1][1] == \"\" and char[1] != \"\":  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "            elif char[1] != \"\":\n",
    "                new_word.append(char)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(\" \".join([char[1] for char in item]))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS MATRIX INTO THEIR CORRESPONDING WORDS, WHILE RETAINING ROW NOS. OR FRAME NOS.\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(logit_frames_decoded):\n",
    "    if i == 0:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if logit_frames_decoded[i-1] in [\"\", \"[PAD]\"] and char not in [\"\", \"[PAD]\"]:\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if \"\" in logit_frames_decoded[i-2:i-1] and char not in [\"\", \"[PAD]\"]:  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "            elif logit_frames_decoded[i-1] == char and char != \"\":  # repeating characters\n",
    "                new_word.append((i, char))\n",
    "            elif logit_frames_decoded[i-1] != char and char != \"\":  # new character detected\n",
    "                new_word.append((i, char))\n",
    "\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # detecting applicable frames & caclulating average cosdist\n",
    "        # frame_group = []\n",
    "        # for token_char in token_char_list:\n",
    "        #     detect_count = 0\n",
    "        #     if token_char in [x[1] for x in char_group_copy]:\n",
    "        #         for i, char in enumerate(char_group_copy):\n",
    "        #             # print(char[1], token_char, [x[1] for x in char_group_copy])\n",
    "        #             if char[1] == token_char:\n",
    "        #                 frame_group.append(char)\n",
    "        #                 detect_count += 1\n",
    "        #             else:\n",
    "        #                 if detect_count > 0:\n",
    "        #                     for count in range(detect_count):\n",
    "        #                         char_group_copy.pop(0)\n",
    "        #                     break\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # cosdist_divided = token_cosdist / len(frame_group)  # divide ASD token cosdist by number of frames for word\n",
    "        # assign divided ASD token cosdist\n",
    "        # frame_cosdist = []\n",
    "        # for char in frame_group:\n",
    "        #     if char[1] in token_char_list:\n",
    "        #         frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING THE ASD COSDIST PER NO. FRAMES\n",
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
