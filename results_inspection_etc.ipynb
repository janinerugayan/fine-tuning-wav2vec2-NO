{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origloss_NBTale = pd.read_csv(\"./logs/OrigLoss_V5/NBTale_results_wav2vec2_NO_origLossV5_2023-09-18.csv\")\n",
    "# origloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "# origloss_NBTale.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/OrigLoss_V5/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "origloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)\n",
    "    origloss_dfs.append(df)\n",
    "\n",
    "origloss = pd.concat(origloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customloss_NBTale = pd.read_csv(\"./logs/CustomLoss_V7/NBTale_results_wav2vec2_NO_customLossV7_2023-10-02.csv\")\n",
    "# customloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "# customloss_NBTale.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/CustomLoss_V7/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "customloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)\n",
    "    customloss_dfs.append(df)\n",
    "\n",
    "customloss = pd.concat(customloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged results of orig loss & custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>ref_str</th>\n",
       "      <th>asr_origloss</th>\n",
       "      <th>wer_origloss</th>\n",
       "      <th>asd_origloss</th>\n",
       "      <th>asr_customloss</th>\n",
       "      <th>wer_customloss</th>\n",
       "      <th>asd_customloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_0</td>\n",
       "      <td>kraftselskapet hafslund blir solgt hvis prisen er god nok sier oslo kommune som ikke vil la seg presse av regjeringen</td>\n",
       "      <td>kraftselskapet hafslund blir solgt hvis prisen er god nok sier oslo kommune som ikke vil la seg presse av regjeringen</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kraftselskapet hafslund blir solgt hvis prisen er god nok sier oslo kommune som ikke vil la seg presse av regjeringen</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_1</td>\n",
       "      <td>kvinnen som var savnet i et sjøfly i telemark er funnet omkommet og folk sender gaver til maud angelica men</td>\n",
       "      <td>kvinnen som var savnet i et sjøfly i telemark er funnet omkommet og folk sender gaver til mod angelika men</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.123614</td>\n",
       "      <td>kvinnen som var savnet i et sjøfly i telemark er funnet omkommet og folk sender gaver til mod angelika men</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.123614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_2</td>\n",
       "      <td>hva hun får er hemmelig her er nrk dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo</td>\n",
       "      <td>hva hun får er hemmelig her er n r k dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.136711</td>\n",
       "      <td>hva hun får er hemmelig her er n r k dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.136711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_3</td>\n",
       "      <td>kommune vil ikke la seg presse til å beholde aksjene i selskapet olje og energiministeren vil stanse salget for å</td>\n",
       "      <td>kommune vil ikke la seg presse til å beholde aksjene i selskapet olje og energiministeren vil stanse salget for å</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kommune vil ikke la seg presse til å beholde aksjene i selskapet olje og energiministeren vil stanse salget for</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.052995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_4</td>\n",
       "      <td>sikre norske eiere i hafslund men finansbyråd i oslo andré støylen lar seg ikke presse og sier at selskapet blir</td>\n",
       "      <td>sikre norske eiere i hafslund men finansbyråd i oslo andre støylen lar seg ikke presse og sier at selskapet blir</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070778</td>\n",
       "      <td>sikre norske eiere i hafslund men finansbyråd i oslo andre støylen lar seg ikke presse og sier at selskapet blir</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.070778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_292</td>\n",
       "      <td>økonomiske utviklingen til kommunen ikke var bærekraftig det er sterke ord i en perspektivmelding  resultatet av det var at høyre</td>\n",
       "      <td>økonomiske utviklingen til kommunen ikke var bærekraftig og det er sterke ord i en perspektivmelding resultatet av det var at høyre</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.061569</td>\n",
       "      <td>økonomiske utviklingen til kommunen ikke var bærekraftig og det er sterke ord i en perspektivmelding resultatet av det var at høyre</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.061569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_293</td>\n",
       "      <td>frp   med frp finansbyråd innførte eiendomsskatt det var ikke noe venstre eller krf i det byrådet som tvang dem til</td>\n",
       "      <td>frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de til det det</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.229273</td>\n",
       "      <td>frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de tider</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.234647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_294</td>\n",
       "      <td>det det var frps finansbyråd  frps  egen kollega her på stortinget silje hjemdal som i dag er representant på stortinget</td>\n",
       "      <td>frp sin finansbyråd og frp sin egen kollega her på stortinget silje hjemdal som i dag er representant her på stortinget</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.278696</td>\n",
       "      <td>frp sin finansbyråd og frp sin egen kollega her på stortinget silje hjemdal som i dag er representant her på stortinget</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.278696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_295</td>\n",
       "      <td>satt i dette byrådet så det er bare å forvente at kollegaene hennes  går tilbake fra denne saken og unnskylder</td>\n",
       "      <td>satt i dette byrådet så det er bare til å forvente at kollegaene hennes går tilbake fra denne saken unnskylder</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.098308</td>\n",
       "      <td>satt i dette byrådet så det er jo bare til å forvente at kollegaene hennes går tilbake fra denne saken unnskylder</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.137447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_296</td>\n",
       "      <td>språkbruken om at hun var en grisk kommunepolitiker i tjue femten da frp innførte eiendomsskatt i bergen</td>\n",
       "      <td>språkbruken fordi at hun var en grisk kommunepolitiker i tjue femten da frp innførte eiendomsskatt i bergen takk</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.098546</td>\n",
       "      <td>språkbruken fordi at hun var en grisk kommunepolitiker i tjuefemten da frp innførte eiendomsskatt i bergen takk</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.175020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           segment_id  \\\n",
       "0    N2_030505_NRK_D12_NO_cut_0                                         \n",
       "1    N2_030505_NRK_D12_NO_cut_1                                         \n",
       "2    N2_030505_NRK_D12_NO_cut_2                                         \n",
       "3    N2_030505_NRK_D12_NO_cut_3                                         \n",
       "4    N2_030505_NRK_D12_NO_cut_4                                         \n",
       "..                          ...                                         \n",
       "824  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_292   \n",
       "825  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_293   \n",
       "826  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_294   \n",
       "827  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_295   \n",
       "828  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_296   \n",
       "\n",
       "                                                                                                                                ref_str  \\\n",
       "0    kraftselskapet hafslund blir solgt hvis prisen er god nok sier oslo kommune som ikke vil la seg presse av regjeringen                \n",
       "1    kvinnen som var savnet i et sjøfly i telemark er funnet omkommet og folk sender gaver til maud angelica men                          \n",
       "2    hva hun får er hemmelig her er nrk dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo               \n",
       "3    kommune vil ikke la seg presse til å beholde aksjene i selskapet olje og energiministeren vil stanse salget for å                    \n",
       "4    sikre norske eiere i hafslund men finansbyråd i oslo andré støylen lar seg ikke presse og sier at selskapet blir                     \n",
       "..                                                                                                                 ...                    \n",
       "824  økonomiske utviklingen til kommunen ikke var bærekraftig det er sterke ord i en perspektivmelding  resultatet av det var at høyre    \n",
       "825  frp   med frp finansbyråd innførte eiendomsskatt det var ikke noe venstre eller krf i det byrådet som tvang dem til                  \n",
       "826  det det var frps finansbyråd  frps  egen kollega her på stortinget silje hjemdal som i dag er representant på stortinget             \n",
       "827  satt i dette byrådet så det er bare å forvente at kollegaene hennes  går tilbake fra denne saken og unnskylder                       \n",
       "828  språkbruken om at hun var en grisk kommunepolitiker i tjue femten da frp innførte eiendomsskatt i bergen                             \n",
       "\n",
       "                                                                                                                            asr_origloss  \\\n",
       "0    kraftselskapet hafslund blir solgt hvis prisen er god nok sier oslo kommune som ikke vil la seg presse av regjeringen                 \n",
       "1    kvinnen som var savnet i et sjøfly i telemark er funnet omkommet og folk sender gaver til mod angelika men                            \n",
       "2    hva hun får er hemmelig her er n r k dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo              \n",
       "3    kommune vil ikke la seg presse til å beholde aksjene i selskapet olje og energiministeren vil stanse salget for å                     \n",
       "4    sikre norske eiere i hafslund men finansbyråd i oslo andre støylen lar seg ikke presse og sier at selskapet blir                      \n",
       "..                                                                                                                ...                      \n",
       "824  økonomiske utviklingen til kommunen ikke var bærekraftig og det er sterke ord i en perspektivmelding resultatet av det var at høyre   \n",
       "825  frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de til det det    \n",
       "826  frp sin finansbyråd og frp sin egen kollega her på stortinget silje hjemdal som i dag er representant her på stortinget               \n",
       "827  satt i dette byrådet så det er bare til å forvente at kollegaene hennes går tilbake fra denne saken unnskylder                        \n",
       "828  språkbruken fordi at hun var en grisk kommunepolitiker i tjue femten da frp innførte eiendomsskatt i bergen takk                      \n",
       "\n",
       "     wer_origloss  asd_origloss  \\\n",
       "0    0.000000      0.000000       \n",
       "1    0.100000      0.123614       \n",
       "2    0.150000      0.136711       \n",
       "3    0.000000      0.000000       \n",
       "4    0.050000      0.070778       \n",
       "..        ...           ...       \n",
       "824  0.050000      0.061569       \n",
       "825  0.250000      0.229273       \n",
       "826  0.350000      0.278696       \n",
       "827  0.100000      0.098308       \n",
       "828  0.117647      0.098546       \n",
       "\n",
       "                                                                                                                          asr_customloss  \\\n",
       "0    kraftselskapet hafslund blir solgt hvis prisen er god nok sier oslo kommune som ikke vil la seg presse av regjeringen                 \n",
       "1    kvinnen som var savnet i et sjøfly i telemark er funnet omkommet og folk sender gaver til mod angelika men                            \n",
       "2    hva hun får er hemmelig her er n r k dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo              \n",
       "3    kommune vil ikke la seg presse til å beholde aksjene i selskapet olje og energiministeren vil stanse salget for                       \n",
       "4    sikre norske eiere i hafslund men finansbyråd i oslo andre støylen lar seg ikke presse og sier at selskapet blir                      \n",
       "..                                                                                                                ...                      \n",
       "824  økonomiske utviklingen til kommunen ikke var bærekraftig og det er sterke ord i en perspektivmelding resultatet av det var at høyre   \n",
       "825  frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de tider          \n",
       "826  frp sin finansbyråd og frp sin egen kollega her på stortinget silje hjemdal som i dag er representant her på stortinget               \n",
       "827  satt i dette byrådet så det er jo bare til å forvente at kollegaene hennes går tilbake fra denne saken unnskylder                     \n",
       "828  språkbruken fordi at hun var en grisk kommunepolitiker i tjuefemten da frp innførte eiendomsskatt i bergen takk                       \n",
       "\n",
       "     wer_customloss  asd_customloss  \n",
       "0    0.000000        0.000000        \n",
       "1    0.100000        0.123614        \n",
       "2    0.150000        0.136711        \n",
       "3    0.050000        0.052995        \n",
       "4    0.050000        0.070778        \n",
       "..        ...             ...        \n",
       "824  0.050000        0.061569        \n",
       "825  0.200000        0.234647        \n",
       "826  0.350000        0.278696        \n",
       "827  0.150000        0.137447        \n",
       "828  0.235294        0.175020        \n",
       "\n",
       "[829 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results = pd.merge(origloss, customloss, on=\"ref_str\")\n",
    "merged_results = merged_results.reindex(columns=[\"segment_id\", \"ref_str\", \"asr_origloss\", \"wer_origloss\", \"asd_origloss\", \"asr_customloss\", \"wer_customloss\", \"asd_customloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer = merged_results[merged_results[\"wer_origloss\"] > merged_results[\"wer_customloss\"]]\n",
    "worse_wer = merged_results[merged_results[\"wer_origloss\"] < merged_results[\"wer_customloss\"]]\n",
    "better_asd = merged_results[merged_results[\"asd_origloss\"] > merged_results[\"asd_customloss\"]]\n",
    "worse_asd = merged_results[merged_results[\"asd_origloss\"] < merged_results[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better WER but *worse* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>ref_str</th>\n",
       "      <th>asr_origloss</th>\n",
       "      <th>wer_origloss</th>\n",
       "      <th>asd_origloss</th>\n",
       "      <th>asr_customloss</th>\n",
       "      <th>wer_customloss</th>\n",
       "      <th>asd_customloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_11</td>\n",
       "      <td>kan gjøre noe her inne og kor dykk eventuelt vil arrangere meg finansbyråden i oslo måtte svare på mange henvendelser</td>\n",
       "      <td>marineonline finansbyråden i oslo måtte svare på mange henvendelser</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.492595</td>\n",
       "      <td>kan nå hariris finansbyråden i oslo måtte svare på mange henvendelser</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.521667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>N2_050211_NRK_D17_NO_OOLremoved_cut_156</td>\n",
       "      <td>han held fast på at han er  uskuldig han er naturligvis fortvilet og skuffet over resultatet straffeutmålingen noterer en at</td>\n",
       "      <td>han held fast på at han er uskuldeghan er naturligvis fortvilet og skuffet over resultatet straffutmålingen noterer en at</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.098264</td>\n",
       "      <td>han held fast på at han er uskyldig han er naturligvis fortvilet og skuffet over resultatet straffutmålingen noterer en at</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.123088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>N2_051206_NRK_D12_NO_OOLremoved_cut_45</td>\n",
       "      <td>i og med at dei har mistillit frå ni av tretten medlemmar i landsrådet så det betyr at spelet er</td>\n",
       "      <td>i og med at de har mistillit fra ni av tretten medlemmer i landsrådet så det betyr at spillet er</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.109594</td>\n",
       "      <td>i og med at dei har mistillit fra ni av tretten medlemmer i landsrådet så det betyr at spillet er</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.110652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_13</td>\n",
       "      <td>jo sånn rett ned fem hundre meter utfor autovernet de andre kom  rett etter meg og klart så hva altså</td>\n",
       "      <td>sånn rett ned fem hundre meter utføre ut vernet de andre komme rett etter meg og klarte å så kvasse</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.490793</td>\n",
       "      <td>sånn rett ned fem hundre meter utføre utover de andre kom rett etter meg og klarte å så kasomskjedd</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.552101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_13</td>\n",
       "      <td>eigedomsskatt innan tjue tjuefire  eigedomsskatten er ein kommunal skatt og høyrer inn under det kommunale sjølvstyret det er opp til</td>\n",
       "      <td>eigendomsskatt innen tjue tjuefire eiendomsskatten er en kommunal skatt og hører inn under det kommunale sjølstyret det er opp til</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.172710</td>\n",
       "      <td>eigendomskatt innen tjue tjuefire eiendomsskatten er en kommunal skatt og høyrer inn under det kommunale sjølstyret det er opp til</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.176661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_191</td>\n",
       "      <td>renovasjonsavgiften har også hatt et betydelig løft for å få dette til man har hatt gateopprustninger der man   har fått</td>\n",
       "      <td>innovasjonssystem renovasjonsavgiften har også hatt et betydelig løft for å få dette til og man har hatt gateopprustninger la andre hvordan også har fått</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.301255</td>\n",
       "      <td>innovasjonssystem renovasjonsavgiften har også hatt et betydelig løft for å få dette til og man har hatt gateopprustninger klandre hordeogså har fått</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.383861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_289</td>\n",
       "      <td>gjør dette som et heltidsverv også  helgheim og johnsen og johansen og flere fra frp bør unnskylde til egne kollegaer</td>\n",
       "      <td>gjør dette som et heltidsverv og så bør jo helgheim johnsen johansen og flere fra frp unnskylder til egne kolleger</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.272333</td>\n",
       "      <td>gjør dette som et heltidsverv og så blir jo helge johnsen og johansen og flere fra frp unnskylder til egne kolleger</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.277996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_293</td>\n",
       "      <td>frp   med frp finansbyråd innførte eiendomsskatt det var ikke noe venstre eller krf i det byrådet som tvang dem til</td>\n",
       "      <td>frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de til det det</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.229273</td>\n",
       "      <td>frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de tider</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.234647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           segment_id  \\\n",
       "11   N2_030505_NRK_D12_NO_cut_11                                        \n",
       "272  N2_050211_NRK_D17_NO_OOLremoved_cut_156                            \n",
       "397  N2_051206_NRK_D12_NO_OOLremoved_cut_45                             \n",
       "478  p1_g02_m2_5_x-free_cut_13                                          \n",
       "545  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_13    \n",
       "723  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_191   \n",
       "821  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_289   \n",
       "825  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_293   \n",
       "\n",
       "                                                                                                                                    ref_str  \\\n",
       "11   kan gjøre noe her inne og kor dykk eventuelt vil arrangere meg finansbyråden i oslo måtte svare på mange henvendelser                    \n",
       "272  han held fast på at han er  uskuldig han er naturligvis fortvilet og skuffet over resultatet straffeutmålingen noterer en at             \n",
       "397  i og med at dei har mistillit frå ni av tretten medlemmar i landsrådet så det betyr at spelet er                                         \n",
       "478  jo sånn rett ned fem hundre meter utfor autovernet de andre kom  rett etter meg og klart så hva altså                                    \n",
       "545  eigedomsskatt innan tjue tjuefire  eigedomsskatten er ein kommunal skatt og høyrer inn under det kommunale sjølvstyret det er opp til    \n",
       "723  renovasjonsavgiften har også hatt et betydelig løft for å få dette til man har hatt gateopprustninger der man   har fått                 \n",
       "821  gjør dette som et heltidsverv også  helgheim og johnsen og johansen og flere fra frp bør unnskylde til egne kollegaer                    \n",
       "825  frp   med frp finansbyråd innførte eiendomsskatt det var ikke noe venstre eller krf i det byrådet som tvang dem til                      \n",
       "\n",
       "                                                                                                                                                  asr_origloss  \\\n",
       "11   marineonline finansbyråden i oslo måtte svare på mange henvendelser                                                                                         \n",
       "272  han held fast på at han er uskuldeghan er naturligvis fortvilet og skuffet over resultatet straffutmålingen noterer en at                                   \n",
       "397  i og med at de har mistillit fra ni av tretten medlemmer i landsrådet så det betyr at spillet er                                                            \n",
       "478  sånn rett ned fem hundre meter utføre ut vernet de andre komme rett etter meg og klarte å så kvasse                                                         \n",
       "545  eigendomsskatt innen tjue tjuefire eiendomsskatten er en kommunal skatt og hører inn under det kommunale sjølstyret det er opp til                          \n",
       "723  innovasjonssystem renovasjonsavgiften har også hatt et betydelig løft for å få dette til og man har hatt gateopprustninger la andre hvordan også har fått   \n",
       "821  gjør dette som et heltidsverv og så bør jo helgheim johnsen johansen og flere fra frp unnskylder til egne kolleger                                          \n",
       "825  frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de til det det                          \n",
       "\n",
       "     wer_origloss  asd_origloss  \\\n",
       "11   0.60          0.492595       \n",
       "272  0.15          0.098264       \n",
       "397  0.20          0.109594       \n",
       "478  0.45          0.490793       \n",
       "545  0.30          0.172710       \n",
       "723  0.30          0.301255       \n",
       "821  0.45          0.272333       \n",
       "825  0.25          0.229273       \n",
       "\n",
       "                                                                                                                                            asr_customloss  \\\n",
       "11   kan nå hariris finansbyråden i oslo måtte svare på mange henvendelser                                                                                   \n",
       "272  han held fast på at han er uskyldig han er naturligvis fortvilet og skuffet over resultatet straffutmålingen noterer en at                              \n",
       "397  i og med at dei har mistillit fra ni av tretten medlemmer i landsrådet så det betyr at spillet er                                                       \n",
       "478  sånn rett ned fem hundre meter utføre utover de andre kom rett etter meg og klarte å så kasomskjedd                                                     \n",
       "545  eigendomskatt innen tjue tjuefire eiendomsskatten er en kommunal skatt og høyrer inn under det kommunale sjølstyret det er opp til                      \n",
       "723  innovasjonssystem renovasjonsavgiften har også hatt et betydelig løft for å få dette til og man har hatt gateopprustninger klandre hordeogså har fått   \n",
       "821  gjør dette som et heltidsverv og så blir jo helge johnsen og johansen og flere fra frp unnskylder til egne kolleger                                     \n",
       "825  frpbyrådet med frp finansbyråd innførte eiendomsskatt og det var ikke noe venstre eller krf i det byrådet som tvang de tider                            \n",
       "\n",
       "     wer_customloss  asd_customloss  \n",
       "11   0.55            0.521667        \n",
       "272  0.10            0.123088        \n",
       "397  0.15            0.110652        \n",
       "478  0.35            0.552101        \n",
       "545  0.25            0.176661        \n",
       "723  0.20            0.383861        \n",
       "821  0.40            0.277996        \n",
       "825  0.20            0.234647        "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "better_wer[better_wer[\"asd_origloss\"] < better_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worse WER but *better* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>ref_str</th>\n",
       "      <th>asr_origloss</th>\n",
       "      <th>wer_origloss</th>\n",
       "      <th>asd_origloss</th>\n",
       "      <th>asr_customloss</th>\n",
       "      <th>wer_customloss</th>\n",
       "      <th>asd_customloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_9</td>\n",
       "      <td>å sikre et nasjonalt eigarskap det er en debatt som dei nasjonale politikarane får ta eg får  forholde meg til</td>\n",
       "      <td>sikre et nasjonalt eierskap det er en debatt som dei nasjonale politikerne får ta for forholde meg til</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.284005</td>\n",
       "      <td>sikre et nasjonalt eierskap det er en debatt som de nasjonale politikerne får ta jeg for forholde meg til</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.245522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_97</td>\n",
       "      <td>vinglet frem og tilbake i dette spørsmålet og fikk til slutt beskjed av statsminister guy verhofstadt om å godta  nattflyvninger</td>\n",
       "      <td>vinglet frem og tilbake i dette spørsmålet og fikk til slutt beskjed av statsminister gifer hofstad om å godta nattflygningr</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.282468</td>\n",
       "      <td>vinglet frem og tilbake i dette spørsmålet og fikk til slutt beskjed av statsminister gifer hovstad om å godta natt flyvninger</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.197169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>N2_050211_NRK_D17_NO_OOLremoved_cut_5</td>\n",
       "      <td>liksom sånn masse forskjellige  oster og masse forskjellig melkesort  altså ost rømme hva det nå helst måtte være så vil</td>\n",
       "      <td>liksom sånn masse forskjellige oster og masse forskjellig melketår altså ut være</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.488312</td>\n",
       "      <td>liksom sånn masse forskjellige oster og masse forskjellig melke stort altså ottere</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.478594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>N2_050211_NRK_D17_NO_OOLremoved_cut_149</td>\n",
       "      <td>ut i frå det tapet dei føle då tingrettsdommar   terje mowatt i dag leste opp drapsdommen på tretten år  satt</td>\n",
       "      <td>få det tapet deg føle då tingrett dommer terje mowat i dag leste opp drapsdommen på tretten år satt</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.314504</td>\n",
       "      <td>ut ifrå det tapet deg føle da tingrett dommer terje mowat i dag leste opp drapsdommen på tretten år sa</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.239040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>N2_050211_NRK_D17_NO_OOLremoved_cut_186</td>\n",
       "      <td>kvart å mista lyttarar  og dermed inntekter til kanal tjuefire dette er bakgrunnen for at radioselskapa kan bli slått saman</td>\n",
       "      <td>kvart og mista lytter og dermed inntekter til kanal tjuefire dette er bakgrunnen for at radio selskapet kan bli slått sammen</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.233716</td>\n",
       "      <td>kvart og mistet lyttere og dermed inntekter til kanal tjuefire dette er bakgrunnen for at radio selskapet kan bli slått sammen</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.220201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>N2_051206_NRK_D12_NO_OOLremoved_cut_50</td>\n",
       "      <td>med ein viss verdigheit men det er framleis usikkert om ho blir sittande eller ikkje og det får vi ikkje</td>\n",
       "      <td>med en viss verdighet men det er framleis usikkert om hun blir sittende eller ikkje og det får vi ikke</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.223463</td>\n",
       "      <td>med en viss verdighet men det er framleis usikkert om hun blir sittende eller ikke og det får vi ikke</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.207827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>p1_g04_m2_5_x-free_cut_8</td>\n",
       "      <td>til spania så dagen etter  altså søndag da  satt vi allerede på flyet og landa i madrid sein seint på</td>\n",
       "      <td>til spania så dagen etter altså søndag da satt vi allerede på flyet og landet i madrid sin seint på</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.103505</td>\n",
       "      <td>til spania så dagen etter altså søndag da satt vi allerede på flyet og landet i madrid sent sent på</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.079296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_4</td>\n",
       "      <td>av det mest spennende jeg har gjort er å være tilknyttet senter for borgerkrigsforskning som var ett av de såkalte</td>\n",
       "      <td>det mest spennende jeg har gjort er å være tilknyttet senter for borgerkrigsfrskning som var et av de såkalte</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.188180</td>\n",
       "      <td>det mest spennende jeg har gjort det å være tilknyttet til senter for borgerkrigsforskning som var et av de såkalte</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.151398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_29</td>\n",
       "      <td>på om kommunane kan   løysa   velferdsoppgåvene sine på ein tilfredsstillande måte eller ikkje           med denne regjeringen har vi kuttet skatter</td>\n",
       "      <td>på om kommunene kan løse velferdsoppgavene sine på en tilfredsstillende måte eller ikke ja takk neste taler er representanten aleksander stokkebø høyre deretter ørsal johansen stokkebø ver så god president denne regjeringen har me kuttet skatter</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.092610</td>\n",
       "      <td>på om kommunene kan løyse velferdsoppgavene sine på en tilfredsstillende måte eller ikke tak takk neste taler er representanten aleksander stoke høyre deretter ørsal johansen stokkebø ver så god president med denne regjeringen har me kutte skatter</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.068461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_134</td>\n",
       "      <td>over  etter forrige regjering vi fjernet nesten femti bomselskap vi innførte  tjue prosent rabatt   på bombrikker og vi fikk i</td>\n",
       "      <td>etter etter forrige regjering vi fjerna nesten femti bomselskapeinnførte tjue prosent rabatt for bombrikke og vi fikk inn</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.352134</td>\n",
       "      <td>over etter etter forrige regjering vi fjerna nesten femti bomselskapet innførte tjue prosent rabatt for å ha bombrikke og vi fikk inn</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.302636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           segment_id  \\\n",
       "9    N2_030505_NRK_D12_NO_cut_9                                         \n",
       "97   N2_030505_NRK_D12_NO_cut_97                                        \n",
       "121  N2_050211_NRK_D17_NO_OOLremoved_cut_5                              \n",
       "265  N2_050211_NRK_D17_NO_OOLremoved_cut_149                            \n",
       "302  N2_050211_NRK_D17_NO_OOLremoved_cut_186                            \n",
       "402  N2_051206_NRK_D12_NO_OOLremoved_cut_50                             \n",
       "500  p1_g04_m2_5_x-free_cut_8                                           \n",
       "524  p1_g08_m2_5_x-free_cut_4                                           \n",
       "561  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_29    \n",
       "666  2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_134   \n",
       "\n",
       "                                                                                                                                                   ref_str  \\\n",
       "9    å sikre et nasjonalt eigarskap det er en debatt som dei nasjonale politikarane får ta eg får  forholde meg til                                          \n",
       "97   vinglet frem og tilbake i dette spørsmålet og fikk til slutt beskjed av statsminister guy verhofstadt om å godta  nattflyvninger                        \n",
       "121  liksom sånn masse forskjellige  oster og masse forskjellig melkesort  altså ost rømme hva det nå helst måtte være så vil                                \n",
       "265  ut i frå det tapet dei føle då tingrettsdommar   terje mowatt i dag leste opp drapsdommen på tretten år  satt                                           \n",
       "302  kvart å mista lyttarar  og dermed inntekter til kanal tjuefire dette er bakgrunnen for at radioselskapa kan bli slått saman                             \n",
       "402  med ein viss verdigheit men det er framleis usikkert om ho blir sittande eller ikkje og det får vi ikkje                                                \n",
       "500  til spania så dagen etter  altså søndag da  satt vi allerede på flyet og landa i madrid sein seint på                                                   \n",
       "524  av det mest spennende jeg har gjort er å være tilknyttet senter for borgerkrigsforskning som var ett av de såkalte                                      \n",
       "561  på om kommunane kan   løysa   velferdsoppgåvene sine på ein tilfredsstillande måte eller ikkje           med denne regjeringen har vi kuttet skatter    \n",
       "666  over  etter forrige regjering vi fjernet nesten femti bomselskap vi innførte  tjue prosent rabatt   på bombrikker og vi fikk i                          \n",
       "\n",
       "                                                                                                                                                                                                                                              asr_origloss  \\\n",
       "9    sikre et nasjonalt eierskap det er en debatt som dei nasjonale politikerne får ta for forholde meg til                                                                                                                                                  \n",
       "97   vinglet frem og tilbake i dette spørsmålet og fikk til slutt beskjed av statsminister gifer hofstad om å godta nattflygningr                                                                                                                            \n",
       "121  liksom sånn masse forskjellige oster og masse forskjellig melketår altså ut være                                                                                                                                                                        \n",
       "265  få det tapet deg føle då tingrett dommer terje mowat i dag leste opp drapsdommen på tretten år satt                                                                                                                                                     \n",
       "302  kvart og mista lytter og dermed inntekter til kanal tjuefire dette er bakgrunnen for at radio selskapet kan bli slått sammen                                                                                                                            \n",
       "402  med en viss verdighet men det er framleis usikkert om hun blir sittende eller ikkje og det får vi ikke                                                                                                                                                  \n",
       "500  til spania så dagen etter altså søndag da satt vi allerede på flyet og landet i madrid sin seint på                                                                                                                                                     \n",
       "524  det mest spennende jeg har gjort er å være tilknyttet senter for borgerkrigsfrskning som var et av de såkalte                                                                                                                                           \n",
       "561  på om kommunene kan løse velferdsoppgavene sine på en tilfredsstillende måte eller ikke ja takk neste taler er representanten aleksander stokkebø høyre deretter ørsal johansen stokkebø ver så god president denne regjeringen har me kuttet skatter   \n",
       "666  etter etter forrige regjering vi fjerna nesten femti bomselskapeinnførte tjue prosent rabatt for bombrikke og vi fikk inn                                                                                                                               \n",
       "\n",
       "     wer_origloss  asd_origloss  \\\n",
       "9    0.25          0.284005       \n",
       "97   0.15          0.282468       \n",
       "121  0.50          0.488312       \n",
       "265  0.35          0.314504       \n",
       "302  0.25          0.233716       \n",
       "402  0.25          0.223463       \n",
       "500  0.10          0.103505       \n",
       "524  0.15          0.188180       \n",
       "561  1.20          1.092610       \n",
       "666  0.40          0.352134       \n",
       "\n",
       "                                                                                                                                                                                                                                              asr_customloss  \\\n",
       "9    sikre et nasjonalt eierskap det er en debatt som de nasjonale politikerne får ta jeg for forholde meg til                                                                                                                                                 \n",
       "97   vinglet frem og tilbake i dette spørsmålet og fikk til slutt beskjed av statsminister gifer hovstad om å godta natt flyvninger                                                                                                                            \n",
       "121  liksom sånn masse forskjellige oster og masse forskjellig melke stort altså ottere                                                                                                                                                                        \n",
       "265  ut ifrå det tapet deg føle da tingrett dommer terje mowat i dag leste opp drapsdommen på tretten år sa                                                                                                                                                    \n",
       "302  kvart og mistet lyttere og dermed inntekter til kanal tjuefire dette er bakgrunnen for at radio selskapet kan bli slått sammen                                                                                                                            \n",
       "402  med en viss verdighet men det er framleis usikkert om hun blir sittende eller ikke og det får vi ikke                                                                                                                                                     \n",
       "500  til spania så dagen etter altså søndag da satt vi allerede på flyet og landet i madrid sent sent på                                                                                                                                                       \n",
       "524  det mest spennende jeg har gjort det å være tilknyttet til senter for borgerkrigsforskning som var et av de såkalte                                                                                                                                       \n",
       "561  på om kommunene kan løyse velferdsoppgavene sine på en tilfredsstillende måte eller ikke tak takk neste taler er representanten aleksander stoke høyre deretter ørsal johansen stokkebø ver så god president med denne regjeringen har me kutte skatter   \n",
       "666  over etter etter forrige regjering vi fjerna nesten femti bomselskapet innførte tjue prosent rabatt for å ha bombrikke og vi fikk inn                                                                                                                     \n",
       "\n",
       "     wer_customloss  asd_customloss  \n",
       "9    0.30            0.245522        \n",
       "97   0.20            0.197169        \n",
       "121  0.60            0.478594        \n",
       "265  0.40            0.239040        \n",
       "302  0.30            0.220201        \n",
       "402  0.30            0.207827        \n",
       "500  0.15            0.079296        \n",
       "524  0.20            0.151398        \n",
       "561  1.25            1.068461        \n",
       "666  0.45            0.302636        "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worse_wer[worse_wer[\"asd_origloss\"] > worse_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_improved_percent = (len(better_wer)/len(merged_results)) * 100\n",
    "asd_improved_percent = (len(better_asd)/len(merged_results)) * 100\n",
    "\n",
    "wer_worsened_percent = (len(worse_wer)/len(merged_results)) * 100\n",
    "asd_worsened_percent = (len(worse_asd)/len(merged_results)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER improved: 19.30%\n",
      "ASD improved: 25.33%\n",
      "WER worsened: 17.13%\n",
      "ASD worsened: 24.85%\n"
     ]
    }
   ],
   "source": [
    "print(f\"WER improved: {wer_improved_percent:.2f}%\")\n",
    "print(f\"ASD improved: {asd_improved_percent:.2f}%\")\n",
    "\n",
    "print(f\"WER worsened: {wer_worsened_percent:.2f}%\")\n",
    "print(f\"ASD worsened: {asd_worsened_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC LOSS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/stipendiater/janinelr/miniconda3/envs/wav2vec/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "import types\n",
    "from tabulate import tabulate\n",
    "from dtw import *\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/stipendiater/janinelr/miniconda3/envs/wav2vec/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 15.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/5468 [00:00<?, ?ex/s]/localhome/stipendiater/janinelr/miniconda3/envs/wav2vec/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/localhome/stipendiater/janinelr/miniconda3/envs/wav2vec/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/localhome/stipendiater/janinelr/miniconda3/envs/wav2vec/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/localhome/stipendiater/janinelr/miniconda3/envs/wav2vec/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   1%|▏         | 82/5468 [00:00<00:06, 819.78ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   3%|▎         | 171/5468 [00:00<00:06, 856.16ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   5%|▍         | 263/5468 [00:00<00:05, 882.75ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   6%|▋         | 352/5468 [00:00<00:05, 882.65ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   8%|▊         | 443/5468 [00:00<00:05, 889.67ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  10%|▉         | 535/5468 [00:00<00:05, 899.28ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  12%|█▏        | 629/5468 [00:00<00:05, 910.42ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  13%|█▎        | 721/5468 [00:00<00:05, 896.41ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  15%|█▍        | 811/5468 [00:00<00:05, 790.05ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  16%|█▋        | 893/5468 [00:01<00:06, 724.82ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  18%|█▊        | 968/5468 [00:01<00:06, 651.73ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  19%|█▉        | 1036/5468 [00:01<00:11, 395.13ex/s]\n",
      "\n",
      "#0:  21%|██        | 1126/5468 [00:01<00:08, 484.32ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  22%|██▏       | 1220/5468 [00:01<00:07, 576.23ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  24%|██▍       | 1318/5468 [00:01<00:06, 666.25ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  26%|██▌       | 1403/5468 [00:02<00:05, 710.41ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  27%|██▋       | 1491/5468 [00:02<00:05, 753.87ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  29%|██▉       | 1583/5468 [00:02<00:04, 797.92ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  31%|███       | 1674/5468 [00:02<00:04, 828.85ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  32%|███▏      | 1765/5468 [00:02<00:04, 849.91ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  34%|███▍      | 1860/5468 [00:02<00:04, 875.83ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  36%|███▌      | 1965/5468 [00:02<00:03, 925.85ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "#0:  39%|███▉      | 2157/5468 [00:03<00:05, 635.29ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  41%|████▏     | 2257/5468 [00:03<00:04, 714.63ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  43%|████▎     | 2355/5468 [00:03<00:04, 776.85ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  45%|████▍     | 2449/5468 [00:03<00:03, 817.39ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  47%|████▋     | 2548/5468 [00:03<00:03, 860.54ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  48%|████▊     | 2641/5468 [00:03<00:03, 876.32ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  50%|█████     | 2734/5468 [00:03<00:03, 875.85ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  52%|█████▏    | 2826/5468 [00:03<00:02, 884.37ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  53%|█████▎    | 2921/5468 [00:03<00:02, 903.07ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  59%|█████▉    | 3239/5468 [00:04<00:03, 614.52ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  61%|██████    | 3327/5468 [00:04<00:03, 677.65ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  63%|██████▎   | 3420/5468 [00:04<00:02, 741.81ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  64%|██████▍   | 3513/5468 [00:04<00:02, 791.32ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  66%|██████▌   | 3608/5468 [00:04<00:02, 832.79ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  68%|██████▊   | 3698/5468 [00:05<00:02, 850.35ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  69%|██████▉   | 3793/5468 [00:05<00:01, 876.59ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  71%|███████   | 3884/5468 [00:05<00:01, 869.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  73%|███████▎  | 3978/5468 [00:05<00:01, 889.35ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  74%|███████▍  | 4069/5468 [00:05<00:02, 593.75ex/s]\n",
      "\n",
      "#0:  78%|███████▊  | 4260/5468 [00:05<00:01, 740.29ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|███████▉  | 4351/5468 [00:05<00:01, 782.76ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  81%|████████  | 4438/5468 [00:06<00:01, 696.24ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  83%|████████▎ | 4515/5468 [00:06<00:01, 696.94ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  84%|████████▍ | 4609/5468 [00:06<00:01, 758.55ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  86%|████████▌ | 4705/5468 [00:06<00:00, 810.54ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  88%|████████▊ | 4801/5468 [00:06<00:00, 851.05ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  89%|████████▉ | 4893/5468 [00:06<00:00, 870.03ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  91%|█████████ | 4985/5468 [00:06<00:00, 883.89ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  93%|█████████▎| 5076/5468 [00:06<00:00, 599.42ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  96%|█████████▌| 5262/5468 [00:07<00:00, 731.12ex/s]\n",
      "#0:  98%|█████████▊| 5365/5468 [00:07<00:00, 805.30ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0: 100%|██████████| 5468/5468 [00:07<00:00, 738.79ex/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#1: 100%|██████████| 5468/5468 [00:07<00:00, 722.73ex/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#2: 100%|██████████| 5467/5467 [00:07<00:00, 712.16ex/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 5467/5467 [00:07<00:00, 704.17ex/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../model_ckpts/fine-tuning_wav2vec2_customLossV11/runs/train_set.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.cast_column(\"path\", Audio())\n",
    "train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = [{\"input_values\": train_dataset[2][\"input_values\"]}]\n",
    "label_feature = [{\"input_ids\": train_dataset[2][\"labels\"]}]\n",
    "\n",
    "batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "batch[\"labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-0.4508, -0.3754, -0.2800,  ...,  0.2719,  0.2864,  0.3113]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32), 'labels': tensor([[23, 15, 26, 14,  9,  1,  3, 11,  9, 19,  0,  4, 18, 29, 13,  0, 15, 13,\n",
       "          0, 19,  5,  9,  5, 18,  0, 11, 14, 21, 19, 20,  0,  1, 22,  0, 18, 21,\n",
       "         19, 19,  9, 19, 11,  5,  0, 10,  5, 11,  1, 20,  5, 18,  9, 14,  1,  0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    numphones = params.shape[0]  # number of labels, not even used in the code\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    # print(seqLen, numphones, L, T)\n",
    "\n",
    "    alphas = np.zeros((L,T))\n",
    "    betas = np.zeros((L,T))\n",
    "\n",
    "    # convert logits to log softmax\n",
    "    params = params - np.max(params,axis=0)\n",
    "    params = np.exp(params)\n",
    "    params = params / np.sum(params,axis=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = np.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = np.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = np.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward += np.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = np.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = np.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = np.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward += np.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = np.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = np.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = np.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or np.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(np.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_tensor(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = torch.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428.9810518359343\n"
     ]
    }
   ],
   "source": [
    "logits = output[\"logits\"][0].detach().cpu().numpy().transpose()\n",
    "seq = batch[\"labels\"][0].detach().cpu().numpy()\n",
    "\n",
    "loss, grad = ctc_loss(logits, seq.squeeze(), blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(428.9811, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "targets = batch[\"labels\"][0]\n",
    "input_lengths = torch.tensor(output[\"logits\"][0].shape[0])\n",
    "target_lengths = torch.tensor(batch[\"labels\"][0].shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=targets, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff in forward/backward LL : 0.000031\n",
      "Zeros found : (0/203)\n",
      "tensor(428.9811, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "logits = output[\"logits\"][0].transpose(1,0)\n",
    "seq = batch[\"labels\"][0]\n",
    "\n",
    "loss, grad = ctc_loss_tensor(logits, seq, blank=0)\n",
    "print(loss)\n",
    "# loss.backward()\n",
    "\n",
    "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.4962,   1.0071,   1.6117,  ...,   2.2399,   6.2250,   9.0228],\n",
       "        [ -2.6267,  -2.2761,  -0.2974,  ...,   0.2028,   0.0742,  -1.0642],\n",
       "        [ -4.1935,  -3.8417,  -3.9895,  ...,  -3.0990,  -2.9767,  -2.8504],\n",
       "        ...,\n",
       "        [  5.8301,   4.6000,   3.2805,  ...,   6.2615,   3.8786,   3.0810],\n",
       "        [-15.2999, -15.3380, -14.9798,  ..., -15.0261, -14.9139, -14.6649],\n",
       "        [-15.4351, -15.3723, -15.0139,  ..., -15.0612, -15.0430, -14.8176]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss & ASD alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', 'n', 'n', 'e', '', '', 'v', 'a', 'a', '[PAD]', '[PAD]', 's', 's', '[PAD]', 'n', '[PAD]', '[PAD]', 'j', 'a', 'a', 'a', 'k', 'k', '[PAD]', '[PAD]', 'k', '[PAD]', '[PAD]', 'e', '[PAD]', '[PAD]', '[PAD]', 's', '[PAD]', '[PAD]', 'e', '[PAD]', '[PAD]', '[PAD]', '', '', 'd', 'd', 'r', 'r', 'r', 'ø', 'ø', 'm', 'm', '[PAD]', '[PAD]', '[PAD]', '', '', '', 'o', 'm', 'm', '[PAD]', '[PAD]', '[PAD]', '', '', '', '[PAD]', '[PAD]', 's', '[PAD]', '[PAD]', '[PAD]', 'e', 'e', 'i', 'i', 'i', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'e', 'e', 'r', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '', '', '', '[PAD]', 'k', 'k', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'n', 'n', 'u', 'u', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 's', '[PAD]', '[PAD]', '[PAD]', '', '', 'a', 'a', 'a', '[PAD]', 'v', 'v', '[PAD]', '', 'r', 'r', 'u', 'u', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 't', 't', '[PAD]', 'i', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 's', 's', 's', '[PAD]', '[PAD]', 'k', '[PAD]', 'e', 'e', 'e', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '', '', '', 'g', 'j', 'e', 'e', '[PAD]', '[PAD]', '[PAD]', 'k', 'k', 'a', 'a', 'a', '[PAD]', '[PAD]', '[PAD]', 'a', '[PAD]', '[PAD]', 'r', 'r', '[PAD]', 'i', 'i', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'n', 'n', '[PAD]', 'e', 'e', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '', '']\n",
      "['w', 'o', 'z', 'n', 'i', 'a', 'c', 'k', 'i', 's', '', 'd', 'r', 'ø', 'm', '', 'o', 'm', '', 's', 'e', 'i', 'e', 'r', '', 'k', 'n', 'u', 's', 't', '', 'a', 'v', '', 'r', 'u', 's', 's', 'i', 's', 'k', 'e', '', 'j', 'e', 'k', 'a', 't', 'e', 'r', 'i', 'n', 'a', '']\n"
     ]
    }
   ],
   "source": [
    "logits = output[\"logits\"][0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "\n",
    "pred_ids = np.argmax(logits.detach().numpy(), axis=-1)\n",
    "logit_frames_decoded = processor.tokenizer.batch_decode(pred_ids, group_tokens=False)\n",
    "print(logit_frames_decoded)\n",
    "\n",
    "label_ids = batch[\"labels\"][0]\n",
    "label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "print(processor.tokenizer.batch_decode(label_ids, group_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(logit_frames_decoded, open(\"logit_frames.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: wozniackis drøm om seier knust av russiske jekaterina\n",
      "HYP: asnake drøm om sier knut av russiske karine\n"
     ]
    }
   ],
   "source": [
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "output_logits = output[\"logits\"].detach().numpy()\n",
    "hyp = processor.batch_decode(output_logits).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(logits, open(\"logits_8Nov.pkl\", \"wb\"))\n",
    "# pickle.dump(logit_frames_decoded, open(\"logit_frames_decoded_8Nov.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: start running here! (when loading from pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits decoded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>[PAD]</td><td>n</td><td>n</td><td>e</td><td></td><td></td><td>v</td><td>a</td><td>a</td><td>[PAD]</td><td>[PAD]</td><td>s</td><td>s</td><td>[PAD]</td><td>n</td><td>[PAD]</td><td>[PAD]</td><td>j</td><td>a</td><td>a</td><td>a</td><td>k</td><td>k</td><td>[PAD]</td><td>[PAD]</td><td>k</td><td>[PAD]</td><td>[PAD]</td><td>e</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>s</td><td>[PAD]</td><td>[PAD]</td><td>e</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td><td>d</td><td>d</td><td>r</td><td>r</td><td>r</td><td>ø</td><td>ø</td><td>m</td><td>m</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td><td></td><td>o</td><td>m</td><td>m</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td><td></td><td>[PAD]</td><td>[PAD]</td><td>s</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>e</td><td>e</td><td>i</td><td>i</td><td>i</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>e</td><td>e</td><td>r</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td><td></td><td>[PAD]</td><td>k</td><td>k</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>n</td><td>n</td><td>u</td><td>u</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>s</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td><td>a</td><td>a</td><td>a</td><td>[PAD]</td><td>v</td><td>v</td><td>[PAD]</td><td></td><td>r</td><td>r</td><td>u</td><td>u</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>t</td><td>t</td><td>[PAD]</td><td>i</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>s</td><td>s</td><td>s</td><td>[PAD]</td><td>[PAD]</td><td>k</td><td>[PAD]</td><td>e</td><td>e</td><td>e</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td><td></td><td>g</td><td>j</td><td>e</td><td>e</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>k</td><td>k</td><td>a</td><td>a</td><td>a</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>a</td><td>[PAD]</td><td>[PAD]</td><td>r</td><td>r</td><td>[PAD]</td><td>i</td><td>i</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>n</td><td>n</td><td>[PAD]</td><td>e</td><td>e</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td>[PAD]</td><td></td><td></td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOV-8 DATA\n",
    "ref = \"wozniackis drøm om seier knust av russiske jekaterina\"\n",
    "hyp = \"asnake drøm om sier knut av russiske karine\"\n",
    "\n",
    "logit_frames_decoded = pickle.load(open(\"logit_frames_decoded_8Nov.pkl\", \"rb\"))\n",
    "\n",
    "alignment_table = [logit_frames_decoded]\n",
    "table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "print(\"logits decoded\")\n",
    "display(HTML(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "modelname = 'ltg/norbert2'\n",
    "model = BertModel.from_pretrained(modelname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asd_align(ref, hyp, model, tokenizer):\n",
    "    tokenized_ref = tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    cosdist_alignment_tokens = []  # FOR PRINTING\n",
    "    # ref_alignments = []\n",
    "    hyp_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        cosdist_alignment_tokens.append(round(cosdist, 3))  # FOR PRINTING\n",
    "        # ref_token = tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        hyp_token = tokenizer.convert_ids_to_tokens(hyp_input_ids[hyp_alignment_idxs[i]].reshape(1))[0]\n",
    "        # ref_alignments.append((ref_token, cosdist))\n",
    "        hyp_alignments.append((hyp_alignment_idxs[i], hyp_token, cosdist))\n",
    "\n",
    "    # FOR PRINTING\n",
    "    ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(ref_alignment_idxs):\n",
    "        ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    ref_alignment_tokens = tokenizer.convert_ids_to_tokens(torch.from_numpy(ref_alignment_input_ids))\n",
    "\n",
    "    hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(hyp_alignment_idxs):\n",
    "        hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    hyp_alignment_tokens = tokenizer.convert_ids_to_tokens(torch.from_numpy(hyp_alignment_input_ids))\n",
    "\n",
    "    hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    print(\"Token alignment table:\")\n",
    "    display(HTML(table))\n",
    "\n",
    "    return hyp_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>[UNK]</td><td>[UNK]</td><td>[UNK]</td><td>[UNK]</td><td>drøm </td><td>om   </td><td>seier</td><td>knust</td><td>knust</td><td>av   </td><td>russiske</td><td>je      </td><td>##kat</td><td>##eri</td><td>##na </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>as   </td><td>##na </td><td>##ke </td><td>drøm </td><td>om   </td><td>sier </td><td>kn   </td><td>##ut </td><td>av   </td><td>russiske</td><td>russiske</td><td>kar  </td><td>kar  </td><td>##ine</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.19 </td><td>0.611</td><td>0.631</td><td>0.712</td><td>0.568</td><td>0.124</td><td>0.179</td><td>0.413</td><td>0.631</td><td>0.753</td><td>0.169</td><td>0.105   </td><td>0.593   </td><td>0.573</td><td>0.621</td><td>0.48 </td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyp_alignments = get_asd_align(ref, hyp, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('combined', 'asnake', 0.6369385619958242)\n",
      "(4, 'drøm', 0.12406861782073975)\n",
      "(5, 'om', 0.17880988121032715)\n",
      "(6, 'sier', 0.4131525754928589)\n",
      "('combined', 'knut', 0.6922986581921577)\n",
      "(9, 'av', 0.16893887519836426)\n",
      "('collapsed', 'russiske', 0.13685202598571777)\n",
      "('combined', 'karine', 0.531245581805706)\n"
     ]
    }
   ],
   "source": [
    "# collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "clean_hyp_alignment = []\n",
    "collapsed_indices = []\n",
    "for i, item in enumerate(hyp_alignments):\n",
    "    if i < (len(hyp_alignments) - 1):\n",
    "        if item[0] == hyp_alignments[i+1][0]:\n",
    "            averaged_cosdist = sum([item[2], hyp_alignments[i-1][2]]) / 2\n",
    "            clean_hyp_alignment.append((\"collapsed\", item[1], averaged_cosdist))\n",
    "            collapsed_indices.append(item[0])\n",
    "        elif item[0] not in collapsed_indices and item[1] != \"[CLS]\":\n",
    "            clean_hyp_alignment.append(item)\n",
    "\n",
    "\n",
    "# GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "regrouped_tokens = []\n",
    "for i, item in enumerate(clean_hyp_alignment):\n",
    "    if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "        if \"##\" not in item[1] and \"##\" in clean_hyp_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "            wordpiece_group = []\n",
    "            wordpiece_group.append(item)\n",
    "            regrouped_tokens.append(wordpiece_group)\n",
    "        elif \"##\" in item[1]:  # parts of the word\n",
    "            wordpiece_group.append(item)\n",
    "        else:  # not wordpieces\n",
    "            regrouped_tokens.append(item)\n",
    "\n",
    "\n",
    "# for token in regrouped_tokens:\n",
    "#     print(token)\n",
    "\n",
    "\n",
    "# COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "tokens_compressed = []\n",
    "for token_group in regrouped_tokens:\n",
    "    if isinstance(token_group, list):\n",
    "        wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "        token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "        tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "    else:\n",
    "        tokens_compressed.append(token_group)\n",
    "\n",
    "\n",
    "for token in tokens_compressed:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n e\n",
      "[(2, 'n'), (3, 'e')]\n",
      "v a a s s n j a a a k k k e s e\n",
      "[(6, 'v'), (7, 'a'), (8, 'a'), (11, 's'), (12, 's'), (14, 'n'), (17, 'j'), (18, 'a'), (19, 'a'), (20, 'a'), (21, 'k'), (22, 'k'), (25, 'k'), (28, 'e'), (32, 's'), (35, 'e')]\n",
      "d d r r r ø ø m m\n",
      "[(41, 'd'), (42, 'd'), (43, 'r'), (44, 'r'), (45, 'r'), (46, 'ø'), (47, 'ø'), (48, 'm'), (49, 'm')]\n",
      "o m m\n",
      "[(56, 'o'), (57, 'm'), (58, 'm')]\n",
      "s e e i i i e e r\n",
      "[(67, 's'), (71, 'e'), (72, 'e'), (73, 'i'), (74, 'i'), (75, 'i'), (83, 'e'), (84, 'e'), (85, 'r')]\n",
      "k k n n u u s\n",
      "[(95, 'k'), (96, 'k'), (101, 'n'), (102, 'n'), (103, 'u'), (104, 'u'), (113, 's')]\n",
      "a a a v v\n",
      "[(119, 'a'), (120, 'a'), (121, 'a'), (123, 'v'), (124, 'v')]\n",
      "r r u u t t i s s s k e e e\n",
      "[(127, 'r'), (128, 'r'), (129, 'u'), (130, 'u'), (136, 't'), (137, 't'), (139, 'i'), (144, 's'), (145, 's'), (146, 's'), (149, 'k'), (151, 'e'), (152, 'e'), (153, 'e')]\n",
      "g j e e k k a a a a r r i i n n e e\n",
      "[(163, 'g'), (164, 'j'), (165, 'e'), (166, 'e'), (170, 'k'), (171, 'k'), (172, 'a'), (173, 'a'), (174, 'a'), (178, 'a'), (181, 'r'), (182, 'r'), (184, 'i'), (185, 'i'), (190, 'n'), (191, 'n'), (193, 'e'), (194, 'e')]\n"
     ]
    }
   ],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS w/ FRAME NOS. & DISREGARDING ALL THE PADS\n",
    "\n",
    "clean_logit_frames = []\n",
    "for i, item in enumerate(logit_frames_decoded):\n",
    "    if item != \"[PAD]\":\n",
    "        clean_logit_frames.append((i, item))\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(clean_logit_frames):\n",
    "    if i == 0:\n",
    "        if char[1] == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if char[1] != \"\":\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if clean_logit_frames[i-1][1] == \"\" and char[1] != \"\":  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "            elif char[1] != \"\":\n",
    "                new_word.append(char)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(\" \".join([char[1] for char in item]))\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE ALIGNMENT BETWEEN ASD TOKENS/WORDS & LOGIT FRAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDE THE ASD COSDIST PER NO. OF FRAMES BASED ON ALIGNMENT & ASIGN AVERAGE COSDIST PER FRAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING THE ASD COSDIST PER NO. FRAMES\n",
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ASD_cosdist_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ASD_cosdist_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS MATRIX INTO THEIR CORRESPONDING WORDS, WHILE RETAINING ROW NOS. OR FRAME NOS.\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(logit_frames_decoded):\n",
    "    if i == 0:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if logit_frames_decoded[i-1] in [\"\", \"[PAD]\"] and char not in [\"\", \"[PAD]\"]:\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if \"\" in logit_frames_decoded[i-2:i-1] and char not in [\"\", \"[PAD]\"]:  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "            elif logit_frames_decoded[i-1] == char and char != \"\":  # repeating characters\n",
    "                new_word.append((i, char))\n",
    "            elif logit_frames_decoded[i-1] != char and char != \"\":  # new character detected\n",
    "                new_word.append((i, char))\n",
    "\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # detecting applicable frames & caclulating average cosdist\n",
    "        # frame_group = []\n",
    "        # for token_char in token_char_list:\n",
    "        #     detect_count = 0\n",
    "        #     if token_char in [x[1] for x in char_group_copy]:\n",
    "        #         for i, char in enumerate(char_group_copy):\n",
    "        #             # print(char[1], token_char, [x[1] for x in char_group_copy])\n",
    "        #             if char[1] == token_char:\n",
    "        #                 frame_group.append(char)\n",
    "        #                 detect_count += 1\n",
    "        #             else:\n",
    "        #                 if detect_count > 0:\n",
    "        #                     for count in range(detect_count):\n",
    "        #                         char_group_copy.pop(0)\n",
    "        #                     break\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # cosdist_divided = token_cosdist / len(frame_group)  # divide ASD token cosdist by number of frames for word\n",
    "        # assign divided ASD token cosdist\n",
    "        # frame_cosdist = []\n",
    "        # for char in frame_group:\n",
    "        #     if char[1] in token_char_list:\n",
    "        #         frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
