{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbest_log_distribution: torch.Size([3, 8]) tensor([[ -86.1507,  -34.0924, -141.7790,  -31.1852,  -26.1941, -140.0957,\n",
      "          -73.2255, -100.2174],\n",
      "        [-124.9425,  -75.4899, -182.2957,  -34.4313,  -91.5705, -142.6367,\n",
      "          -81.5478, -147.0352],\n",
      "        [-116.9663,  -67.3523, -177.0863,  -40.7054,  -70.1901, -145.5527,\n",
      "          -85.9843, -179.9039]], device='cuda:0', requires_grad=True)\n",
      "sum_nbest_log_distribution: torch.Size([8]) tensor([ -86.1507,  -34.0924, -141.7790,  -31.1470,  -26.1941, -140.0159,\n",
      "         -73.2253, -100.2174], device='cuda:0', grad_fn=<LogsumexpBackward0>)\n",
      "normal_nbest_distribution: torch.Size([3, 8]) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6247e-01, 1.0000e+00, 9.2331e-01,\n",
      "         9.9976e-01, 1.0000e+00],\n",
      "        [1.4221e-17, 1.0503e-18, 2.5340e-18, 3.7465e-02, 4.0492e-29, 7.2744e-02,\n",
      "         2.4298e-04, 4.6480e-21],\n",
      "        [4.1393e-14, 3.5927e-15, 4.6369e-16, 7.0606e-05, 7.8121e-20, 3.9390e-03,\n",
      "         2.8762e-06, 2.4694e-35]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "asd_loss: tensor([0.5059, 0.2090, 3.2362, 0.3981, 0.8621, 4.6285, 2.8125, 1.6432],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "asd_loss_mean: tensor(1.7869, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"nbest_log_dist.pkl\", \"rb\") as file:\n",
    "    nbest_log_distribution = pickle.load(file)\n",
    "\n",
    "with open(\"asd_scores.pkl\", \"rb\") as file:\n",
    "    asd_scores = pickle.load(file)\n",
    "\n",
    "def compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=True):\n",
    "    # Computes log distribution\n",
    "    # (n, b) -> (b,): log( p1+p2+...+pn ) = log( exp(log_p1)+exp(log_p2)+...+exp(log_pn) )\n",
    "    sum_nbest_log_distribution = torch.logsumexp(nbest_log_distribution, 0)\n",
    "    print(\"nbest_log_distribution:\", nbest_log_distribution.size(), nbest_log_distribution)\n",
    "    print(\"sum_nbest_log_distribution:\", sum_nbest_log_distribution.size(), sum_nbest_log_distribution)\n",
    "\n",
    "    # Re-normalized over just the N-best hypotheses.\n",
    "    # (n, b) - (b,) -> (n, b): exp(log_p)/exp(log_p_sum) = exp(log_p-log_p_sum)\n",
    "    normal_nbest_distribution = torch.exp(nbest_log_distribution - sum_nbest_log_distribution)\n",
    "    print(\"normal_nbest_distribution:\", normal_nbest_distribution.size(), normal_nbest_distribution)\n",
    "\n",
    "    if normalized_asd == True:\n",
    "        mean_asd = torch.mean(asd_scores, 0)\n",
    "        asd_norm = asd_scores - mean_asd\n",
    "        print(\"mean_asd:\", mean_asd.size(), mean_asd)\n",
    "        print(\"asd_norm:\", asd_norm.size(), asd_norm)\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_norm, 0)\n",
    "    else:\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_scores, 0)\n",
    "\n",
    "    return asd_loss\n",
    "\n",
    "asd_loss = compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=False)\n",
    "print(\"asd_loss:\", asd_loss)\n",
    "print(\"asd_loss_mean:\", torch.mean(asd_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - mASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import jiwer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import BertModel, AutoModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from dtw import *\n",
    "from scipy.spatial import distance\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, display\n",
    "from scipy import stats\n",
    "# pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "metric_modelname = 'ltg/norbert2'  # changed to latest version of NorBERT (20-Mar-2023)\n",
    "metric_model = BertModel.from_pretrained(metric_modelname)\n",
    "metric_tokenizer = AutoTokenizer.from_pretrained(metric_modelname)\n",
    "\n",
    "# this is not working at the moment:\n",
    "# metric_modelname = \"ltg/norbert3-small\"\n",
    "# metric_model = AutoModel.from_pretrained(metric_modelname, trust_remote_code=True)\n",
    "# metric_tokenizer = AutoTokenizer.from_pretrained(metric_modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VISUAL INSPECTION OF ALIGNMENTS\n",
    "\n",
    "def get_asd_alignment(tokenized_ref, tokenized_hyp, model):\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                             hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                             hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(x=hyp_embedding_sequence, y=ref_embedding_sequence, dist_method=\"cosine\", keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "    return alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score\n",
    "\n",
    "\n",
    "def print_token_alignment(tokenizer, alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids):\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "\n",
    "    hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(hyp_alignment_idxs):\n",
    "        hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    hyp_alignment_input_ids_tensor = torch.from_numpy(hyp_alignment_input_ids)\n",
    "\n",
    "    ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(ref_alignment_idxs):\n",
    "        ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    ref_alignment_inpud_ids_tensor = torch.from_numpy(ref_alignment_input_ids)\n",
    "\n",
    "    hyp_alignment_tokens = tokenizer.convert_ids_to_tokens(hyp_alignment_input_ids_tensor)\n",
    "    ref_alignment_tokens = tokenizer.convert_ids_to_tokens(ref_alignment_inpud_ids_tensor)\n",
    "\n",
    "    ref_alignment_token_embeddings = []\n",
    "    for index in ref_alignment_idxs:\n",
    "        ref_alignment_token_embeddings.append(ref_embedding_sequence[index])\n",
    "\n",
    "    hyp_alignment_token_embeddings = []\n",
    "    for index in hyp_alignment_idxs:\n",
    "        hyp_alignment_token_embeddings.append(hyp_embedding_sequence[index])\n",
    "\n",
    "    cosdist_alignment_tokens = []\n",
    "    for i in range(len(ref_alignment_token_embeddings)):\n",
    "        ref_embedding = ref_alignment_token_embeddings[i]\n",
    "        hyp_embedding = hyp_alignment_token_embeddings[i]\n",
    "        cosdist_alignment_tokens.append(round((distance.cosine(ref_embedding, hyp_embedding)), 3))\n",
    "    hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    print(\"Token alignment table:\")\n",
    "    display(HTML(table))\n",
    "\n",
    "    # print(\"ASD score from alignment:\", total_dist/len(ref_alignment_token_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_modelname = \"bert-base-multilingual-cased\"\n",
    "# metric_model_multi = BertModel.from_pretrained(metric_modelname)\n",
    "# metric_tokenizer_multi = AutoTokenizer.from_pretrained(metric_modelname)\n",
    "\n",
    "# def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis, normalized=True):\n",
    "#     ref_text = re.sub(r\"\\s+\", \" \", reference.replace(\"[UNK]\", \"\"))\n",
    "#     hyp_text = re.sub(r\"\\s+\", \" \", hypothesis.replace(\"[UNK]\", \"\").replace(\"</s>\", \"\"))\n",
    "#     tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "#         model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "#     hidden_states_ref = model_output_ref.hidden_states\n",
    "#     hidden_states_hyp = model_output_hyp.hidden_states\n",
    "#     all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "#                             hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "#                             hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "#     all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "#                                 hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "#                                 hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "#     output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "#     output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "#     alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "#     num_tokens = len(output_mean_reference)\n",
    "#     if normalized == True:\n",
    "#         asd_score = alignment.distance / num_tokens\n",
    "#     else:\n",
    "#         asd_score = alignment.distance\n",
    "#     return asd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_results(mod_loss_results, orig_loss_results):\n",
    "    merged_results = orig_loss_results.join(mod_loss_results.set_index(\"segment_id\"), on=\"segment_id\")\n",
    "    merged_results = merged_results[['segment_id', 'ref_str', 'orig_asr', 'orig_cer', 'orig_wer', 'orig_asd', 'mod_asr', 'mod_cer', 'mod_wer', 'mod_asd']]\n",
    "    return merged_results\n",
    "\n",
    "def load_csv_to_df(csv_file, mod_loss):\n",
    "    results_df = pd.read_csv(csv_file, index_col=0)\n",
    "    if mod_loss == True:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\", \"ref_str\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"mod_asr\", \"wer\":\"mod_wer\", \"asd\":\"mod_asd\", \"cer\":\"mod_cer\"})\n",
    "    else:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"orig_asr\", \"wer\":\"orig_wer\", \"asd\":\"orig_asd\", \"cer\":\"orig_cer\"})\n",
    "    return results_df\n",
    "\n",
    "def load_results_to_df(dir_path, mod_loss):\n",
    "    df_list = []\n",
    "    for (root, dirs, files) in os.walk(dir_path, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(dir_path, fn)\n",
    "                df_list.append(load_csv_to_df(csv_path, mod_loss))\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_dir = \"../../nlp_models/ner_pos/finetuned_bert_pos_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pos_model_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pos_model_dir)\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "def get_pos_tags(nlp_pipeline, text, label_list_pos):\n",
    "    pos_tags = nlp_pipeline(text)\n",
    "\n",
    "    # error count vector\n",
    "    # pos_error_count = np.zeros(len(label_list_pos))\n",
    "    # for x in pos_tags:\n",
    "    #     print(x[\"entity\"])\n",
    "    #     pos_error_count[int(x[\"entity\"].split(\"_\")[1])] += 1\n",
    "\n",
    "    # merging subword tokens and making word list wiht pos tag list\n",
    "    labels = [label_list_pos[int(x[\"entity\"].split(\"_\")[1])] for x in pos_tags]\n",
    "    sub_words = [x[\"word\"] for x in pos_tags]\n",
    "    idx_for_labels = []\n",
    "    for i, item in enumerate(sub_words):\n",
    "        if item[0:2] != \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                idx_for_labels.append(i)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                idx_for_labels.append(i)\n",
    "        if item[0:2] == \"##\" and sub_words[i-1][0:2] != \"##\":\n",
    "            sub_word_combined = []\n",
    "            sub_word_combined.append(i-1)\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "        elif item[0:2] == \"##\" and sub_words[i-1][0:2] == \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "    new_word_list = []\n",
    "    new_label_list = []\n",
    "    for idx in idx_for_labels:\n",
    "        if type(idx) is int:\n",
    "            new_word_list.append(sub_words[idx])\n",
    "            new_label_list.append(labels[idx])\n",
    "        else:\n",
    "            new_word_list.append(\"\".join(sub_words[idx[0]:idx[1]]).replace(\"##\", \"\"))\n",
    "            new_label_list.append(labels[idx[0]])\n",
    "\n",
    "    return new_word_list, new_label_list\n",
    "\n",
    "\n",
    "def extract_errors_pos_tags(sub_count, ins_count, del_count, pos_tags, alignment, ref_labels, hyp_labels,\n",
    "                            ref_words, hyp_words):\n",
    "    sub = 0\n",
    "    ins = 0\n",
    "    dele = 0\n",
    "    pos = []\n",
    "    # ref_error_words = []\n",
    "    # asr_error_words = []\n",
    "    error_dict = []\n",
    "\n",
    "    for i in range(len(alignment)):\n",
    "        ref_start = alignment[i].ref_start_idx\n",
    "        ref_end = alignment[i].ref_end_idx\n",
    "        hyp_start = alignment[i].hyp_start_idx\n",
    "        hyp_end = alignment[i].hyp_end_idx\n",
    "\n",
    "        if alignment[i].type != \"equal\":\n",
    "            errors = {\n",
    "                \"type\": alignment[i].type,\n",
    "                \"ref\": ref_words[ref_start:ref_end],\n",
    "                \"ref_pos\": ref_labels[ref_start:ref_end],\n",
    "                \"hyp\": hyp_words[hyp_start:hyp_end],\n",
    "                \"hyp_pos\": hyp_labels[hyp_start:hyp_end]\n",
    "            }\n",
    "            error_dict.append(errors)\n",
    "\n",
    "        if alignment[i].type == \"substitute\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                sub += 1\n",
    "            # for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "            #     ref_words[ref_start+i] = word.upper()\n",
    "            #     ref_error_words.append(word)\n",
    "            # for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "            #     hyp_words[ref_start+i] = word.upper()\n",
    "            #     asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"insert\":\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "                ins += 1\n",
    "            # for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "            #     hyp_words[ref_start+i] = word.upper()\n",
    "            #     asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"delete\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                dele += 1\n",
    "            # for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "            #     ref_words[ref_start+i] = word.upper()\n",
    "            #     ref_error_words.append(word)\n",
    "\n",
    "    sub_count.append(sub)\n",
    "    ins_count.append(ins)\n",
    "    del_count.append(dele)\n",
    "    pos_tags.append(pos)\n",
    "\n",
    "    error_ref_text = \" \".join(ref_words)\n",
    "    error_asr_text = \" \".join(hyp_words)\n",
    "\n",
    "    return error_ref_text, error_asr_text, error_dict\n",
    "\n",
    "\n",
    "def merge_df_wer_pos_tags(df, nlp_pipeline, label_list_pos):\n",
    "    mod_error_ref = []\n",
    "    mod_error_asr = []\n",
    "    # mod_ref_error_words = []\n",
    "    # mod_asr_error_words = []\n",
    "    mod_error_dict_list = []\n",
    "    # mod_pos_err_count_list = []\n",
    "\n",
    "    orig_error_ref = []\n",
    "    orig_error_asr = []\n",
    "    # orig_ref_error_words = []\n",
    "    # orig_asr_error_words = []\n",
    "    orig_error_dict_list = []\n",
    "    # orig_pos_err_count_list = []\n",
    "\n",
    "    mod_sub_count = []\n",
    "    mod_ins_count = []\n",
    "    mod_del_count = []\n",
    "    mod_pos_tags = []\n",
    "\n",
    "    orig_sub_count = []\n",
    "    orig_ins_count = []\n",
    "    orig_del_count = []\n",
    "    orig_pos_tags = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Error Tagging\"):\n",
    "        ref_words, ref_labels = get_pos_tags(nlp_pipeline, row.ref_str, label_list_pos)\n",
    "        orig_words, orig_labels = get_pos_tags(nlp_pipeline, row.orig_asr, label_list_pos)\n",
    "        mod_words, mod_labels = get_pos_tags(nlp_pipeline, row.mod_asr, label_list_pos)\n",
    "        out = jiwer.process_words([row.ref_str, row.ref_str], [row.orig_asr, row.mod_asr])\n",
    "        orig_alignment = out.alignments[0]\n",
    "        mod_alignment = out.alignments[1]\n",
    "\n",
    "        # substitute, insert, delete\n",
    "        mod_ref_text, mod_asr_text, mod_error_dict = extract_errors_pos_tags(mod_sub_count, mod_ins_count,\n",
    "                                                                             mod_del_count, mod_pos_tags, mod_alignment,\n",
    "                                                                             ref_labels, mod_labels, ref_words, mod_words)\n",
    "        orig_ref_text, orig_asr_text, orig_error_dict = extract_errors_pos_tags(orig_sub_count, orig_ins_count,\n",
    "                                                                                orig_del_count, orig_pos_tags,\n",
    "                                                                                orig_alignment, ref_labels,\n",
    "                                                                                orig_labels, ref_words, orig_words)\n",
    "\n",
    "        # whole text of the utterances\n",
    "        mod_error_ref.append(mod_ref_text)\n",
    "        mod_error_asr.append(mod_asr_text)\n",
    "        orig_error_ref.append(orig_ref_text)\n",
    "        orig_error_asr.append(orig_asr_text)\n",
    "\n",
    "        # specific words that are mistaken\n",
    "        # mod_ref_error_words.append(mod_ref_error)\n",
    "        # mod_asr_error_words.append(mod_asr_error)\n",
    "        # orig_ref_error_words.append(orig_ref_error)\n",
    "        # orig_asr_error_words.append(orig_asr_error)\n",
    "\n",
    "        # dictionary of wrong words\n",
    "        mod_error_dict_list.append(mod_error_dict)\n",
    "        # mod_pos_err_count_list.append(mod_pos_err_count)\n",
    "        orig_error_dict_list.append(orig_error_dict)\n",
    "        # orig_pos_err_count_list.append(orig_pos_err_count)\n",
    "\n",
    "    df.drop(labels=[\"ref_str\", \"orig_asr\", \"mod_asr\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    df[\"orig_ref\"] = orig_error_ref  # text/sentence form with uppercase errors\n",
    "    df[\"orig_asr\"] = orig_error_asr  # text/sentence form with uppercase errors\n",
    "    # df[\"orig_ref_error_words\"] = orig_ref_error_words\n",
    "    # df[\"orig_asr_error_words\"] = orig_asr_error_words\n",
    "    df[\"orig_error_dict\"] = orig_error_dict_list\n",
    "    # df[\"orig_pos_err_count\"] = orig_pos_err_count_list\n",
    "\n",
    "    df[\"orig_sub\"] = orig_sub_count\n",
    "    df[\"orig_ins\"] = orig_ins_count\n",
    "    df[\"orig_del\"] = orig_del_count\n",
    "    df[\"orig_pos_tags\"] = orig_pos_tags\n",
    "\n",
    "    df[\"mod_ref\"] = mod_error_ref  # text/sentence form with uppercase errors\n",
    "    df[\"mod_asr\"] = mod_error_asr  # text/sentence form with uppercase errors\n",
    "    # df[\"mod_ref_error_words\"] = mod_ref_error_words\n",
    "    # df[\"mod_asr_error_words\"] = mod_asr_error_words\n",
    "    df[\"mod_error_dict\"] = mod_error_dict_list\n",
    "    # df[\"mod_pos_err_count\"] = mod_pos_err_count_list\n",
    "\n",
    "    df[\"mod_sub\"] = mod_sub_count\n",
    "    df[\"mod_ins\"] = mod_ins_count\n",
    "    df[\"mod_del\"] = mod_del_count\n",
    "    df[\"mod_pos_tags\"] = mod_pos_tags\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the test transcriptions\n",
    "# path = \"./logs/trial35_masd_RundkastOnly_aulus7\"\n",
    "# mod_results = load_results_to_df(path, mod_loss=True)\n",
    "# path = \"./logs/trial_origloss_RundkastOnly_aulus7\"\n",
    "# orig_results = load_results_to_df(path, mod_loss=False)\n",
    "\n",
    "# merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# # higher asd score but lower wer when using modified loss\n",
    "# mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "# mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "# mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "# print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# # lower asd score but higher wer when using modified loss\n",
    "# mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "# mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "# mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "# print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# # merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "# df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "# df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('mod_pos_tags').reset_index(name='count')\n",
    "# print(\"mod loss pos errors:\", df_mod_pos_count[\"count\"].sum(axis=0))\n",
    "# print(df_mod_pos_count)\n",
    "\n",
    "# df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "# df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('orig_pos_tags').reset_index(name='count')\n",
    "# print(\"orig loss pos errors:\", df_orig_pos_count[\"count\"].sum(axis=0))\n",
    "# print(df_orig_pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results & POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD but lower WER - using mod loss: 7\n",
      "lower ASD but higher or equal WER - using mod loss: 222\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "mod_results = load_results_to_df(\"./logs/trial60_masd_6ep_allDataSmall_titan1\", mod_loss=True)\n",
    "orig_results = load_results_to_df(\"./logs/trial_origloss_6ep_allDataSmall_aulus7\", mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd_df = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"mod_wer\"] < mod_higher_asd_df[\"orig_wer\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD but lower WER - using mod loss:\", len(mod_higher_asd_df))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd_df = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"mod_wer\"] >= mod_lower_asd_df[\"orig_wer\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD but higher or equal WER - using mod loss:\", len(mod_lower_asd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_mod = []\n",
    "# multi_orig = []\n",
    "\n",
    "# for row in tqdm(merged_results.itertuples(), total=merged_results.shape[0], desc=\"multi-ASD scoring\"):\n",
    "#     multi_orig.append(compute_asd_score_single_utt(metric_model_multi, metric_tokenizer_multi, row.ref_str, row.orig_asr, normalized=True))\n",
    "#     multi_mod.append(compute_asd_score_single_utt(metric_model_multi, metric_tokenizer_multi, row.ref_str, row.mod_asr, normalized=True))\n",
    "\n",
    "# merged_results[\"orig_multi-asd\"] = multi_orig\n",
    "# merged_results[\"mod_multi-asd\"] = multi_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS error count (NOT equal CER utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 382/382 [00:37<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances for POS analysis: 382\n",
      "mod loss pos errors: 1950\n",
      "orig loss pos errors: 1989\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "\n",
    "not_equal_cer = merged_results[merged_results[\"mod_cer\"] != merged_results[\"orig_cer\"]]\n",
    "df_pos = merge_df_wer_pos_tags(not_equal_cer.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "print(\"number of utterances for POS analysis:\", len(df_pos))\n",
    "\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "# print(\"number of utterances for POS analysis:\", len(df_pos))\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('pos_tags').reset_index(name='CTC+ASD')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"CTC+ASD\"].sum(axis=0))\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('pos_tags').reset_index(name='CTC')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"CTC\"].sum(axis=0))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "merged_pos_count = pd.merge(df_orig_pos_count, df_mod_pos_count, on=[\"pos_tags\"])\n",
    "merged_pos_count = pd.melt(merged_pos_count, id_vars=[\"pos_tags\"], value_vars=[\"CTC\", \"CTC+ASD\"])\n",
    "merged_pos_count = merged_pos_count.rename(columns={\"variable\":\"model\", \"value\":\"count\"})\n",
    "# merged_pos_count = merged_pos_count.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax = sns.barplot(data=merged_pos_count, x=\"pos_tags\", y=\"count\", hue=\"model\", palette=\"dark\", alpha=0.7)\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.grid(axis='y')\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"POS Tags\")\n",
    "# sns.move_legend(ax, bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- among the nouns that were wrong, look at the lemmas\n",
    "- look at the adpositions and pronouns as well\n",
    "- is ASD making improvements, where are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD but lower WER - using mod loss: 6\n",
      "lower ASD but higher or equal WER - using mod loss: 47\n"
     ]
    }
   ],
   "source": [
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd_df = df_pos[df_pos[\"orig_asd\"] < df_pos[\"mod_asd\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"mod_wer\"] < mod_higher_asd_df[\"orig_wer\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD but lower WER - using mod loss:\", len(mod_higher_asd_df))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd_df = df_pos[df_pos[\"mod_asd\"] < df_pos[\"orig_asd\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"mod_wer\"] >= mod_lower_asd_df[\"orig_wer\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD but higher or equal WER - using mod loss:\", len(mod_lower_asd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>orig_cer</th>\n",
       "      <th>orig_wer</th>\n",
       "      <th>orig_asd</th>\n",
       "      <th>mod_cer</th>\n",
       "      <th>mod_wer</th>\n",
       "      <th>mod_asd</th>\n",
       "      <th>orig_ref</th>\n",
       "      <th>orig_asr</th>\n",
       "      <th>orig_error_dict</th>\n",
       "      <th>orig_sub</th>\n",
       "      <th>orig_ins</th>\n",
       "      <th>orig_del</th>\n",
       "      <th>orig_pos_tags</th>\n",
       "      <th>mod_ref</th>\n",
       "      <th>mod_asr</th>\n",
       "      <th>mod_error_dict</th>\n",
       "      <th>mod_sub</th>\n",
       "      <th>mod_ins</th>\n",
       "      <th>mod_del</th>\n",
       "      <th>mod_pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_100</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.138008</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.129134</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['an'], 'ref_po...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PROPN]</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['an', 'cathrin...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PROPN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_102</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.212464</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.084633</td>\n",
       "      <td>kjent at viruset kan overleve flere timer uten...</td>\n",
       "      <td>at viruset kan overleve flere timer utenfor me...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['kjent'], 'ref_pos...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, DET, NOUN, NOUN]</td>\n",
       "      <td>kjent at viruset kan overleve flere timer uten...</td>\n",
       "      <td>at viruset kan overleve flere timer utenfor me...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['kjent'], 'ref_pos...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, PROPN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_103</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.091840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_106</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.196988</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076057</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['märtha', 'lou...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, PROPN, PROPN, VERB]</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['märtha'], 're...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_108</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.168720</td>\n",
       "      <td>gang slottets informasjonssjef wenche rask for...</td>\n",
       "      <td>gang slottets informasjonssjef venke rask forr...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['wenche'], 're...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, PRON]</td>\n",
       "      <td>gang slottets informasjonssjef wenche rask for...</td>\n",
       "      <td>gang slottets informasjonssjef wenke rask for ...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['wenche'], 're...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, DET, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.139344</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.268045</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.249815</td>\n",
       "      <td>ikke liker det de står for vil de altså tvinge...</td>\n",
       "      <td>ikke liker det de står for så vil også tvinge ...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[ADV, PRON, ADV, ADP, AUX, VERB]</td>\n",
       "      <td>ikke liker det de står for vil de altså tvinge...</td>\n",
       "      <td>liker det de står for så vil det også tvinge f...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['ikke'], 'ref_pos'...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[PART, ADV, PRON, ADV, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.127637</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.112867</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelser a...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelse av...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['masseoppsigel...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, ADP]</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelser a...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelse av...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['masseoppsigel...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, NOUN, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.253129</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.151328</td>\n",
       "      <td>fremskrittspartiet nå vil fjerne hele formuess...</td>\n",
       "      <td>partiene vil fjerne hele formunsskatten som jo...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['fremskrittspa...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, ADV, NOUN, PRON]</td>\n",
       "      <td>fremskrittspartiet nå vil fjerne hele formuess...</td>\n",
       "      <td>partiene vil fjerne hele formuesskatten som jo...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['fremskrittspa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, ADV, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.079557</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.192340</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['å'], 'ref_pos...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PART, VERB]</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['å'], 'ref_pos...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[PART, VERB, VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.817789</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.853918</td>\n",
       "      <td>økonomisk men derfor står de også helt aleine ...</td>\n",
       "      <td>økonomisk men derfor så står det også helt ale...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PRON, DET, DET, DET, NOUN, AUX, NOUN, PR...</td>\n",
       "      <td>økonomisk men derfor står de også helt aleine ...</td>\n",
       "      <td>økonomisk men derfor så står det også helt ale...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PRON, DET, DET, DET, NOUN, AUX, NOUN, PR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            segment_id  orig_cer  orig_wer  \\\n",
       "0                         N2_030505_NRK_D12_NO_cut_100  0.031496      0.10   \n",
       "1                         N2_030505_NRK_D12_NO_cut_102  0.084746      0.20   \n",
       "2                         N2_030505_NRK_D12_NO_cut_103  0.015625      0.10   \n",
       "3                         N2_030505_NRK_D12_NO_cut_106  0.062500      0.20   \n",
       "4                         N2_030505_NRK_D12_NO_cut_108  0.048276      0.10   \n",
       "..                                                 ...       ...       ...   \n",
       "377  2021H264-fullStorting0510Stortinget-20210510-1...  0.139344      0.30   \n",
       "378  2021H264-fullStorting0510Stortinget-20210510-1...  0.058394      0.25   \n",
       "379  2021H264-fullStorting0510Stortinget-20210510-1...  0.138686      0.20   \n",
       "380  2021H264-fullStorting0510Stortinget-20210510-1...  0.026786      0.10   \n",
       "381  2021H264-fullStorting0510Stortinget-20210510-1...  0.796748      0.85   \n",
       "\n",
       "     orig_asd   mod_cer  mod_wer   mod_asd  \\\n",
       "0    0.138008  0.023622     0.15  0.129134   \n",
       "1    0.212464  0.076271     0.15  0.084633   \n",
       "2    0.091840  0.000000     0.00  0.000000   \n",
       "3    0.196988  0.008929     0.05  0.076057   \n",
       "4    0.143233  0.068966     0.15  0.168720   \n",
       "..        ...       ...      ...       ...   \n",
       "377  0.268045  0.147541     0.30  0.249815   \n",
       "378  0.127637  0.051095     0.20  0.112867   \n",
       "379  0.253129  0.131387     0.15  0.151328   \n",
       "380  0.079557  0.053571     0.20  0.192340   \n",
       "381  0.817789  0.804878     0.85  0.853918   \n",
       "\n",
       "                                              orig_ref  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    kjent at viruset kan overleve flere timer uten...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenche rask for...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for vil de altså tvinge...   \n",
       "378  flertall så ville det blitt masseoppsigelser a...   \n",
       "379  fremskrittspartiet nå vil fjerne hele formuess...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor står de også helt aleine ...   \n",
       "\n",
       "                                              orig_asr  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    at viruset kan overleve flere timer utenfor me...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef venke rask forr...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for så vil også tvinge ...   \n",
       "378  flertall så ville det blitt masseoppsigelse av...   \n",
       "379  partiene vil fjerne hele formunsskatten som jo...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor så står det også helt ale...   \n",
       "\n",
       "                                       orig_error_dict  orig_sub  orig_ins  \\\n",
       "0    [{'type': 'substitute', 'ref': ['an'], 'ref_po...         2         0   \n",
       "1    [{'type': 'delete', 'ref': ['kjent'], 'ref_pos...         1         2   \n",
       "2    [{'type': 'insert', 'ref': [], 'ref_pos': [], ...         1         1   \n",
       "3    [{'type': 'substitute', 'ref': ['märtha', 'lou...         3         0   \n",
       "4    [{'type': 'substitute', 'ref': ['wenche'], 're...         1         0   \n",
       "..                                                 ...       ...       ...   \n",
       "377  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...         2         1   \n",
       "378  [{'type': 'substitute', 'ref': ['masseoppsigel...         4         0   \n",
       "379  [{'type': 'substitute', 'ref': ['fremskrittspa...         2         1   \n",
       "380  [{'type': 'substitute', 'ref': ['å'], 'ref_pos...         1         0   \n",
       "381  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...         2        15   \n",
       "\n",
       "     orig_del                                      orig_pos_tags  \\\n",
       "0           0                                       [ADV, PROPN]   \n",
       "1           1                             [ADJ, DET, NOUN, NOUN]   \n",
       "2           0                                     [PROPN, PROPN]   \n",
       "3           1                        [PROPN, PROPN, PROPN, VERB]   \n",
       "4           1                                      [PROPN, PRON]   \n",
       "..        ...                                                ...   \n",
       "377         3                   [ADV, PRON, ADV, ADP, AUX, VERB]   \n",
       "378         1                      [NOUN, NOUN, NOUN, NOUN, ADP]   \n",
       "379         1                            [NOUN, ADV, NOUN, PRON]   \n",
       "380         1                                       [PART, VERB]   \n",
       "381         0  [ADV, PRON, DET, DET, DET, NOUN, AUX, NOUN, PR...   \n",
       "\n",
       "                                               mod_ref  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    kjent at viruset kan overleve flere timer uten...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenche rask for...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for vil de altså tvinge...   \n",
       "378  flertall så ville det blitt masseoppsigelser a...   \n",
       "379  fremskrittspartiet nå vil fjerne hele formuess...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor står de også helt aleine ...   \n",
       "\n",
       "                                               mod_asr  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    at viruset kan overleve flere timer utenfor me...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenke rask for ...   \n",
       "..                                                 ...   \n",
       "377  liker det de står for så vil det også tvinge f...   \n",
       "378  flertall så ville det blitt masseoppsigelse av...   \n",
       "379  partiene vil fjerne hele formuesskatten som jo...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor så står det også helt ale...   \n",
       "\n",
       "                                        mod_error_dict  mod_sub  mod_ins  \\\n",
       "0    [{'type': 'substitute', 'ref': ['an', 'cathrin...        3        0   \n",
       "1    [{'type': 'delete', 'ref': ['kjent'], 'ref_pos...        1        1   \n",
       "2                                                   []        0        0   \n",
       "3    [{'type': 'substitute', 'ref': ['märtha'], 're...        1        0   \n",
       "4    [{'type': 'substitute', 'ref': ['wenche'], 're...        2        0   \n",
       "..                                                 ...      ...      ...   \n",
       "377  [{'type': 'delete', 'ref': ['ikke'], 'ref_pos'...        2        1   \n",
       "378  [{'type': 'substitute', 'ref': ['masseoppsigel...        3        0   \n",
       "379  [{'type': 'substitute', 'ref': ['fremskrittspa...        1        1   \n",
       "380  [{'type': 'substitute', 'ref': ['å'], 'ref_pos...        2        1   \n",
       "381  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...        2       15   \n",
       "\n",
       "     mod_del                                       mod_pos_tags  \n",
       "0          0                                [ADV, PROPN, PROPN]  \n",
       "1          1                                 [ADJ, PROPN, NOUN]  \n",
       "2          0                                                 []  \n",
       "3          0                                            [PROPN]  \n",
       "4          1                                 [PROPN, DET, PRON]  \n",
       "..       ...                                                ...  \n",
       "377        3                  [PART, ADV, PRON, ADV, ADP, VERB]  \n",
       "378        1                            [NOUN, NOUN, NOUN, ADP]  \n",
       "379        1                                  [NOUN, ADV, PRON]  \n",
       "380        1                           [PART, VERB, VERB, VERB]  \n",
       "381        0  [ADV, PRON, DET, DET, DET, NOUN, AUX, NOUN, PR...  \n",
       "\n",
       "[382 rows x 21 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between POS errors & ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 829/829 [01:23<00:00,  9.97it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>ASD_Pearson_r</th>\n",
       "      <th>WER_Pearson_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>X</td>\n",
       "      <td>0.107091</td>\n",
       "      <td>0.095872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ADV</td>\n",
       "      <td>0.076968</td>\n",
       "      <td>0.076742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>0.066592</td>\n",
       "      <td>0.073863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>0.069228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>0.019963</td>\n",
       "      <td>0.053997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DET</td>\n",
       "      <td>0.012511</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PRON</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.022087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.020121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VERB</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>0.026021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PART</td>\n",
       "      <td>-0.020840</td>\n",
       "      <td>0.008559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>-0.023058</td>\n",
       "      <td>-0.015174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>-0.027016</td>\n",
       "      <td>-0.038653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>-0.029199</td>\n",
       "      <td>-0.021489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADP</td>\n",
       "      <td>-0.051963</td>\n",
       "      <td>-0.108331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>-0.081762</td>\n",
       "      <td>-0.069944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AUX</td>\n",
       "      <td>-0.121659</td>\n",
       "      <td>-0.108993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      POS  ASD_Pearson_r  WER_Pearson_r\n",
       "12      X       0.107091       0.095872\n",
       "13    ADV       0.076968       0.076742\n",
       "1   PUNCT       0.066592       0.073863\n",
       "9   CCONJ       0.038502       0.069228\n",
       "14   INTJ       0.019963       0.053997\n",
       "8     DET       0.012511       0.000461\n",
       "11   PRON       0.010388       0.022087\n",
       "3     NUM       0.007474       0.020121\n",
       "15   VERB      -0.012770       0.026021\n",
       "7    PART      -0.020840       0.008559\n",
       "10  PROPN      -0.023058      -0.015174\n",
       "6     ADJ      -0.027016      -0.038653\n",
       "5   SCONJ      -0.029199      -0.021489\n",
       "2     ADP      -0.051963      -0.108331\n",
       "0    NOUN      -0.081762      -0.069944\n",
       "16    AUX      -0.121659      -0.108993\n",
       "4     SYM            NaN            NaN"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_corr = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "data_source = df_pos_corr.copy(deep=True)\n",
    "data_source = data_source[data_source[\"mod_wer\"] <= 0.2]\n",
    "data_source = data_source.reset_index(drop=True)\n",
    "\n",
    "data_matrix = np.zeros((len(label_list_pos),len(data_source)), dtype=int)\n",
    "for row_num, row in data_source.iterrows():\n",
    "    for idx, count in enumerate(row.mod_pos_err_count):\n",
    "        data_matrix[idx][row_num] += count\n",
    "\n",
    "asd_values = data_source.mod_asd\n",
    "wer_values = data_source.mod_wer\n",
    "\n",
    "pos_asd_pearson_stat = []\n",
    "pos_wer_pearson_stat = []\n",
    "for row_num, row in enumerate(data_matrix):\n",
    "    pos_item = label_list_pos[row_num]\n",
    "    pos_error = row\n",
    "    pos_asd_pearson_stat.append(stats.pearsonr(pos_error, asd_values)[0])\n",
    "    pos_wer_pearson_stat.append(stats.pearsonr(pos_error, wer_values)[0])\n",
    "\n",
    "pos_corr_df = pd.DataFrame(data=zip(label_list_pos, pos_asd_pearson_stat, pos_wer_pearson_stat), columns=[\"POS\", \"ASD_Pearson_r\", \"WER_Pearson_r\"])\n",
    "pos_corr_ASDsorted_df = pos_corr_df.sort_values(by=\"ASD_Pearson_r\", ascending=False)\n",
    "pos_corr_ASDsorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the errors made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher asd: 180\n",
      "lower asd: 202\n"
     ]
    }
   ],
   "source": [
    "higher_asd_df = df_pos[df_pos[\"orig_asd\"] < df_pos[\"mod_asd\"]]\n",
    "lower_asd_df = df_pos[df_pos[\"mod_asd\"] < df_pos[\"orig_asd\"]]\n",
    "\n",
    "print(\"higher asd:\", len(higher_asd_df))\n",
    "print(\"lower asd:\", len(lower_asd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_556170/2773266186.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lower_asd_df[\"asd_score_diff\"] = lower_asd_df[\"orig_asd\"] - lower_asd_df[\"mod_asd\"]\n"
     ]
    }
   ],
   "source": [
    "lower_asd_df[\"asd_score_diff\"] = lower_asd_df[\"orig_asd\"] - lower_asd_df[\"mod_asd\"]\n",
    "lower_asd_df = lower_asd_df.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_556170/3867863823.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  higher_asd_df[\"asd_score_diff\"] = higher_asd_df[\"mod_asd\"] - higher_asd_df[\"orig_asd\"]\n"
     ]
    }
   ],
   "source": [
    "higher_asd_df[\"asd_score_diff\"] = higher_asd_df[\"mod_asd\"] - higher_asd_df[\"orig_asd\"]\n",
    "higher_asd_df = higher_asd_df.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an example with errors to demonstrate errors that we want to analyze: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when nouns are incorrect, we expect the asd score to be higher\n",
    "# fine-tuning with ASD should reduce these types of errors\n",
    "\n",
    "noun_errors_df = lower_asd_df.copy(deep=True)\n",
    "\n",
    "orig_noun_errors = []\n",
    "mod_noun_errors = []\n",
    "for row in noun_errors_df.itertuples():\n",
    "    orig_errors = []\n",
    "    mod_errors = []\n",
    "    for error_dict in row.orig_error_dict:\n",
    "        if \"NOUN\" in error_dict[\"ref_pos\"]:\n",
    "            orig_errors.append((error_dict[\"ref\"], error_dict[\"hyp\"]))\n",
    "    for error_dict in row.mod_error_dict:\n",
    "        if \"NOUN\" in error_dict[\"ref_pos\"]:\n",
    "            mod_errors.append((error_dict[\"ref\"], error_dict[\"hyp\"]))\n",
    "    orig_noun_errors.append(orig_errors)\n",
    "    mod_noun_errors.append(mod_errors)\n",
    "\n",
    "noun_errors_df[\"orig_noun_errors\"] = orig_noun_errors\n",
    "noun_errors_df[\"mod_noun_errors\"] = mod_noun_errors\n",
    "\n",
    "# noun_errors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower ASD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Notes for paper:</u>\n",
    "- difference in the results will highly depend on the token vocabulary, look at the vocab of the tokenizer\n",
    "- systematic analysis - looking at the overall POS counts, word stems & lemmas (we expect ASD not to be too high if mistaken)\n",
    "- look if fixed phrases, multi-word expressions, or compound words were correctly transcribed\n",
    "- distribution \n",
    "- entropy of the logits produced by the model with original loss & with modified loss\n",
    "- example-based \n",
    "- changing the metric does not necessarily reduce the word error rate\n",
    "- it's okay to introduce the proposed approach and the difficulties faced during implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['utføre', 'ut'], 'hyp_pos': ['VERB', 'ADP']}\n",
      "{'type': 'substitute', 'ref': ['utfor', 'autovernet'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['o', 'vernet'], 'hyp_pos': ['NUM', 'NOUN']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.40\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utfor </td><td>utfor</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utføre</td><td>ut   </td><td>o    </td><td>vernet  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.022</td><td>0.024 </td><td>0.015  </td><td>0.016</td><td>0.054 </td><td>0.028</td><td>0.057</td><td>0.13</td><td>0.637 </td><td>0.664</td><td>0.706</td><td>0.532   </td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['utfor'], 'ref_pos': ['ADP'], 'hyp': ['utføre'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utfor </td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utføre</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.005</td><td>0.008 </td><td>0.004  </td><td>0.005</td><td>0.013 </td><td>0.015</td><td>0.024</td><td>0.113</td><td>0.541 </td><td>0.054</td><td>0.052   </td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['må'], 'ref_pos': ['AUX'], 'hyp': ['vil'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['ikke', 'herske', 'tvil', 'om'], 'ref_pos': ['PART', 'VERB', 'NOUN', 'ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mot  </td><td>at   </td><td>interesse</td><td>##grupper</td><td>får  </td><td>betale</td><td>reiser</td><td>og   </td><td>konferanser</td><td>han  </td><td>vil  </td><td>ha  </td><td>regler</td><td>mot  </td><td>dette</td><td>det  </td><td>må   </td><td>ikke </td><td>her  </td><td>##ske</td><td>tvil </td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mot  </td><td>at   </td><td>interesse</td><td>##grupper</td><td>får  </td><td>betale</td><td>reiser</td><td>og   </td><td>konferanser</td><td>han  </td><td>vil  </td><td>ha  </td><td>regler</td><td>mot  </td><td>dette</td><td>det  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.053</td><td>0.007</td><td>0.004</td><td>0.002    </td><td>0.002    </td><td>0.003</td><td>0.003 </td><td>0.002 </td><td>0.002</td><td>0.004      </td><td>0.009</td><td>0.021</td><td>0.01</td><td>0.006 </td><td>0.014</td><td>0.027</td><td>0.101</td><td>0.598</td><td>0.791</td><td>0.878</td><td>0.836</td><td>0.843</td><td>0.688</td><td>0.018</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mot</td><td>at</td><td>interesse</td><td>##grupper</td><td>får</td><td>betale</td><td>reiser</td><td>og</td><td>konferanser</td><td>han</td><td>vil</td><td>ha</td><td>regler</td><td>mot</td><td>dette</td><td>det</td><td>må</td><td>ikke</td><td>her</td><td>##ske</td><td>tvil</td><td>om</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mot</td><td>at</td><td>interesse</td><td>##grupper</td><td>får</td><td>betale</td><td>reiser</td><td>og</td><td>konferanser</td><td>han</td><td>vil</td><td>ha</td><td>regler</td><td>mot</td><td>dette</td><td>det</td><td>må</td><td>ikke</td><td>her</td><td>##ske</td><td>tvil</td><td>om</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0 </td><td>0        </td><td>0        </td><td>0  </td><td>0     </td><td>0     </td><td>0 </td><td>0          </td><td>0  </td><td>0  </td><td>0 </td><td>0     </td><td>0  </td><td>0    </td><td>0  </td><td>0 </td><td>0   </td><td>0  </td><td>0    </td><td>0   </td><td>0 </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['uavhengig'], 'ref_pos': ['ADJ'], 'hyp': ['uavhenget'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['venstre'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['venstresida'], 'ref_pos': ['NOUN'], 'hyp': ['side'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['skyhøye', 'engangsavgifter'], 'ref_pos': ['ADJ', 'NOUN'], 'hyp': ['skyhøge', 'eengangsavgifter'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>uavhengig</td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>##høye</td><td>engangs</td><td>engangs</td><td>##avgifter</td><td>##avgifter</td><td>og  </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uav      </td><td>##heng   </td><td>##et     </td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstre </td><td>side </td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høg </td><td>##e   </td><td>een    </td><td>##gang </td><td>##savgift </td><td>##er      </td><td>og  </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.012 </td><td>0.015  </td><td>0.014</td><td>0.021</td><td>0.035</td><td>0.345    </td><td>0.48     </td><td>0.557    </td><td>0.075</td><td>0.026</td><td>0.028</td><td>0.019</td><td>0.022</td><td>0.032 </td><td>0.051</td><td>0.361   </td><td>0.472</td><td>0.056</td><td>0.041</td><td>0.081</td><td>0.404 </td><td>0.449 </td><td>0.64   </td><td>0.611  </td><td>0.254     </td><td>0.457     </td><td>0.04</td><td>0.037</td><td>0.043</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['engangsavgifter'], 'ref_pos': ['NOUN'], 'hyp': ['eengangsavgifter'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.11\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>engangs</td><td>engangs</td><td>##avgifter</td><td>##avgifter</td><td>og   </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>een    </td><td>##gang </td><td>##savgift </td><td>##er      </td><td>og   </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.002 </td><td>0.002  </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001    </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002 </td><td>0.002</td><td>0.002   </td><td>0.001</td><td>0.002</td><td>0.003</td><td>0.009</td><td>0.015 </td><td>0.592  </td><td>0.588  </td><td>0.243     </td><td>0.433     </td><td>0.016</td><td>0.017</td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['åtte'], 'ref_pos': ['NUM'], 'hyp': ['åtteogsytti'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['og', 'sytti'], 'ref_pos': ['CCONJ', 'NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>og   </td><td>sy  </td><td>##tti</td><td>##tti</td><td>##tti</td><td>##tti</td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>i     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>åtte </td><td>åtte</td><td>åtte </td><td>##ogs</td><td>##ytt</td><td>##i  </td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>##aria</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.083</td><td>0.012</td><td>0.006</td><td>0.004</td><td>0.004</td><td>0.008</td><td>0.009 </td><td>0.024</td><td>0.097</td><td>0.722</td><td>0.53</td><td>0.455</td><td>0.687</td><td>0.598</td><td>0.584</td><td>0.019    </td><td>0.006</td><td>0.004</td><td>0.006</td><td>0.006</td><td>0.006 </td><td>0.01 </td><td>0.009</td><td>0.011</td><td>0.026 </td><td>0.667 </td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0   </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0     </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0    </td><td>0        </td><td>0 </td><td>0 </td><td>0  </td><td>0  </td><td>0     </td><td>0    </td><td>0   </td><td>0  </td><td>0     </td><td>0</td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['avhengig'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'substitute', 'ref': ['av', 'langsomhet', 'ormestad'], 'ref_pos': ['ADP', 'NOUN', 'PROPN'], 'hyp': ['som', 'heter', 'da'], 'hyp_pos': ['PRON', 'VERB', 'ADV']}\n",
      "{'type': 'substitute', 'ref': ['vel', 'av', 'den', 'oppfatning'], 'ref_pos': ['ADV', 'ADP', 'DET', 'NOUN'], 'hyp': ['i', 'har', 'denne', 'oppfatningen'], 'hyp_pos': ['ADP', 'VERB', 'DET', 'NOUN']}\n",
      "ASD-NorBERT ref len: 28, ASD=0.46\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e </td><td>av   </td><td>av      </td><td>av   </td><td>langsom</td><td>##het</td><td>or  </td><td>##mest</td><td>##ad </td><td>##ad </td><td>##ad</td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vel  </td><td>av   </td><td>av   </td><td>den  </td><td>oppfatning  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e </td><td>##e  </td><td>avhengig</td><td>som  </td><td>som    </td><td>som  </td><td>som </td><td>som   </td><td>som  </td><td>heter</td><td>da  </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vi   </td><td>i    </td><td>har  </td><td>denne</td><td>oppfatningen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.06 </td><td>0.045</td><td>0.025</td><td>0.027</td><td>0.019</td><td>0.021      </td><td>0.03</td><td>0.023</td><td>0.041</td><td>0.029  </td><td>0.046</td><td>0.07</td><td>0.573</td><td>0.696   </td><td>0.633</td><td>0.812  </td><td>0.707</td><td>0.77</td><td>0.807 </td><td>0.744</td><td>0.804</td><td>0.8 </td><td>0.086</td><td>0.046</td><td>0.043</td><td>0.139</td><td>0.083</td><td>0.729</td><td>0.634</td><td>0.599</td><td>0.446</td><td>0.229       </td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['av', 'langsomhet', 'ormestad'], 'ref_pos': ['ADP', 'NOUN', 'PROPN'], 'hyp': ['avhengig', 'somhet', 'sta'], 'hyp_pos': ['ADJ', 'NOUN', 'VERB']}\n",
      "{'type': 'delete', 'ref': ['vel'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 28, ASD=0.27\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e  </td><td>av  </td><td>av      </td><td>av    </td><td>langsom</td><td>##het </td><td>or    </td><td>##mest</td><td>##ad </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vel  </td><td>av  </td><td>den </td><td>oppfatning</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e  </td><td>##e </td><td>avhengig</td><td>somhet</td><td>somhet </td><td>somhet</td><td>somhet</td><td>somhet</td><td>sta  </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vi   </td><td>av  </td><td>den </td><td>oppfatning</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.017</td><td>0.015</td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.012      </td><td>0.02</td><td>0.019</td><td>0.048</td><td>0.031  </td><td>0.048</td><td>0.072</td><td>0.56</td><td>0.63    </td><td>0.556 </td><td>0.632  </td><td>0.571 </td><td>0.718 </td><td>0.727 </td><td>0.603</td><td>0.056</td><td>0.033</td><td>0.029</td><td>0.034</td><td>0.038</td><td>0.706</td><td>0.05</td><td>0.04</td><td>0.031     </td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['monika', 'kristensen', 'solås'], 'ref_pos': ['PROPN', 'PROPN', 'PROPN'], 'hyp': ['monica', 'christine', 'sone'], 'hyp_pos': ['PROPN', 'X', 'X']}\n",
      "{'type': 'delete', 'ref': ['da'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at  </td><td>man  </td><td>må   </td><td>stille</td><td>alle </td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ika</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##ås </td><td>##ås</td><td>bråket</td><td>startet</td><td>da     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at  </td><td>man  </td><td>må   </td><td>stille</td><td>alle </td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ica</td><td>christ </td><td>##ine</td><td>##ine</td><td>##ine</td><td>sone</td><td>bråket</td><td>startet</td><td>startet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.11 </td><td>0.01</td><td>0.006</td><td>0.006</td><td>0.005 </td><td>0.003</td><td>0.006</td><td>0.01     </td><td>0.005</td><td>0.005 </td><td>0.004</td><td>0.01   </td><td>0.004</td><td>0.006      </td><td>0.019   </td><td>0.094</td><td>0.383</td><td>0.497  </td><td>0.537</td><td>0.626</td><td>0.631</td><td>0.66</td><td>0.288 </td><td>0.265  </td><td>0.663  </td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['monika'], 'ref_pos': ['PROPN'], 'hyp': ['monica'], 'hyp_pos': ['PROPN']}\n",
      "{'type': 'substitute', 'ref': ['solås'], 'ref_pos': ['PROPN'], 'hyp': ['solus'], 'hyp_pos': ['X']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>man  </td><td>må   </td><td>stille</td><td>alle</td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ika</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##ås </td><td>bråket</td><td>startet</td><td>da   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>man  </td><td>må   </td><td>stille</td><td>alle</td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ica</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##us </td><td>bråket</td><td>startet</td><td>da   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001 </td><td>0.0 </td><td>0.001</td><td>0.002    </td><td>0.001</td><td>0.001 </td><td>0.001</td><td>0.002  </td><td>0.001</td><td>0.001      </td><td>0.009   </td><td>0.073</td><td>0.388</td><td>0.025  </td><td>0.035</td><td>0.088</td><td>0.499</td><td>0.015 </td><td>0.004  </td><td>0.003</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['han', 'seiar'], 'ref_pos': ['PRON', 'VERB'], 'hyp': ['hans', 'eier'], 'hyp_pos': ['PRON', 'VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['om'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['tenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['vil'], 'ref_pos': ['VERB'], 'hyp': ['ville'], 'hyp_pos': ['AUX']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.27\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når </td><td>han  </td><td>han  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk   </td><td>##ingstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når </td><td>hans </td><td>eier </td><td>han  </td><td>han  </td><td>han  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>om   </td><td>tenkning</td><td>##stid   </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>ville</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.017</td><td>0.007</td><td>0.006</td><td>0.005</td><td>0.009  </td><td>0.06</td><td>0.335</td><td>0.765</td><td>0.118</td><td>0.761</td><td>0.673</td><td>0.085</td><td>0.039</td><td>0.028</td><td>0.027</td><td>0.051</td><td>0.025</td><td>0.024   </td><td>0.102</td><td>0.603</td><td>0.51    </td><td>0.307    </td><td>0.021</td><td>0.019     </td><td>0.021</td><td>0.017</td><td>0.192</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['seier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['omtenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ingstid </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seier</td><td>seier</td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ningstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.012</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.004  </td><td>0.012</td><td>0.011</td><td>0.201</td><td>0.443</td><td>0.021</td><td>0.14 </td><td>0.019</td><td>0.019</td><td>0.031</td><td>0.012</td><td>0.014   </td><td>0.028</td><td>0.048</td><td>0.065</td><td>0.245     </td><td>0.018</td><td>0.011     </td><td>0.009</td><td>0.007</td><td>0.01</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['nedla'], 'ref_pos': ['VERB'], 'hyp': ['nedelaaktoratet'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['aktoratet'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>saken</td><td>kom  </td><td>opp  </td><td>for  </td><td>lagmannsretten</td><td>i    </td><td>januar</td><td>ned  </td><td>##la </td><td>aktor</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om   </td><td>seks </td><td>års </td><td>fengsel</td><td>for  </td><td>å    </td><td>å   </td><td>##ke </td><td>gren </td><td>men  </td><td>i    </td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>saken</td><td>kom  </td><td>opp  </td><td>for  </td><td>lagmannsretten</td><td>i    </td><td>januar</td><td>nede </td><td>##la </td><td>##akt</td><td>##ora</td><td>##tet </td><td>påstand</td><td>om   </td><td>seks </td><td>års </td><td>fengsel</td><td>for  </td><td>å    </td><td>å   </td><td>##ke </td><td>gren </td><td>men  </td><td>i    </td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.004</td><td>0.003</td><td>0.004</td><td>0.003</td><td>0.004         </td><td>0.004</td><td>0.008 </td><td>0.313</td><td>0.081</td><td>0.643</td><td>0.644</td><td>0.422 </td><td>0.021  </td><td>0.012</td><td>0.012</td><td>0.01</td><td>0.01   </td><td>0.025</td><td>0.148</td><td>0.13</td><td>0.037</td><td>0.021</td><td>0.017</td><td>0.014</td><td>0.011 </td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 25, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>saken</td><td>kom</td><td>opp</td><td>for</td><td>lagmannsretten</td><td>i</td><td>januar</td><td>ned</td><td>##la</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om</td><td>seks</td><td>års</td><td>fengsel</td><td>for</td><td>å</td><td>##ke</td><td>gren</td><td>men</td><td>i</td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>saken</td><td>kom</td><td>opp</td><td>for</td><td>lagmannsretten</td><td>i</td><td>januar</td><td>ned</td><td>##la</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om</td><td>seks</td><td>års</td><td>fengsel</td><td>for</td><td>å</td><td>##ke</td><td>gren</td><td>men</td><td>i</td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0             </td><td>0</td><td>0     </td><td>0  </td><td>0   </td><td>0    </td><td>0     </td><td>0      </td><td>0 </td><td>0   </td><td>0  </td><td>0      </td><td>0  </td><td>0</td><td>0   </td><td>0   </td><td>0  </td><td>0</td><td>0     </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['gjeruldsen', 'er'], 'ref_pos': ['PROPN', 'AUX'], 'hyp': ['gjeroldsenb', 'heleverden'], 'hyp_pos': ['PROPN', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['selvfølgelig', 'den', 'skjønneste', 'babyen', 'i', 'hele', 'verden', 'helt'], 'ref_pos': ['ADJ', 'DET', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['sett', 'ja'], 'ref_pos': ['VERB', 'INTJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.47\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal </td><td>se  </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld </td><td>##sen</td><td>er   </td><td>selvfølgelig</td><td>den  </td><td>skjønne</td><td>##ste</td><td>babyen</td><td>i    </td><td>hele </td><td>verden  </td><td>helt    </td><td>objektivt</td><td>sett     </td><td>ja       </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal </td><td>se  </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##olds</td><td>##en </td><td>##en </td><td>##en        </td><td>##en </td><td>##en   </td><td>##en </td><td>##b   </td><td>##b  </td><td>hele </td><td>##verden</td><td>##verden</td><td>objektivt</td><td>objektivt</td><td>objektivt</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.097</td><td>0.015</td><td>0.015</td><td>0.026</td><td>0.02</td><td>0.014</td><td>0.014</td><td>0.019</td><td>0.023</td><td>0.089</td><td>0.067</td><td>0.532 </td><td>0.357</td><td>0.613</td><td>0.732       </td><td>0.621</td><td>0.861  </td><td>0.775</td><td>0.749 </td><td>0.703</td><td>0.255</td><td>0.37    </td><td>0.818   </td><td>0.385    </td><td>0.732    </td><td>0.676    </td><td>0.022</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['nesten'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'delete', 'ref': ['selvfølgelig', 'den', 'skjønneste', 'babyen', 'i'], 'ref_pos': ['ADJ', 'DET', 'ADJ', 'NOUN', 'ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['ja'], 'ref_pos': ['INTJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal</td><td>se   </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld</td><td>##sen</td><td>er   </td><td>selvfølgelig</td><td>den   </td><td>skjønne</td><td>##ste </td><td>babyen</td><td>i     </td><td>hele </td><td>verden</td><td>helt </td><td>objektivt</td><td>sett </td><td>ja   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal</td><td>se   </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld</td><td>##sen</td><td>##sen</td><td>nesten      </td><td>nesten</td><td>nesten </td><td>nesten</td><td>nesten</td><td>nesten</td><td>hele </td><td>verden</td><td>helt </td><td>objektivt</td><td>sett </td><td>sett </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.046</td><td>0.011</td><td>0.012</td><td>0.02</td><td>0.017</td><td>0.012</td><td>0.012</td><td>0.013</td><td>0.015</td><td>0.087</td><td>0.025</td><td>0.021</td><td>0.068</td><td>0.662</td><td>0.579       </td><td>0.742 </td><td>0.782  </td><td>0.787 </td><td>0.835 </td><td>0.678 </td><td>0.197</td><td>0.115 </td><td>0.059</td><td>0.055    </td><td>0.081</td><td>0.647</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['politistasjon'], 'ref_pos': ['NOUN'], 'hyp': ['politivstersjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['flyet'], 'ref_pos': ['NOUN'], 'hyp': ['flyene'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ha'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>politistasjon</td><td>politistasjon</td><td>politistasjon</td><td>så   </td><td>skal </td><td>flyet </td><td>ha    </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som  </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politi       </td><td>##vs         </td><td>##ters       </td><td>##jon        </td><td>så   </td><td>skal </td><td>flyene</td><td>flyene</td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som  </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.034</td><td>0.164</td><td>0.195  </td><td>0.52         </td><td>0.732        </td><td>0.694        </td><td>0.587        </td><td>0.056</td><td>0.088</td><td>0.179 </td><td>0.655 </td><td>0.082  </td><td>0.04 </td><td>0.019</td><td>0.034 </td><td>0.034  </td><td>0.016</td><td>0.018</td><td>0.02  </td><td>0.028</td><td>0.014</td><td>0.017</td><td>0.016</td><td>0.016 </td><td>0.013 </td><td>0.014</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['flyet'], 'ref_pos': ['NOUN'], 'hyp': ['fly'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ha'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>så  </td><td>skal </td><td>flyet</td><td>ha   </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>så  </td><td>skal </td><td>fly  </td><td>fly  </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.007</td><td>0.014</td><td>0.015  </td><td>0.013        </td><td>0.02</td><td>0.107</td><td>0.187</td><td>0.572</td><td>0.074  </td><td>0.031</td><td>0.015</td><td>0.015 </td><td>0.016  </td><td>0.01</td><td>0.012</td><td>0.012 </td><td>0.018</td><td>0.009</td><td>0.012</td><td>0.012</td><td>0.011 </td><td>0.008 </td><td>0.009</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['karakterisere'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['karakteriserer'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'substitute', 'ref': ['late'], 'ref_pos': ['ADJ'], 'hyp': ['later'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['om'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['kreativitet'], 'ref_pos': ['NOUN'], 'hyp': ['kreativitetsnok'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['nok'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>karakteriserer</td><td>karakteriserer</td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>og   </td><td>nærmest</td><td>late </td><td>som </td><td>om   </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>kreativitet</td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>karakteris    </td><td>##ere         </td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>og   </td><td>nærmest</td><td>later</td><td>som </td><td>som  </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>##sn       </td><td>##ok       </td><td>##ok </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.038</td><td>0.632</td><td>0.288         </td><td>0.544         </td><td>0.016  </td><td>0.004  </td><td>0.012       </td><td>0.027</td><td>0.011</td><td>0.014</td><td>0.129</td><td>0.187</td><td>0.06   </td><td>0.348</td><td>0.16</td><td>0.568</td><td>0.067</td><td>0.015</td><td>0.036</td><td>0.033</td><td>0.148      </td><td>0.623      </td><td>0.615      </td><td>0.712</td><td>0.079</td><td>0.029</td><td>0.014</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['karakterisere'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['karakteriserer'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['om'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>karakteriserer</td><td>karakteriserer</td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>nærmest</td><td>late </td><td>som  </td><td>om   </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>karakteris    </td><td>##ere         </td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>nærmest</td><td>late </td><td>som  </td><td>som  </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.037</td><td>0.637</td><td>0.279         </td><td>0.533         </td><td>0.014  </td><td>0.002  </td><td>0.007       </td><td>0.017</td><td>0.004</td><td>0.005</td><td>0.011</td><td>0.005  </td><td>0.023</td><td>0.081</td><td>0.544</td><td>0.042</td><td>0.008</td><td>0.007</td><td>0.006</td><td>0.014      </td><td>0.007</td><td>0.006</td><td>0.007</td><td>0.007</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['er', 'blitt', 'et', 'diskusjonene'], 'ref_pos': ['AUX', 'VERB', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['den'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['lurt'], 'ref_pos': ['ADJ'], 'hyp': ['rushan'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['for', 'ham'], 'ref_pos': ['ADP', 'PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['selv'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['har'], 'hyp_pos': ['AUX']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.55\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er     </td><td>blitt  </td><td>et     </td><td>diskusjonen</td><td>##e    </td><td>omkring</td><td>hans </td><td>troverdighet</td><td>hadde</td><td>det </td><td>nok  </td><td>vært </td><td>lurt </td><td>lurt </td><td>lurt </td><td>for  </td><td>ham  </td><td>å    </td><td>gå   </td><td>selv </td><td>om   </td><td>han  </td><td>ikke </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>omkring</td><td>omkring</td><td>omkring</td><td>omkring    </td><td>omkring</td><td>omkring</td><td>hans </td><td>troverdighet</td><td>hadde</td><td>den </td><td>nok  </td><td>vært </td><td>vært </td><td>rush </td><td>##an </td><td>##an </td><td>##an </td><td>å    </td><td>gå   </td><td>gå   </td><td>om   </td><td>han  </td><td>ikke </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.059</td><td>0.518  </td><td>0.761  </td><td>0.738  </td><td>0.678      </td><td>0.675  </td><td>0.312  </td><td>0.063</td><td>0.072       </td><td>0.063</td><td>0.51</td><td>0.064</td><td>0.146</td><td>0.646</td><td>0.676</td><td>0.787</td><td>0.809</td><td>0.787</td><td>0.174</td><td>0.112</td><td>0.826</td><td>0.202</td><td>0.077</td><td>0.136</td><td>0.513</td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['er', 'blitt', 'et', 'diskusjonene'], 'ref_pos': ['AUX', 'VERB', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['ham'], 'ref_pos': ['PRON'], 'hyp': ['han'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'delete', 'ref': ['selv'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['han'], 'ref_pos': ['PRON'], 'hyp': ['man'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.38\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er     </td><td>blitt  </td><td>et     </td><td>diskusjonen</td><td>##e    </td><td>omkring</td><td>hans </td><td>troverdighet</td><td>troverdighet</td><td>hadde</td><td>det  </td><td>nok  </td><td>vært </td><td>lurt </td><td>for  </td><td>ham  </td><td>å    </td><td>gå   </td><td>selv </td><td>om  </td><td>han  </td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>omkring</td><td>omkring</td><td>omkring</td><td>omkring    </td><td>omkring</td><td>omkring</td><td>hans </td><td>troverdighet</td><td>så          </td><td>hadde</td><td>det  </td><td>nok  </td><td>vært </td><td>lurt </td><td>for  </td><td>han  </td><td>å    </td><td>gå   </td><td>gå   </td><td>om  </td><td>man  </td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.521  </td><td>0.762  </td><td>0.754  </td><td>0.669      </td><td>0.678  </td><td>0.28   </td><td>0.056</td><td>0.066       </td><td>0.752       </td><td>0.074</td><td>0.048</td><td>0.039</td><td>0.041</td><td>0.043</td><td>0.063</td><td>0.193</td><td>0.056</td><td>0.048</td><td>0.809</td><td>0.18</td><td>0.481</td><td>0.08</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['friksjon', 'enn'], 'ref_pos': ['NOUN', 'ADP'], 'hyp': ['fleksjon', 'en'], 'hyp_pos': ['NOUN', 'DET']}\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['john', 'gulldahl'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['jon', 'guldal'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.26\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>friksjon</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>vi   </td><td>kjenner</td><td>hjemme</td><td>takk</td><td>skal </td><td>du  </td><td>ha   </td><td>john </td><td>gull </td><td>##dahl</td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>fle     </td><td>##ksjon </td><td>en      </td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>jeg  </td><td>kjenner</td><td>hjemme</td><td>takk</td><td>skal </td><td>du  </td><td>ha   </td><td>jon  </td><td>gul  </td><td>##dal </td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.028</td><td>0.029</td><td>0.025</td><td>0.032</td><td>0.565   </td><td>0.548   </td><td>0.765   </td><td>0.105</td><td>0.049</td><td>0.022</td><td>0.035</td><td>0.287</td><td>0.032  </td><td>0.027 </td><td>0.04</td><td>0.021</td><td>0.03</td><td>0.024</td><td>0.206</td><td>0.436</td><td>0.36  </td><td>0.03     </td><td>0.018</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['john', 'gulldahl'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['jon', 'guldal'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>vi   </td><td>kjenner</td><td>hjemme</td><td>takk </td><td>skal </td><td>du   </td><td>ha   </td><td>john </td><td>gull </td><td>##dahl</td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>jeg  </td><td>kjenner</td><td>hjemme</td><td>takk </td><td>skal </td><td>du   </td><td>ha   </td><td>jon  </td><td>gul  </td><td>##dal </td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.007</td><td>0.004</td><td>0.004</td><td>0.002</td><td>0.004   </td><td>0.003</td><td>0.003</td><td>0.005</td><td>0.006</td><td>0.271</td><td>0.017  </td><td>0.009 </td><td>0.022</td><td>0.012</td><td>0.017</td><td>0.013</td><td>0.222</td><td>0.429</td><td>0.348 </td><td>0.02     </td><td>0.009</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['fordi', 'at'], 'ref_pos': ['SCONJ', 'SCONJ'], 'hyp': ['fred', 'vil'], 'hyp_pos': ['NOUN', 'AUX']}\n",
      "{'type': 'delete', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['variabler', 'som'], 'ref_pos': ['NOUN', 'PRON'], 'hyp': ['variable', 'så'], 'hyp_pos': ['ADJ', 'ADV']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>som     </td><td>var     </td><td>var  </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fred    </td><td>vil  </td><td>vil  </td><td>vil  </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variable </td><td>variable</td><td>variable</td><td>så   </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.048</td><td>0.047 </td><td>0.027</td><td>0.011</td><td>0.015  </td><td>0.016</td><td>0.025</td><td>0.019  </td><td>0.026</td><td>0.103   </td><td>0.428   </td><td>0.553</td><td>0.581</td><td>0.579</td><td>0.123</td><td>0.143</td><td>0.066</td><td>0.029</td><td>0.041</td><td>0.049</td><td>0.354    </td><td>0.631   </td><td>0.676   </td><td>0.707</td><td>0.166</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['at'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['som'], 'ref_pos': ['PRON'], 'hyp': ['svar'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['var', 'mye'], 'ref_pos': ['AUX', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>variabler</td><td>som  </td><td>var  </td><td>mye </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>fordi</td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>svar     </td><td>svar </td><td>svar </td><td>svar</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.078</td><td>0.009 </td><td>0.003</td><td>0.003</td><td>0.004  </td><td>0.004</td><td>0.005</td><td>0.005  </td><td>0.005</td><td>0.009   </td><td>0.041</td><td>0.451</td><td>0.019</td><td>0.019</td><td>0.024</td><td>0.019</td><td>0.015</td><td>0.021</td><td>0.038</td><td>0.148    </td><td>0.638    </td><td>0.656</td><td>0.697</td><td>0.68</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['n', 'r'], 'hyp_pos': ['ADV', 'ADV']}\n",
      "{'type': 'substitute', 'ref': ['nrk'], 'ref_pos': ['PROPN'], 'hyp': ['k'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.14\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hva  </td><td>hun  </td><td>får  </td><td>er   </td><td>hemmelig</td><td>her  </td><td>er   </td><td>er   </td><td>er   </td><td>nrk  </td><td>dags </td><td>##nytt</td><td>klokka</td><td>er  </td><td>tolv </td><td>tretti</td><td>først</td><td>til  </td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha   </td><td>##fs </td><td>##lund</td><td>oslo </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hva  </td><td>hun  </td><td>får  </td><td>er   </td><td>hemmelig</td><td>her  </td><td>er   </td><td>n    </td><td>r    </td><td>k    </td><td>dags </td><td>##nytt</td><td>klokka</td><td>er  </td><td>tolv </td><td>tretti</td><td>først</td><td>til  </td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha   </td><td>##fs </td><td>##lund</td><td>oslo </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.002</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.003   </td><td>0.013</td><td>0.046</td><td>0.722</td><td>0.643</td><td>0.659</td><td>0.054</td><td>0.042 </td><td>0.023 </td><td>0.02</td><td>0.023</td><td>0.024 </td><td>0.014</td><td>0.012</td><td>0.017  </td><td>0.016</td><td>0.015</td><td>0.015      </td><td>0.019</td><td>0.022</td><td>0.02  </td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 26, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hva</td><td>hun</td><td>får</td><td>er</td><td>hemmelig</td><td>her</td><td>er</td><td>nrk</td><td>dags</td><td>##nytt</td><td>klokka</td><td>er</td><td>tolv</td><td>tretti</td><td>først</td><td>til</td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha</td><td>##fs</td><td>##lund</td><td>oslo</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hva</td><td>hun</td><td>får</td><td>er</td><td>hemmelig</td><td>her</td><td>er</td><td>nrk</td><td>dags</td><td>##nytt</td><td>klokka</td><td>er</td><td>tolv</td><td>tretti</td><td>først</td><td>til</td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha</td><td>##fs</td><td>##lund</td><td>oslo</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0 </td><td>0       </td><td>0  </td><td>0 </td><td>0  </td><td>0   </td><td>0     </td><td>0     </td><td>0 </td><td>0   </td><td>0     </td><td>0    </td><td>0  </td><td>0      </td><td>0    </td><td>0    </td><td>0          </td><td>0 </td><td>0   </td><td>0     </td><td>0   </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved', 'politioverbetjent'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['politijobben', 'arnesen'], 'hyp_pos': ['NOUN', 'PROPN']}\n",
      "{'type': 'delete', 'ref': ['arne', 'skogen'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>##betjent</td><td>ar   </td><td>##ne   </td><td>skogen </td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>politi   </td><td>##jobben </td><td>ar   </td><td>##nesen</td><td>##nesen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.007         </td><td>0.007      </td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.006</td><td>0.008</td><td>0.012</td><td>0.018</td><td>0.021  </td><td>0.816  </td><td>0.712  </td><td>0.173</td><td>0.084</td><td>0.04    </td><td>0.089</td><td>0.761</td><td>0.12  </td><td>0.679 </td><td>0.531    </td><td>0.619    </td><td>0.238</td><td>0.628  </td><td>0.563  </td><td>0.085</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved'], 'ref_pos': ['ADP'], 'hyp': ['politibetjent'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['politioverbetjent'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du  </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne </td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du  </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>##betjent</td><td>ar   </td><td>##ne </td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.004         </td><td>0.004      </td><td>0.005</td><td>0.006</td><td>0.006</td><td>0.004</td><td>0.007</td><td>0.01</td><td>0.016</td><td>0.018  </td><td>0.822  </td><td>0.71   </td><td>0.169</td><td>0.086</td><td>0.047   </td><td>0.096</td><td>0.696</td><td>0.053 </td><td>0.575 </td><td>0.091    </td><td>0.037</td><td>0.041</td><td>0.038 </td><td>0.032</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['felleseiga', 'selskap'], 'ref_pos': ['ADJ', 'NOUN'], 'hyp': ['felles', 'eierselskap'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['driva', 'kanalane'], 'ref_pos': ['VERB', 'NOUN'], 'hyp': ['drive', 'kanalene'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['mekler', 'selskapet'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['meklarselskapet', 'first', 'securities'], 'ref_pos': ['NOUN', 'ADJ', 'X'], 'hyp': ['først', 'i', 'kuritis'], 'hyp_pos': ['ADV', 'ADP', 'NOUN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.46\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>##eig</td><td>##a  </td><td>selskap </td><td>skal </td><td>driv </td><td>##a  </td><td>kanal   </td><td>##ane   </td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##lar</td><td>##selskapet</td><td>first</td><td>first</td><td>first</td><td>sec  </td><td>sec  </td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>eiers</td><td>eiers</td><td>##elskap</td><td>skal </td><td>drive</td><td>drive</td><td>kanalene</td><td>kanalene</td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##ler</td><td>selskapet  </td><td>først</td><td>i    </td><td>kur  </td><td>kur  </td><td>##iti</td><td>##s  </td><td>##s      </td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.043</td><td>0.089</td><td>0.081 </td><td>0.523</td><td>0.821</td><td>0.251   </td><td>0.042</td><td>0.169</td><td>0.601</td><td>0.336   </td><td>0.596   </td><td>0.036 </td><td>0.032</td><td>0.037</td><td>0.136</td><td>0.057</td><td>0.04       </td><td>0.133</td><td>0.097</td><td>0.316</td><td>0.276      </td><td>0.606</td><td>0.713</td><td>0.718</td><td>0.748</td><td>0.75 </td><td>0.737</td><td>0.741    </td><td>0.076</td><td>0.033      </td><td>0.051</td><td>0.072</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['et', 'felleseiga', 'selskap'], 'ref_pos': ['DET', 'ADJ', 'NOUN'], 'hyp': ['eit', 'felles', 'eierselskap'], 'hyp_pos': ['DET', 'ADJ', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['driva', 'kanalane'], 'ref_pos': ['VERB', 'NOUN'], 'hyp': ['drive', 'kanalene'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['mekler'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['meklarselskapet', 'first'], 'ref_pos': ['NOUN', 'ADJ'], 'hyp': ['selskapet', 'først'], 'hyp_pos': ['NOUN', 'ADV']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.33\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>##eig</td><td>##a  </td><td>selskap </td><td>skal </td><td>driv </td><td>##a  </td><td>kanal   </td><td>##ane   </td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##lar</td><td>##selskapet</td><td>first</td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>eit  </td><td>felles</td><td>eiers</td><td>eiers</td><td>##elskap</td><td>skal </td><td>drive</td><td>drive</td><td>kanalene</td><td>kanalene</td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##ler</td><td>selskapet  </td><td>først</td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.252</td><td>0.082 </td><td>0.529</td><td>0.826</td><td>0.236   </td><td>0.036</td><td>0.183</td><td>0.615</td><td>0.422   </td><td>0.603   </td><td>0.033 </td><td>0.027</td><td>0.029</td><td>0.049</td><td>0.037</td><td>0.036      </td><td>0.126</td><td>0.092</td><td>0.307</td><td>0.255      </td><td>0.566</td><td>0.163</td><td>0.085    </td><td>0.048</td><td>0.039      </td><td>0.055</td><td>0.071</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['konflikter'], 'ref_pos': ['NOUN'], 'hyp': ['konflikten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['borgerkrig'], 'ref_pos': ['NOUN'], 'hyp': ['borgekrig'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>borgerkrig</td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikten</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borg      </td><td>##ek      </td><td>##rig     </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.006   </td><td>0.005</td><td>0.003</td><td>0.003</td><td>0.003</td><td>0.004   </td><td>0.025</td><td>0.158     </td><td>0.039     </td><td>0.021</td><td>0.037</td><td>0.007</td><td>0.022 </td><td>0.78    </td><td>0.1</td><td>0.025</td><td>0.018</td><td>0.014     </td><td>0.015</td><td>0.028</td><td>0.015</td><td>0.052</td><td>0.549     </td><td>0.743     </td><td>0.546     </td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.05\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.002</td><td>0.001   </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.001   </td><td>0.001</td><td>0.003     </td><td>0.003     </td><td>0.003</td><td>0.03 </td><td>0.005</td><td>0.02  </td><td>0.781   </td><td>0.097</td><td>0.022</td><td>0.016</td><td>0.01      </td><td>0.011</td><td>0.008</td><td>0.008</td><td>0.008</td><td>0.01      </td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['kjent'], 'ref_pos': ['ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['e', 'ø'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['eøsområdet'], 'ref_pos': ['NOUN'], 'hyp': ['sområdet'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.21\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kjent</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e   </td><td>##øs </td><td>##øs </td><td>##området</td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e   </td><td>ø    </td><td>som  </td><td>##rådet  </td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.036</td><td>0.709</td><td>0.174</td><td>0.018  </td><td>0.019</td><td>0.012   </td><td>0.016</td><td>0.013</td><td>0.013  </td><td>0.015   </td><td>0.023    </td><td>0.049</td><td>0.029</td><td>0.028</td><td>0.033</td><td>0.081</td><td>0.14</td><td>0.579</td><td>0.725</td><td>0.56     </td><td>0.047</td><td>0.027</td><td>0.019</td><td>0.011</td><td>0.013</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['kjent'], 'ref_pos': ['ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['eøs'], 'hyp_pos': ['PROPN']}\n",
      "{'type': 'substitute', 'ref': ['eøsområdet'], 'ref_pos': ['NOUN'], 'hyp': ['området'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kjent</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e    </td><td>##øs </td><td>##området</td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e    </td><td>##øs </td><td>området  </td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.034</td><td>0.703</td><td>0.173</td><td>0.014  </td><td>0.015</td><td>0.009   </td><td>0.012</td><td>0.01 </td><td>0.01   </td><td>0.012   </td><td>0.015    </td><td>0.019</td><td>0.013</td><td>0.012</td><td>0.009</td><td>0.017</td><td>0.032</td><td>0.097</td><td>0.175    </td><td>0.009</td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['martha', 'louis'], 'hyp_pos': ['X', 'X']}\n",
      "{'type': 'substitute', 'ref': ['behn'], 'ref_pos': ['PROPN'], 'hyp': ['ben'], 'hyp_pos': ['X']}\n",
      "{'type': 'delete', 'ref': ['har'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 29, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og  </td><td>ar   </td><td>##i  </td><td>beh </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>og  </td><td>ar   </td><td>##i  </td><td>##i </td><td>##i  </td><td>ben  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.041</td><td>0.008</td><td>0.004</td><td>0.003     </td><td>0.002</td><td>0.003 </td><td>0.002 </td><td>0.004</td><td>0.005</td><td>0.002</td><td>0.002</td><td>0.003 </td><td>0.002</td><td>0.003</td><td>0.006</td><td>0.004  </td><td>0.031</td><td>0.037    </td><td>0.608</td><td>0.563</td><td>0.081</td><td>0.121</td><td>0.579</td><td>0.03</td><td>0.035</td><td>0.066</td><td>0.72</td><td>0.504</td><td>0.578</td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['märtha'], 'ref_pos': ['PROPN'], 'hyp': ['martha'], 'hyp_pos': ['PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.001     </td><td>0.0</td><td>0.001 </td><td>0.001 </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.0   </td><td>0.0</td><td>0.001</td><td>0.002</td><td>0.001  </td><td>0.002</td><td>0.025    </td><td>0.605</td><td>0.556</td><td>0.048</td><td>0.028</td><td>0.026</td><td>0.009</td><td>0.014</td><td>0.018</td><td>0.012</td><td>0.015</td><td>0.005</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 20\n",
    "\n",
    "for row in lower_asd_df.itertuples():\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['jo'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['ut'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['utfor'], 'ref_pos': ['ADP'], 'hyp': ['før'], 'hyp_pos': ['SCONJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['kva'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['hva', 'altså'], 'ref_pos': ['PRON', 'ADV'], 'hyp': ['som', 'skjedd'], 'hyp_pos': ['PRON', 'VERB']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>jo   </td><td>sånn </td><td>rett</td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>utfor</td><td>utfor</td><td>auto </td><td>##vernet</td><td>de   </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klart</td><td>klart</td><td>så   </td><td>hva  </td><td>hva  </td><td>altså </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>sånn </td><td>rett</td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>ut   </td><td>før  </td><td>auto </td><td>##vernet</td><td>de   </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klart</td><td>og   </td><td>så   </td><td>kva  </td><td>som  </td><td>skjedd</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.049</td><td>0.755</td><td>0.169</td><td>0.03</td><td>0.031</td><td>0.026</td><td>0.021 </td><td>0.035</td><td>0.567</td><td>0.605</td><td>0.029</td><td>0.039   </td><td>0.032</td><td>0.01 </td><td>0.026</td><td>0.017</td><td>0.011</td><td>0.011</td><td>0.049</td><td>0.108</td><td>0.715</td><td>0.205</td><td>0.322</td><td>0.693</td><td>0.704 </td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['jo'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['utfor', 'autovernet', 'de'], 'ref_pos': ['ADP', 'NOUN', 'DET'], 'hyp': ['utføre', 'ut', 'overnetde'], 'hyp_pos': ['VERB', 'ADP', 'ADP']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['klarte'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['klart'], 'ref_pos': ['ADJ'], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['hva'], 'ref_pos': ['PRON'], 'hyp': ['kvasse'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'delete', 'ref': ['altså'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 23, ASD=0.55\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>jo   </td><td>sånn </td><td>rett </td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>utfor </td><td>utfor</td><td>utfor</td><td>auto </td><td>##vernet</td><td>##vernet</td><td>de   </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klart </td><td>klart</td><td>så   </td><td>hva </td><td>hva  </td><td>altså</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>sånn </td><td>sånn </td><td>rett </td><td>ned  </td><td>fem  </td><td>hundre</td><td>meter</td><td>utføre</td><td>ut   </td><td>overn</td><td>overn</td><td>overn   </td><td>##et    </td><td>##de </td><td>andre</td><td>kom  </td><td>rett </td><td>etter</td><td>meg  </td><td>og   </td><td>klarte</td><td>å    </td><td>så   </td><td>kva </td><td>##sse</td><td>##sse</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.091</td><td>0.457</td><td>0.184</td><td>0.037</td><td>0.041</td><td>0.026</td><td>0.033 </td><td>0.066</td><td>0.555 </td><td>0.628</td><td>0.706</td><td>0.73 </td><td>0.691   </td><td>0.711   </td><td>0.517</td><td>0.118</td><td>0.031</td><td>0.022</td><td>0.015</td><td>0.018</td><td>0.094</td><td>0.513 </td><td>0.743</td><td>0.275</td><td>0.53</td><td>0.747</td><td>0.753</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['martha', 'louis'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.10\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til</td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da </td><td>har</td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å  </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og  </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til</td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da </td><td>har</td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å  </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>og  </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.0</td><td>0.002</td><td>0.001     </td><td>0.0</td><td>0.001 </td><td>0.001 </td><td>0.0</td><td>0.0</td><td>0.001</td><td>0.0</td><td>0.0   </td><td>0.0</td><td>0.0</td><td>0.002</td><td>0.001  </td><td>0.002</td><td>0.026    </td><td>0.602</td><td>0.56 </td><td>0.074</td><td>0.108</td><td>0.569</td><td>0.01</td><td>0.008</td><td>0.013</td><td>0.005</td><td>0.006</td><td>0.001</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise', 'og', 'ari', 'behn'], 'ref_pos': ['PROPN', 'PROPN', 'CCONJ', 'PROPN', 'PROPN'], 'hyp': ['martha', 'louis', 'var', 'i', 'bena'], 'hyp_pos': ['PROPN', 'X', 'AUX', 'ADP', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['har'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 29, ASD=0.31\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>har  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>##uis</td><td>var  </td><td>i    </td><td>bena </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.011</td><td>0.006</td><td>0.004     </td><td>0.004</td><td>0.005 </td><td>0.005 </td><td>0.008</td><td>0.015</td><td>0.004</td><td>0.004</td><td>0.005 </td><td>0.003</td><td>0.004</td><td>0.008</td><td>0.006  </td><td>0.024</td><td>0.04     </td><td>0.598</td><td>0.566</td><td>0.109</td><td>0.151</td><td>0.587</td><td>0.753</td><td>0.8  </td><td>0.658</td><td>0.76 </td><td>0.678</td><td>0.484</td><td>0.534</td><td>0.566</td><td>0.016</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['ryddig'], 'ref_pos': ['ADJ'], 'hyp': ['rødde'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>sku  </td><td>##ta </td><td>til  </td><td>havn </td><td>på   </td><td>en   </td><td>ryddig</td><td>ryddig</td><td>ryddig</td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>sku  </td><td>##ta </td><td>til  </td><td>havn </td><td>på   </td><td>en   </td><td>en    </td><td>rød   </td><td>##de  </td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.003</td><td>0.003</td><td>0.002</td><td>0.001</td><td>0.002</td><td>0.003</td><td>0.004</td><td>0.005</td><td>0.007</td><td>0.005</td><td>0.005</td><td>0.008</td><td>0.007</td><td>0.006</td><td>0.003</td><td>0.005</td><td>0.012</td><td>0.033</td><td>0.058</td><td>0.659 </td><td>0.718 </td><td>0.8   </td><td>0.073</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['skuta'], 'ref_pos': ['NOUN'], 'hyp': ['skutte'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'delete', 'ref': ['til'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ryddig'], 'ref_pos': ['ADJ'], 'hyp': ['rødde'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>sku  </td><td>##ta </td><td>til  </td><td>havn </td><td>på   </td><td>en   </td><td>ryddig</td><td>ryddig</td><td>ryddig</td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ser  </td><td>han  </td><td>frem </td><td>til  </td><td>å    </td><td>gi   </td><td>seg  </td><td>jeg  </td><td>skal </td><td>være </td><td>med  </td><td>å    </td><td>ta   </td><td>skutt</td><td>##e  </td><td>##e  </td><td>havn </td><td>på   </td><td>en   </td><td>en    </td><td>rød   </td><td>##de  </td><td>måte </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.013</td><td>0.006</td><td>0.008</td><td>0.005</td><td>0.003</td><td>0.005</td><td>0.006</td><td>0.008</td><td>0.013</td><td>0.011</td><td>0.012</td><td>0.014</td><td>0.023</td><td>0.069</td><td>0.598</td><td>0.636</td><td>0.621</td><td>0.204</td><td>0.053</td><td>0.073</td><td>0.667 </td><td>0.709 </td><td>0.788 </td><td>0.075</td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['DET'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['telegrambyrået'], 'ref_pos': ['NOUN'], 'hyp': ['telegrambyrå'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.04\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hvor</td><td>mange</td><td>som</td><td>er </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>##gram</td><td>##byrået</td><td>ir   </td><td>##na</td><td>så   </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hvor</td><td>mange</td><td>som</td><td>er </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>##gram</td><td>##byrå  </td><td>ir   </td><td>##na</td><td>så   </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.0 </td><td>0.0  </td><td>0.0</td><td>0.0</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002  </td><td>0.01  </td><td>0.111</td><td>0.106</td><td>0.019  </td><td>0.02</td><td>0.026 </td><td>0.186   </td><td>0.037</td><td>0.02</td><td>0.011</td><td>0.007</td><td>0.006</td><td>0.006</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['DET'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['telegrambyrået'], 'ref_pos': ['NOUN'], 'hyp': ['telegranbyirna'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['irna'], 'ref_pos': ['PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hvor </td><td>mange</td><td>som  </td><td>er   </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>##gram</td><td>##gram</td><td>##gram</td><td>##byrået</td><td>ir   </td><td>##na </td><td>så  </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hvor </td><td>mange</td><td>som  </td><td>er   </td><td>drept</td><td>vet  </td><td>vi   </td><td>ennå </td><td>ikke </td><td>konkret</td><td>ifølge</td><td>det  </td><td>det  </td><td>iranske</td><td>tele</td><td>tele  </td><td>##gran</td><td>##by  </td><td>##ir    </td><td>##ir </td><td>##na </td><td>så  </td><td>er   </td><td>det  </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.004</td><td>0.001</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.008</td><td>0.002</td><td>0.004</td><td>0.002</td><td>0.002</td><td>0.003  </td><td>0.012 </td><td>0.101</td><td>0.135</td><td>0.028  </td><td>0.11</td><td>0.494 </td><td>0.688 </td><td>0.716 </td><td>0.602   </td><td>0.624</td><td>0.262</td><td>0.02</td><td>0.011</td><td>0.009</td><td>0.008</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['høystbydende'], 'ref_pos': ['ADJ'], 'hyp': ['høydsbydene'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.16\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ett  </td><td>om   </td><td>gangen</td><td>mens </td><td>det  </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er   </td><td>for  </td><td>salg </td><td>til  </td><td>høyst</td><td>høyst</td><td>##by </td><td>##dende</td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ett  </td><td>om   </td><td>gangen</td><td>mens </td><td>det  </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er   </td><td>for  </td><td>salg </td><td>til  </td><td>høy  </td><td>##ds </td><td>##by </td><td>##dene </td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.004</td><td>0.002</td><td>0.002 </td><td>0.002</td><td>0.003</td><td>0.003</td><td>0.005</td><td>0.002</td><td>0.003</td><td>0.002</td><td>0.004</td><td>0.009</td><td>0.011</td><td>0.015</td><td>0.019</td><td>0.039</td><td>0.126</td><td>0.547</td><td>0.734</td><td>0.214</td><td>0.547  </td><td>0.026</td><td>0.016</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['om'], 'ref_pos': ['ADP'], 'hyp': ['omgangen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['gangen'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['de'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['høystbydende'], 'ref_pos': ['ADJ'], 'hyp': ['høydsbydene'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.32\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ett </td><td>ett     </td><td>om      </td><td>gangen  </td><td>mens </td><td>det  </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er  </td><td>for  </td><td>salg </td><td>til  </td><td>høyst</td><td>høyst</td><td>##by </td><td>##dende</td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ett </td><td>omgangen</td><td>omgangen</td><td>omgangen</td><td>mens </td><td>de   </td><td>det  </td><td>de   </td><td>nå   </td><td>har  </td><td>er   </td><td>altså</td><td>at   </td><td>de   </td><td>er  </td><td>for  </td><td>salg </td><td>til  </td><td>høy  </td><td>##ds </td><td>##by </td><td>##dene </td><td>og   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.12</td><td>0.675   </td><td>0.68    </td><td>0.686   </td><td>0.102</td><td>0.449</td><td>0.084</td><td>0.024</td><td>0.014</td><td>0.016</td><td>0.019</td><td>0.02 </td><td>0.017</td><td>0.021</td><td>0.02</td><td>0.026</td><td>0.046</td><td>0.131</td><td>0.545</td><td>0.729</td><td>0.214</td><td>0.545  </td><td>0.022</td><td>0.015</td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regjering'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['i'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['innsigelsesregime'], 'ref_pos': ['NOUN'], 'hyp': ['innsigelseregimet'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['blant'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.35\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>[CLS]    </td><td>da   </td><td>vi   </td><td>var  </td><td>en   </td><td>del  </td><td>av  </td><td>regjeringen</td><td>regjeringen</td><td>snudde</td><td>vi   </td><td>et   </td><td>innsigelse </td><td>##sreg</td><td>##ime</td><td>##ime</td><td>som  </td><td>var  </td><td>bygd </td><td>opp </td><td>gjennom</td><td>flere</td><td>år  </td><td>med  </td><td>blant</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>regjering</td><td>da   </td><td>da   </td><td>i    </td><td>en   </td><td>del  </td><td>av  </td><td>regjeringen</td><td>så         </td><td>snudde</td><td>vi   </td><td>et   </td><td>innsigelser</td><td>##eg  </td><td>##ime</td><td>##t  </td><td>som  </td><td>var  </td><td>bygd </td><td>opp </td><td>gjennom</td><td>flere</td><td>år  </td><td>med  </td><td>med  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.082</td><td>0.755    </td><td>0.228</td><td>0.657</td><td>0.592</td><td>0.197</td><td>0.139</td><td>0.09</td><td>0.067      </td><td>0.719      </td><td>0.05  </td><td>0.053</td><td>0.058</td><td>0.144      </td><td>0.495 </td><td>0.149</td><td>0.607</td><td>0.028</td><td>0.022</td><td>0.023</td><td>0.02</td><td>0.015  </td><td>0.022</td><td>0.02</td><td>0.099</td><td>0.646</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regjering'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['innsigelsesregime'], 'ref_pos': ['NOUN'], 'hyp': ['insigelseregimet'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': ['har'], 'hyp_pos': ['AUX']}\n",
      "{'type': 'delete', 'ref': ['blant'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.51\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>[CLS]    </td><td>da   </td><td>vi   </td><td>var  </td><td>en   </td><td>del  </td><td>av   </td><td>regjeringen</td><td>regjeringen</td><td>snudde</td><td>vi   </td><td>et   </td><td>innsigelse</td><td>##sreg</td><td>##sreg</td><td>##sreg</td><td>##sreg</td><td>##ime</td><td>som  </td><td>var  </td><td>bygd </td><td>opp  </td><td>gjennom</td><td>flere</td><td>år   </td><td>med  </td><td>blant</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>regjering</td><td>da   </td><td>vi   </td><td>vi   </td><td>en   </td><td>del  </td><td>av   </td><td>regjeringen</td><td>så         </td><td>snudde</td><td>vi   </td><td>et   </td><td>ins       </td><td>##ige </td><td>##ls  </td><td>##ere </td><td>##gi  </td><td>##met</td><td>som  </td><td>har  </td><td>bygd </td><td>opp  </td><td>gjennom</td><td>flere</td><td>år   </td><td>med  </td><td>med  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.084</td><td>0.756    </td><td>0.135</td><td>0.078</td><td>0.611</td><td>0.119</td><td>0.073</td><td>0.049</td><td>0.06       </td><td>0.72       </td><td>0.074 </td><td>0.044</td><td>0.094</td><td>0.657     </td><td>0.728 </td><td>0.731 </td><td>0.633 </td><td>0.669 </td><td>0.579</td><td>0.081</td><td>0.446</td><td>0.127</td><td>0.078</td><td>0.041  </td><td>0.043</td><td>0.046</td><td>0.122</td><td>0.658</td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['saddam'], 'ref_pos': ['NOUN'], 'hyp': ['sadam'], 'hyp_pos': ['X']}\n",
      "{'type': 'substitute', 'ref': ['nitten'], 'ref_pos': ['NUM'], 'hyp': ['nittenåttito'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['åttito'], 'ref_pos': ['NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 31, ASD=0.14\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten</td><td>##ta </td><td>##tfor</td><td>##søk</td><td>mot </td><td>sad  </td><td>##dam</td><td>i    </td><td>nit  </td><td>##ten</td><td>åt   </td><td>##tit</td><td>##tit</td><td>##o  </td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten</td><td>##ta </td><td>##tfor</td><td>##søk</td><td>mot </td><td>sad  </td><td>##am </td><td>i    </td><td>nit  </td><td>##ten</td><td>##ten</td><td>##ten</td><td>##ått</td><td>##ito</td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.002    </td><td>0.003</td><td>0.003</td><td>0.003 </td><td>0.003</td><td>0.01</td><td>0.054</td><td>0.383</td><td>0.031</td><td>0.055</td><td>0.117</td><td>0.524</td><td>0.508</td><td>0.614</td><td>0.475</td><td>0.052</td><td>0.015</td><td>0.014  </td><td>0.011</td><td>0.009 </td><td>0.012</td><td>0.01</td><td>0.009</td><td>0.015</td><td>0.009  </td><td>0.009</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['attentatforsøk'], 'ref_pos': ['NOUN'], 'hyp': ['atentatforsøk'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['saddam'], 'ref_pos': ['NOUN'], 'hyp': ['sadam'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['nitten'], 'ref_pos': ['NUM'], 'hyp': ['nittenåttito'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['åttito'], 'ref_pos': ['NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 31, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten    </td><td>atten</td><td>##ta </td><td>##ta </td><td>##tfor</td><td>##søk   </td><td>mot  </td><td>sad  </td><td>##dam</td><td>i    </td><td>nit  </td><td>##ten</td><td>åt   </td><td>##tit</td><td>##tit</td><td>##o  </td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>mislykket</td><td>ate  </td><td>##nt </td><td>##at </td><td>##at  </td><td>##forsøk</td><td>mot  </td><td>sad  </td><td>##am </td><td>i    </td><td>nit  </td><td>##ten</td><td>##ten</td><td>##ten</td><td>##ått</td><td>##ito</td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.004</td><td>0.006</td><td>0.007</td><td>0.012</td><td>0.025    </td><td>0.626    </td><td>0.64 </td><td>0.621</td><td>0.533</td><td>0.645 </td><td>0.349   </td><td>0.042</td><td>0.094</td><td>0.419</td><td>0.037</td><td>0.059</td><td>0.117</td><td>0.522</td><td>0.509</td><td>0.615</td><td>0.477</td><td>0.053</td><td>0.016</td><td>0.016  </td><td>0.011</td><td>0.01  </td><td>0.012</td><td>0.01</td><td>0.008</td><td>0.014</td><td>0.011  </td><td>0.009</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.04\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i  </td><td>tjenestetilbud</td><td>##et</td><td>til</td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>eiendomsskatt</td><td>##en </td><td>##en </td><td>er  </td><td>det  </td><td>helt </td><td>feil</td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i  </td><td>tjenestetilbud</td><td>##et</td><td>til</td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>eiendomsskatt</td><td>##en </td><td>så   </td><td>er  </td><td>det  </td><td>helt </td><td>feil</td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.002</td><td>0.001</td><td>0.001  </td><td>0.001</td><td>0.0  </td><td>0.0</td><td>0.0           </td><td>0.0 </td><td>0.0</td><td>0.001      </td><td>0.004 </td><td>0.002</td><td>0.003  </td><td>0.001        </td><td>0.008</td><td>0.673</td><td>0.07</td><td>0.022</td><td>0.011</td><td>0.01</td><td>0.01     </td><td>0.008</td><td>0.013</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regendomsskatten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['eiendomsskatten'], 'ref_pos': ['NOUN'], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i    </td><td>tjenestetilbud</td><td>##et </td><td>til  </td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>fjerner</td><td>fjerner</td><td>fjerner</td><td>eiendomsskatt</td><td>##en </td><td>##en </td><td>er   </td><td>det  </td><td>helt </td><td>feil </td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>man  </td><td>må   </td><td>begynne</td><td>å    </td><td>kutte</td><td>i    </td><td>tjenestetilbud</td><td>##et </td><td>til  </td><td>innbyggerne</td><td>dersom</td><td>man  </td><td>fjerner</td><td>reg    </td><td>##endom</td><td>##ss   </td><td>##katt       </td><td>##en </td><td>så   </td><td>er   </td><td>det  </td><td>helt </td><td>feil </td><td>president</td><td>for  </td><td>svært</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.004</td><td>0.002</td><td>0.002  </td><td>0.003</td><td>0.006</td><td>0.003</td><td>0.008         </td><td>0.004</td><td>0.004</td><td>0.017      </td><td>0.008 </td><td>0.006</td><td>0.019  </td><td>0.715  </td><td>0.747  </td><td>0.778  </td><td>0.279        </td><td>0.078</td><td>0.682</td><td>0.087</td><td>0.039</td><td>0.032</td><td>0.029</td><td>0.04     </td><td>0.025</td><td>0.034</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>det</td><td>tar</td><td>åtte</td><td>dager</td><td>å</td><td>komme</td><td>inn</td><td>med</td><td>båt</td><td>og</td><td>to</td><td>dagers</td><td>kjøring</td><td>og</td><td>med</td><td>en</td><td>fly</td><td>##stri</td><td>##pe</td><td>så</td><td>er</td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>det</td><td>tar</td><td>åtte</td><td>dager</td><td>å</td><td>komme</td><td>inn</td><td>med</td><td>båt</td><td>og</td><td>to</td><td>dagers</td><td>kjøring</td><td>og</td><td>med</td><td>en</td><td>fly</td><td>##stri</td><td>##pe</td><td>så</td><td>er</td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0  </td><td>0   </td><td>0    </td><td>0</td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0 </td><td>0 </td><td>0     </td><td>0      </td><td>0 </td><td>0  </td><td>0 </td><td>0  </td><td>0     </td><td>0   </td><td>0 </td><td>0 </td><td>0    </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['med', 'en'], 'ref_pos': ['ADP', 'DET'], 'hyp': ['men', 'flystripet'], 'hyp_pos': ['CCONJ', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['flystripe'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>det  </td><td>tar  </td><td>åtte </td><td>dager</td><td>å    </td><td>komme</td><td>inn  </td><td>med  </td><td>båt  </td><td>og   </td><td>to   </td><td>dagers</td><td>kjøring</td><td>og   </td><td>og   </td><td>med  </td><td>en   </td><td>fly  </td><td>##stri</td><td>##pe </td><td>så   </td><td>er  </td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>det  </td><td>tar  </td><td>åtte </td><td>dager</td><td>å    </td><td>komme</td><td>inn  </td><td>med  </td><td>båt  </td><td>og   </td><td>to   </td><td>dagers</td><td>kjøring</td><td>og   </td><td>men  </td><td>men  </td><td>men  </td><td>fly  </td><td>##stri</td><td>##pet</td><td>så   </td><td>er  </td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.007</td><td>0.006</td><td>0.004</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.005</td><td>0.002</td><td>0.006</td><td>0.004</td><td>0.005 </td><td>0.006  </td><td>0.096</td><td>0.555</td><td>0.583</td><td>0.667</td><td>0.061</td><td>0.075 </td><td>0.328</td><td>0.036</td><td>0.03</td><td>0.013</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['seier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['omtenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ingstid </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seier</td><td>seier</td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ningstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.012</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.004  </td><td>0.012</td><td>0.011</td><td>0.201</td><td>0.443</td><td>0.021</td><td>0.14 </td><td>0.019</td><td>0.019</td><td>0.031</td><td>0.012</td><td>0.014   </td><td>0.028</td><td>0.048</td><td>0.065</td><td>0.245     </td><td>0.018</td><td>0.011     </td><td>0.009</td><td>0.007</td><td>0.01</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['sier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['om'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['tenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.22\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk   </td><td>##ingstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikke </td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>sier </td><td>sier </td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den </td><td>domfelte</td><td>tok  </td><td>om   </td><td>tenkning</td><td>##stid   </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.132</td><td>0.012</td><td>0.008</td><td>0.006</td><td>0.012  </td><td>0.021</td><td>0.042</td><td>0.309</td><td>0.481</td><td>0.049</td><td>0.138</td><td>0.068</td><td>0.068</td><td>0.179</td><td>0.04</td><td>0.028   </td><td>0.098</td><td>0.591</td><td>0.501   </td><td>0.29     </td><td>0.022</td><td>0.021     </td><td>0.019</td><td>0.029</td><td>0.021</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['ryggraden'], 'ref_pos': ['NOUN'], 'hyp': ['ryggrad'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['bestående', 'av', 'regjeringspartiene'], 'ref_pos': ['ADJ', 'ADP', 'NOUN'], 'hyp': ['bestå', 'stående', 'regjeringspartiet'], 'hyp_pos': ['VERB', 'ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.15\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##raden</td><td>i    </td><td>praksis</td><td>så   </td><td>er   </td><td>ikke </td><td>dagens</td><td>flertall</td><td>bestående</td><td>av     </td><td>regjerings</td><td>##partiene</td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr   </td><td>##p  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##rad  </td><td>i    </td><td>praksis</td><td>så   </td><td>er   </td><td>ikke </td><td>dagens</td><td>flertall</td><td>bestå    </td><td>stående</td><td>regjerings</td><td>##partiet </td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr   </td><td>##p  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.005</td><td>0.007</td><td>0.015</td><td>0.039</td><td>0.022  </td><td>0.012     </td><td>0.039</td><td>0.269  </td><td>0.016</td><td>0.007  </td><td>0.002</td><td>0.017</td><td>0.009</td><td>0.015 </td><td>0.046   </td><td>0.435    </td><td>0.587  </td><td>0.052     </td><td>0.257     </td><td>0.038</td><td>0.014 </td><td>0.012    </td><td>0.006</td><td>0.005</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['ryggrad'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['ryggraden'], 'ref_pos': ['NOUN'], 'hyp': ['da'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['erke'], 'hyp_pos': ['AUX']}\n",
      "{'type': 'delete', 'ref': ['ikke'], 'ref_pos': ['PART'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['bestående', 'av', 'regjeringspartiene'], 'ref_pos': ['ADJ', 'ADP', 'NOUN'], 'hyp': ['bestå', 'stående', 'regjeringspartiet'], 'hyp_pos': ['VERB', 'ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##raden</td><td>##raden</td><td>i    </td><td>praksis</td><td>så   </td><td>er   </td><td>ikke </td><td>dagens</td><td>flertall</td><td>bestående</td><td>av     </td><td>regjerings</td><td>##partiene</td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr  </td><td>##p </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den </td><td>som  </td><td>har  </td><td>den  </td><td>største</td><td>økonomiske</td><td>rygg </td><td>##rad  </td><td>da     </td><td>i    </td><td>praksis</td><td>så   </td><td>erke </td><td>erke </td><td>dagens</td><td>flertall</td><td>bestå    </td><td>stående</td><td>regjerings</td><td>##partiet </td><td>og   </td><td>støtte</td><td>##partiet</td><td>fr  </td><td>##p </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.024</td><td>0.03</td><td>0.029</td><td>0.028</td><td>0.043</td><td>0.031  </td><td>0.016     </td><td>0.043</td><td>0.28   </td><td>0.821  </td><td>0.174</td><td>0.084  </td><td>0.163</td><td>0.485</td><td>0.625</td><td>0.059 </td><td>0.046   </td><td>0.407    </td><td>0.563  </td><td>0.051     </td><td>0.258     </td><td>0.042</td><td>0.016 </td><td>0.015    </td><td>0.01</td><td>0.01</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['dei', 'aksjane'], 'ref_pos': ['DET', 'NOUN'], 'hyp': ['de', 'aksjene'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['jo', 'en'], 'hyp_pos': ['ADV', 'DET']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['medie'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['mediediskusjon'], 'ref_pos': ['NOUN'], 'hyp': ['diskusjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['jeg'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.23\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>dei  </td><td>aks    </td><td>##jan  </td><td>##e    </td><td>føler</td><td>du   </td><td>deg  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>er   </td><td>er   </td><td>veldig</td><td>stor </td><td>medie</td><td>##diskusjon</td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>jeg  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>de   </td><td>aksjene</td><td>aksjene</td><td>aksjene</td><td>føler</td><td>du   </td><td>deg  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>jo   </td><td>en   </td><td>veldig</td><td>stor </td><td>medie</td><td>diskusjon  </td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>men  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.051</td><td>0.191</td><td>0.53   </td><td>0.608  </td><td>0.58   </td><td>0.023</td><td>0.024</td><td>0.022</td><td>0.018  </td><td>0.022</td><td>0.027</td><td>0.028</td><td>0.028</td><td>0.057</td><td>0.689</td><td>0.371</td><td>0.042 </td><td>0.043</td><td>0.057</td><td>0.14       </td><td>0.018</td><td>0.009</td><td>0.022    </td><td>0.011</td><td>0.068</td><td>0.581</td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['dei', 'aksjane'], 'ref_pos': ['DET', 'NOUN'], 'hyp': ['aksjene', 'følger'], 'hyp_pos': ['NOUN', 'VERB']}\n",
      "{'type': 'delete', 'ref': ['føler'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['deg'], 'ref_pos': ['PRON'], 'hyp': ['det'], 'hyp_pos': ['DET']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['jo', 'en'], 'hyp_pos': ['ADV', 'DET']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['medie'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['mediediskusjon'], 'ref_pos': ['NOUN'], 'hyp': ['diskusjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['jeg'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.35\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>dei    </td><td>aks    </td><td>##jan  </td><td>##e    </td><td>føler </td><td>du   </td><td>deg  </td><td>deg  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>er   </td><td>er   </td><td>veldig</td><td>stor </td><td>medie</td><td>##diskusjon</td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>jeg  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>aksjene</td><td>aksjene</td><td>aksjene</td><td>aksjene</td><td>følger</td><td>du   </td><td>du   </td><td>det  </td><td>presset</td><td>det  </td><td>er   </td><td>klart</td><td>det  </td><td>er   </td><td>jo   </td><td>en   </td><td>veldig</td><td>stor </td><td>medie</td><td>diskusjon  </td><td>om   </td><td>dette</td><td>selskapet</td><td>nå   </td><td>men  </td><td>men  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.427  </td><td>0.644  </td><td>0.696  </td><td>0.572  </td><td>0.568 </td><td>0.085</td><td>0.515</td><td>0.681</td><td>0.192  </td><td>0.042</td><td>0.045</td><td>0.043</td><td>0.038</td><td>0.065</td><td>0.694</td><td>0.378</td><td>0.051 </td><td>0.048</td><td>0.076</td><td>0.149      </td><td>0.022</td><td>0.015</td><td>0.028    </td><td>0.018</td><td>0.084</td><td>0.579</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['morgan'], 'ref_pos': ['PROPN'], 'hyp': ['morgen'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.05\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er </td><td>president</td><td>##er</td><td>men</td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir</td><td>tema</td><td>på </td><td>møter</td><td>de </td><td>skal</td><td>ha </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>morg  </td><td>##an  </td><td>ts   </td><td>##vang</td><td>##ira</td><td>##i  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er </td><td>president</td><td>##er</td><td>men</td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir</td><td>tema</td><td>på </td><td>møter</td><td>de </td><td>skal</td><td>ha </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>morgen</td><td>morgen</td><td>ts   </td><td>##vang</td><td>##ira</td><td>##i  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.001</td><td>0.001   </td><td>0.0</td><td>0.0      </td><td>0.0 </td><td>0.0</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.0  </td><td>0.0   </td><td>0.0 </td><td>0.0 </td><td>0.0</td><td>0.0  </td><td>0.0</td><td>0.0 </td><td>0.0</td><td>0.001</td><td>0.003      </td><td>0.003</td><td>0.002     </td><td>0.018     </td><td>0.375 </td><td>0.591 </td><td>0.045</td><td>0.037 </td><td>0.031</td><td>0.026</td><td>0.003</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['morgan', 'tsvangirai'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['mogen', 'sanger'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er   </td><td>president</td><td>##er </td><td>men  </td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir </td><td>tema </td><td>på   </td><td>møter</td><td>de   </td><td>skal </td><td>ha   </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>morg </td><td>##an </td><td>ts   </td><td>##vang</td><td>##ira</td><td>##i  </td><td>##i   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>tre  </td><td>gjestene</td><td>er   </td><td>president</td><td>##er </td><td>men  </td><td>mu   </td><td>##ga </td><td>##bes</td><td>immun</td><td>##itet</td><td>blir </td><td>tema </td><td>på   </td><td>møter</td><td>de   </td><td>skal </td><td>ha   </td><td>i    </td><td>ettermiddag</td><td>med  </td><td>opposisjon</td><td>##slederen</td><td>mog  </td><td>##en </td><td>##en </td><td>##en  </td><td>##en </td><td>##en </td><td>sanger</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.058</td><td>0.005</td><td>0.007   </td><td>0.002</td><td>0.004    </td><td>0.003</td><td>0.002</td><td>0.014</td><td>0.012</td><td>0.009</td><td>0.004</td><td>0.002 </td><td>0.002</td><td>0.004</td><td>0.002</td><td>0.002</td><td>0.003</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.005      </td><td>0.007</td><td>0.01      </td><td>0.027     </td><td>0.557</td><td>0.428</td><td>0.745</td><td>0.677 </td><td>0.687</td><td>0.532</td><td>0.57  </td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['sakte'], 'ref_pos': ['ADJ'], 'hyp': ['sagt'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['sakte'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skjer</td><td>for  </td><td>sakte</td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>hvorfor</td><td>og     </td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>så   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skjer</td><td>for  </td><td>sagt </td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>hvorfor</td><td>hvorfor</td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>sakte</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.049</td><td>0.022</td><td>0.013</td><td>0.027</td><td>0.129</td><td>0.636</td><td>0.037</td><td>0.014</td><td>0.007</td><td>0.005</td><td>0.005</td><td>0.004</td><td>0.004       </td><td>0.005   </td><td>0.005</td><td>0.012</td><td>0.084  </td><td>0.634  </td><td>0.08   </td><td>0.042</td><td>0.031</td><td>0.09</td><td>0.68 </td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['skjer'], 'ref_pos': ['VERB'], 'hyp': ['skjeforsagt'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['for', 'sakte'], 'ref_pos': ['ADV', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['hvorfor'], 'ref_pos': ['ADV'], 'hyp': ['hvor'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'delete', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['sakte'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skjer</td><td>for   </td><td>sakte </td><td>sakte</td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>til  </td><td>hvorfor</td><td>og     </td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>så   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>det  </td><td>skje </td><td>##fors</td><td>##fors</td><td>##agt</td><td>og   </td><td>det  </td><td>det  </td><td>må   </td><td>også </td><td>helse</td><td>##ministeren</td><td>forholde</td><td>seg  </td><td>til  </td><td>hvor </td><td>hvorfor</td><td>hvorfor</td><td>hvorfor</td><td>går  </td><td>det  </td><td>så  </td><td>sakte</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.049</td><td>0.037</td><td>0.059</td><td>0.288</td><td>0.658 </td><td>0.742 </td><td>0.747</td><td>0.035</td><td>0.019</td><td>0.012</td><td>0.007</td><td>0.006</td><td>0.008</td><td>0.005       </td><td>0.01    </td><td>0.007</td><td>0.026</td><td>0.674</td><td>0.142  </td><td>0.663  </td><td>0.087  </td><td>0.053</td><td>0.036</td><td>0.09</td><td>0.684</td><td>0.014</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['politioverbetjent'], 'ref_pos': ['NOUN'], 'hyp': ['politibetjent'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>politi</td><td>##betjent</td><td>ar   </td><td>##ne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.003         </td><td>0.003      </td><td>0.005</td><td>0.004</td><td>0.005</td><td>0.004</td><td>0.006</td><td>0.009</td><td>0.016</td><td>0.018  </td><td>0.824  </td><td>0.71   </td><td>0.159</td><td>0.058</td><td>0.024   </td><td>0.028</td><td>0.022</td><td>0.029 </td><td>0.562 </td><td>0.058    </td><td>0.026</td><td>0.03</td><td>0.026 </td><td>0.018</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved', 'politioverbetjent'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['politibetjent', 'anne'], 'hyp_pos': ['NOUN', 'PROPN']}\n",
      "{'type': 'delete', 'ref': ['arne'], 'ref_pos': ['PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>##betjent</td><td>anne </td><td>anne</td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.006         </td><td>0.005      </td><td>0.006</td><td>0.007</td><td>0.007</td><td>0.007</td><td>0.008</td><td>0.012</td><td>0.017</td><td>0.019  </td><td>0.817  </td><td>0.708  </td><td>0.17 </td><td>0.088</td><td>0.049   </td><td>0.099</td><td>0.696</td><td>0.064 </td><td>0.573 </td><td>0.117    </td><td>0.629</td><td>0.52</td><td>0.101 </td><td>0.038</td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['reiser'], 'ref_pos': ['NOUN'], 'hyp': ['reise'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.06\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>og   </td><td>og   </td><td>og   </td><td>betale</td><td>for  </td><td>sånne</td><td>reiser</td><td>selv </td><td>jo   </td><td>men  </td><td>hvis </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>og   </td><td>og   </td><td>å    </td><td>betale</td><td>for  </td><td>sånne</td><td>reise </td><td>selv </td><td>jo   </td><td>men  </td><td>hvis </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.001</td><td>0.0  </td><td>0.001   </td><td>0.001 </td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.002</td><td>0.002</td><td>0.005</td><td>0.063</td><td>0.182</td><td>0.365</td><td>0.015 </td><td>0.018</td><td>0.073</td><td>0.143 </td><td>0.009</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['og', 'og'], 'ref_pos': ['CCONJ', 'CCONJ'], 'hyp': ['å', 'å'], 'hyp_pos': ['PART', 'PART']}\n",
      "{'type': 'substitute', 'ref': ['reiser'], 'ref_pos': ['NOUN'], 'hyp': ['reise'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['hvis'], 'ref_pos': ['SCONJ'], 'hyp': ['vi'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>og   </td><td>og   </td><td>betale</td><td>for  </td><td>sånne</td><td>reiser</td><td>selv </td><td>jo   </td><td>men  </td><td>hvis </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>er   </td><td>blitt</td><td>påvirket</td><td>derfor</td><td>så   </td><td>må   </td><td>de   </td><td>stå  </td><td>helt </td><td>fritt</td><td>å    </td><td>å    </td><td>betale</td><td>for  </td><td>sånne</td><td>reise </td><td>selv </td><td>jo   </td><td>men  </td><td>vi   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.085</td><td>0.006</td><td>0.002</td><td>0.003   </td><td>0.003 </td><td>0.004</td><td>0.009</td><td>0.006</td><td>0.017</td><td>0.015</td><td>0.043</td><td>0.413</td><td>0.405</td><td>0.026 </td><td>0.021</td><td>0.071</td><td>0.141 </td><td>0.012</td><td>0.008</td><td>0.037</td><td>0.595</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['guttedager'], 'ref_pos': ['NOUN'], 'hyp': ['gutedagar'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['balder'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['baldersteinen'], 'ref_pos': ['NOUN'], 'hyp': ['steinen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['nord'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['nordeuropas'], 'ref_pos': ['NOUN'], 'hyp': ['europas'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['den'], 'ref_pos': ['PRON'], 'hyp': ['denne'], 'hyp_pos': ['DET']}\n",
      "ASD-NorBERT ref len: 34, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hår  </td><td>##fag</td><td>##re </td><td>sprang</td><td>sine </td><td>gutte</td><td>##dager</td><td>##dager</td><td>og   </td><td>bal  </td><td>##ders</td><td>##teinen</td><td>som  </td><td>er   </td><td>nord </td><td>##euro</td><td>##pas</td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>den  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hår  </td><td>##fag</td><td>##re </td><td>sprang</td><td>sine </td><td>gut  </td><td>##edag </td><td>##ar   </td><td>og   </td><td>bal  </td><td>##der </td><td>steinen </td><td>som  </td><td>er   </td><td>nord </td><td>europa</td><td>##s  </td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>denne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.03 </td><td>0.003       </td><td>0.004</td><td>0.021 </td><td>0.005    </td><td>0.018</td><td>0.029 </td><td>0.014</td><td>0.011</td><td>0.013</td><td>0.014</td><td>0.008</td><td>0.007</td><td>0.011</td><td>0.027 </td><td>0.032</td><td>0.269</td><td>0.305  </td><td>0.547  </td><td>0.064</td><td>0.097</td><td>0.395 </td><td>0.249   </td><td>0.028</td><td>0.017</td><td>0.113</td><td>0.349 </td><td>0.399</td><td>0.021</td><td>0.016</td><td>0.016</td><td>0.018</td><td>0.024  </td><td>0.25 </td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['hårfagre'], 'ref_pos': ['ADJ'], 'hyp': ['håfagresprang'], 'hyp_pos': ['X']}\n",
      "{'type': 'delete', 'ref': ['sprang'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['guttedager'], 'ref_pos': ['NOUN'], 'hyp': ['gutedagar'], 'hyp_pos': ['X']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['balder'], 'hyp_pos': ['X']}\n",
      "{'type': 'substitute', 'ref': ['baldersteinen'], 'ref_pos': ['NOUN'], 'hyp': ['steinen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['nord'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['nordeuropas'], 'ref_pos': ['NOUN'], 'hyp': ['europas'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['den'], 'ref_pos': ['PRON'], 'hyp': ['denne'], 'hyp_pos': ['DET']}\n",
      "ASD-NorBERT ref len: 34, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hår  </td><td>##fag</td><td>##re  </td><td>sprang</td><td>sine </td><td>gutte</td><td>##dager</td><td>##dager</td><td>og   </td><td>bal  </td><td>##ders</td><td>##teinen</td><td>som  </td><td>er   </td><td>nord </td><td>##euro</td><td>##pas</td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>den  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>utgangspunkt</td><td>i    </td><td>gammel</td><td>tradisjon</td><td>bal  </td><td>##ders</td><td>hage </td><td>der  </td><td>har  </td><td>##ald</td><td>hå   </td><td>##fag</td><td>##resp</td><td>##rang</td><td>sine </td><td>gut  </td><td>##edag </td><td>##ar   </td><td>og   </td><td>bal  </td><td>##der </td><td>steinen </td><td>som  </td><td>er   </td><td>nord </td><td>europa</td><td>##s  </td><td>høg  </td><td>##ste</td><td>ba   </td><td>##uta</td><td>##stein</td><td>denne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.032</td><td>0.005       </td><td>0.006</td><td>0.021 </td><td>0.007    </td><td>0.024</td><td>0.031 </td><td>0.018</td><td>0.029</td><td>0.039</td><td>0.076</td><td>0.525</td><td>0.275</td><td>0.548 </td><td>0.408 </td><td>0.083</td><td>0.274</td><td>0.309  </td><td>0.546  </td><td>0.091</td><td>0.104</td><td>0.411 </td><td>0.247   </td><td>0.027</td><td>0.018</td><td>0.111</td><td>0.349 </td><td>0.402</td><td>0.022</td><td>0.017</td><td>0.018</td><td>0.019</td><td>0.026  </td><td>0.249</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 30, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>natten</td><td>enn</td><td>om</td><td>dagen</td><td>derfor</td><td>har</td><td>natt</td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra</td><td>[UNK]</td><td>de</td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er</td><td>over</td><td>brus</td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er</td><td>der</td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>natten</td><td>enn</td><td>om</td><td>dagen</td><td>derfor</td><td>har</td><td>natt</td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra</td><td>[UNK]</td><td>de</td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er</td><td>over</td><td>brus</td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er</td><td>der</td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0     </td><td>0  </td><td>0 </td><td>0    </td><td>0     </td><td>0  </td><td>0   </td><td>0    </td><td>0      </td><td>0    </td><td>0  </td><td>0    </td><td>0 </td><td>0    </td><td>0    </td><td>0    </td><td>0       </td><td>0   </td><td>0   </td><td>0   </td><td>0     </td><td>0       </td><td>0     </td><td>0   </td><td>0  </td><td>0          </td><td>0     </td><td>0      </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['enn'], 'ref_pos': ['ADP'], 'hyp': ['endomdagen'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['om', 'dagen'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 30, ASD=0.11\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>natten</td><td>enn  </td><td>om   </td><td>dagen  </td><td>derfor</td><td>har  </td><td>natt </td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra  </td><td>[UNK]</td><td>de   </td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er </td><td>over </td><td>brus </td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er </td><td>der  </td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>natten</td><td>end  </td><td>##om </td><td>##dagen</td><td>derfor</td><td>har  </td><td>natt </td><td>##fly</td><td>##vning</td><td>##ene</td><td>fra  </td><td>[UNK]</td><td>de   </td><td>siste</td><td>årene</td><td>brukt</td><td>korridor</td><td>##er </td><td>over </td><td>brus </td><td>##sels</td><td>nordlige</td><td>utkant</td><td>##er </td><td>der  </td><td>befolknings</td><td>##tett</td><td>##heten</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.101 </td><td>0.577</td><td>0.522</td><td>0.319  </td><td>0.024 </td><td>0.005</td><td>0.005</td><td>0.002</td><td>0.002  </td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002   </td><td>0.002</td><td>0.002</td><td>0.008</td><td>0.005 </td><td>0.002   </td><td>0.001 </td><td>0.002</td><td>0.007</td><td>0.001      </td><td>0.001 </td><td>0.002  </td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['i'], 'ref_pos': ['SCONJ'], 'hyp': ['så'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['å', 'se'], 'ref_pos': ['PART', 'VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['president'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.15\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>å    </td><td>se   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>ønske    </td><td>at  </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en </td><td>og   </td><td>hans </td><td>likesinnede</td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>så   </td><td>så   </td><td>så   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>president</td><td>at  </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en </td><td>og   </td><td>hans </td><td>likesinnede</td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.015</td><td>0.535</td><td>0.611</td><td>0.468</td><td>0.114</td><td>0.037</td><td>0.022</td><td>0.015</td><td>0.014 </td><td>0.049</td><td>0.708    </td><td>0.04</td><td>0.019</td><td>0.032 </td><td>0.014</td><td>0.015 </td><td>0.011</td><td>0.007</td><td>0.018</td><td>0.016      </td><td>0.007</td><td>0.007</td><td>0.011</td><td>0.008 </td><td>0.009</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['se'], 'ref_pos': ['VERB'], 'hyp': ['så'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['president'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['like', 'siden'], 'hyp_pos': ['ADJ', 'SCONJ']}\n",
      "{'type': 'substitute', 'ref': ['likesinnede'], 'ref_pos': ['ADJ'], 'hyp': ['de'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.26\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>å    </td><td>se   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>ønske    </td><td>at   </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en</td><td>og   </td><td>hans </td><td>likesinnede</td><td>likesinnede</td><td>likesinnede</td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>å    </td><td>å    </td><td>så   </td><td>på   </td><td>film </td><td>men  </td><td>jeg  </td><td>skulle</td><td>ønske</td><td>president</td><td>at   </td><td>ray  </td><td>##mond</td><td>jo   </td><td>##hans</td><td>##en</td><td>og   </td><td>hans </td><td>like       </td><td>siden      </td><td>de         </td><td>ikke </td><td>hadde</td><td>sett </td><td>filmen</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.446</td><td>0.246</td><td>0.302</td><td>0.075</td><td>0.026</td><td>0.023</td><td>0.017</td><td>0.017 </td><td>0.061</td><td>0.698    </td><td>0.081</td><td>0.014</td><td>0.019 </td><td>0.007</td><td>0.012 </td><td>0.02</td><td>0.048</td><td>0.066</td><td>0.555      </td><td>0.802      </td><td>0.676      </td><td>0.094</td><td>0.061</td><td>0.037</td><td>0.018 </td><td>0.022</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['tilfellet'], 'ref_pos': ['NOUN'], 'hyp': ['tilfelle'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.01\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>påstår</td><td>at </td><td>det</td><td>blir</td><td>billigere</td><td>varer</td><td>og </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfellet</td><td>så   </td><td>må   </td><td>jo </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>påstår</td><td>at </td><td>det</td><td>blir</td><td>billigere</td><td>varer</td><td>og </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfelle </td><td>så   </td><td>må   </td><td>jo </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.0  </td><td>0.001 </td><td>0.0</td><td>0.0</td><td>0.0 </td><td>0.0      </td><td>0.0  </td><td>0.0</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.003</td><td>0.009</td><td>0.014</td><td>0.1      </td><td>0.003</td><td>0.001</td><td>0.0</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['påstår'], 'ref_pos': ['VERB'], 'hyp': ['påse'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['på'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['tilfellet'], 'ref_pos': ['NOUN'], 'hyp': ['tilfelle'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 22, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>påstår</td><td>at   </td><td>det  </td><td>blir </td><td>billigere</td><td>varer</td><td>og   </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfellet</td><td>så   </td><td>må   </td><td>jo   </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>påse  </td><td>at   </td><td>det  </td><td>blir </td><td>billigere</td><td>varer</td><td>på   </td><td>bedre</td><td>varer</td><td>og   </td><td>hvis </td><td>det  </td><td>er   </td><td>tilfelle </td><td>så   </td><td>må   </td><td>jo   </td><td>det  </td><td>være </td><td>bra  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.294 </td><td>0.072</td><td>0.017</td><td>0.024</td><td>0.02     </td><td>0.044</td><td>0.526</td><td>0.091</td><td>0.035</td><td>0.009</td><td>0.006</td><td>0.012</td><td>0.016</td><td>0.103    </td><td>0.005</td><td>0.004</td><td>0.003</td><td>0.005</td><td>0.004</td><td>0.005</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 20\n",
    "\n",
    "for row in higher_asd_df.itertuples():\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  word  lemma\n",
      "0       -abel           -abel\n",
      "1       -abels          -abel\n",
      "2       -abelt          -abel\n",
      "3       -abelts         -abel\n",
      "4       -able           -abel\n",
      "...       ...             ...\n",
      "744437  jordskokkpuré   -    \n",
      "744438  rotgrønnsakene  -    \n",
      "744439  albóndigas      -    \n",
      "744440  gelatinplatene  -    \n",
      "744441  kyllinglårene   -    \n",
      "\n",
      "[744442 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/talebase/data/lex/Sprakbanken/NLB/nlb_nob_20181129.lex\"\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines:\n",
    "        lex_list = line.split(\"\\t\")\n",
    "        lemma = lex_list[13].split(\":\")[1].split(\"|\")[0]\n",
    "        word_list.append(lex_list[0])\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "lex_dict = {\"word\":word_list, \"lemma\":lemma_list}\n",
    "lex_df = pd.DataFrame(lex_dict)\n",
    "print(lex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origloss_NBTale = pd.read_csv(\"./logs/OrigLoss_V5/NBTale_results_wav2vec2_NO_origLossV5_2023-09-18.csv\")\n",
    "# origloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "# origloss_NBTale.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/OrigLoss_V5/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "origloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)\n",
    "    origloss_dfs.append(df)\n",
    "\n",
    "origloss = pd.concat(origloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customloss_NBTale = pd.read_csv(\"./logs/CustomLoss_V7/NBTale_results_wav2vec2_NO_customLossV7_2023-10-02.csv\")\n",
    "# customloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "# customloss_NBTale.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/CustomLoss_V7/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "customloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)\n",
    "    customloss_dfs.append(df)\n",
    "\n",
    "customloss = pd.concat(customloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged results of orig loss & custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pd.merge(origloss, customloss, on=\"ref_str\")\n",
    "merged_results = merged_results.reindex(columns=[\"segment_id\", \"ref_str\", \"asr_origloss\", \"wer_origloss\", \"asd_origloss\", \"asr_customloss\", \"wer_customloss\", \"asd_customloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer = merged_results[merged_results[\"wer_origloss\"] > merged_results[\"wer_customloss\"]]\n",
    "worse_wer = merged_results[merged_results[\"wer_origloss\"] < merged_results[\"wer_customloss\"]]\n",
    "better_asd = merged_results[merged_results[\"asd_origloss\"] > merged_results[\"asd_customloss\"]]\n",
    "worse_asd = merged_results[merged_results[\"asd_origloss\"] < merged_results[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better WER but *worse* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer[better_wer[\"asd_origloss\"] < better_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worse WER but *better* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_wer[worse_wer[\"asd_origloss\"] > worse_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_improved_percent = (len(better_wer)/len(merged_results)) * 100\n",
    "asd_improved_percent = (len(better_asd)/len(merged_results)) * 100\n",
    "\n",
    "wer_worsened_percent = (len(worse_wer)/len(merged_results)) * 100\n",
    "asd_worsened_percent = (len(worse_asd)/len(merged_results)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WER improved: {wer_improved_percent:.2f}%\")\n",
    "print(f\"ASD improved: {asd_improved_percent:.2f}%\")\n",
    "\n",
    "print(f\"WER worsened: {wer_worsened_percent:.2f}%\")\n",
    "print(f\"ASD worsened: {asd_worsened_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC LOSS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3535249/3756479965.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, \"Container\"):\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "import types\n",
    "from tabulate import tabulate\n",
    "from dtw import *\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1868.49it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank: 31\n",
      "reduction: mean\n",
      "zero inf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"blank:\", model.config.pad_token_id)\n",
    "print(\"reduction:\", model.config.ctc_loss_reduction)\n",
    "print(\"zero inf:\", model.config.ctc_zero_infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.04, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.047, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.047, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.055, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.047, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n",
      "32 <s>\n",
      "33 </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1688 [00:00<?, ?ex/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   3%|▎         | 50/1688 [00:00<00:03, 494.87ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   8%|▊         | 140/1688 [00:00<00:02, 729.49ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  15%|█▌        | 256/1688 [00:00<00:01, 923.91ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  22%|██▏       | 374/1688 [00:00<00:01, 1022.37ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  29%|██▉       | 492/1688 [00:00<00:01, 1078.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  36%|███▌      | 607/1688 [00:00<00:00, 1102.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  43%|████▎     | 725/1688 [00:00<00:00, 1125.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|████▉     | 841/1688 [00:00<00:00, 1134.06ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  57%|█████▋    | 959/1688 [00:00<00:00, 1147.61ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  64%|██████▎   | 1074/1688 [00:01<00:01, 565.97ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  72%|███████▏  | 1209/1688 [00:01<00:00, 705.40ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|███████▉  | 1343/1688 [00:01<00:00, 834.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  87%|████████▋ | 1476/1688 [00:01<00:00, 945.22ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 1688/1688 [00:01<00:00, 974.09ex/s] \n",
      "#3: 100%|██████████| 1687/1687 [00:01<00:00, 973.48ex/s] \n",
      "\n",
      "#2: 100%|██████████| 1687/1687 [00:01<00:00, 960.13ex/s] \n",
      "#0: 100%|██████████| 1688/1688 [00:01<00:00, 937.21ex/s] \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../model_ckpts/fine-tuning_wav2vec2_expectedASD2_3ep_0p01_Rundkast_aulus6/runs/train_set.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.cast_column(\"path\", Audio())\n",
    "train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_feature = [{\"input_values\": train_dataset[73][\"input_values\"]}]\n",
    "label_feature = [{\"input_ids\": train_dataset[73][\"labels\"]}]\n",
    "\n",
    "# input_feature = [{\"input_values\": input_values} for input_values in train_dataset[34:41][\"input_values\"]]\n",
    "# label_feature = [{\"input_ids\": labels} for labels in train_dataset[34:41][\"labels\"]]\n",
    "\n",
    "batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "batch[\"labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch[\"labels\"][0]\n",
    "labels_mask = labels >= 0\n",
    "flattened_targets = labels.masked_select(labels_mask)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166, 34])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_output.pkl', 'wb') as file:\n",
    "    pickle.dump(output, file)\n",
    "\n",
    "with open('sample_input.pkl', 'wb') as file:\n",
    "    pickle.dump(batch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis):\n",
    "    ref_text = reference.replace(\"[UNK]\", \"\")  # removes the [UNK] token in the reference text, observed during training\n",
    "    hyp_text = hypothesis.replace(\"[UNK]\", \"\")\n",
    "    tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                                hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                                hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(output_mean_reference)\n",
    "    asd_score = alignment.distance / num_tokens\n",
    "    return asd_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import cuda_ctc_decoder, ctc_decoder, download_pretrained_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model.wav2vec2.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'å', 'æ', 'ø', '[UNK]', '[PAD]', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = [None] * 34\n",
    "\n",
    "for key in processor.tokenizer.vocab:\n",
    "    tokens[processor.tokenizer.vocab[key]] = key\n",
    "tokens[32] = \"</s>\"\n",
    "tokens[33] = \"<s>\"\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# f = open(\"tokens.txt\", \"w\")\n",
    "# for i, item in enumerate(tokens):\n",
    "#     if i == (len(tokens) - 1):\n",
    "#         f.write(item)\n",
    "#     else:\n",
    "#         f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 34])\n",
      "torch.Size([1, 34])\n",
      "tensor([[ 5, 18,  0, 22,  9,  0, 19, 20,  1,  4,  9,  7,  0, 14,  5,  4,  5,  0,\n",
      "         15,  7,  0, 11, 21, 18, 19,  5, 18,  0,  4,  9, 19, 19,  5,  0]])\n",
      "['er vi stadig nede og kurser dise']\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# output = model(**batch)\n",
    "# logits = output[\"logits\"]\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# print(\"LABEL:\", processor_woLM.batch_decode(batch[\"labels\"]), \"\\n\")\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# log_probs = F.log_softmax(logits[0], dim=-1, dtype=torch.float32).to(device)\n",
    "# encoder_out_lens = torch.tensor(log_probs.shape[0], device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "# cuda_decoder = cuda_ctc_decoder(tokens, nbest=10, beam_size=10, blank_skip_threshold=0.95)\n",
    "# results = cuda_decoder(log_probs, encoder_out_lens)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: ikke minst er det viktig i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig\n",
      "idx: 6 \thyp:  ikke minst er det viktige i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n",
      "idx: 0 \thyp:  ikke minst er det viktige næringe som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# characters = model_tokenizer.convert_ids_to_tokens(batch[\"labels\"][0])\n",
    "# ref_text = re.sub(\" +\", \" \", \"\".join(characters).replace(\"|\", \" \"))\n",
    "# print(\"LABEL:\", ref_text, \"\\n\")\n",
    "\n",
    "# lm=\"./language_model/\"\n",
    "decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=50, blank_token=\"[PAD]\",\n",
    "                      sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "results = decoder(logits)\n",
    "\n",
    "asd_score_list = [0] * len(results[0])\n",
    "beam_score_list = [0] * len(results[0])\n",
    "hyp_list = []\n",
    "for i, item in enumerate(results[0]):\n",
    "    # print(item.tokens.shape, item.tokens)\n",
    "    chars = decoder.idxs_to_tokens(item.tokens)\n",
    "    hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "    hyp_list.append(hyp_text)\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    asd_score_list[i] = asd_score\n",
    "    beam_score_list[i] = item.score\n",
    "\n",
    "lowest_asd_idx = np.argmin(asd_score_list)\n",
    "highest_beam_score_idx = np.argmax(beam_score_list)\n",
    "\n",
    "print(\"idx:\", lowest_asd_idx, \"\\thyp:\", hyp_list[lowest_asd_idx])\n",
    "print(\"idx:\", highest_beam_score_idx, \"\\thyp:\", hyp_list[highest_beam_score_idx])\n",
    "\n",
    "nbest_loss = torch.tensor([beam_score_list[lowest_asd_idx]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4052.2563], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=nbest_loss, inputs=logits, grad_outputs=nbest_loss, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of log_prob: torch.Size([1, 175, 500]), the shape of encoder_out_lens: torch.Size([1])\n",
      "tensor([175], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "from torchaudio.models.decoder import cuda_ctc_decoder\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio\n",
    "\n",
    "def download_asset_external(url, key):\n",
    "    path = Path(torch.hub.get_dir()) / \"torchaudio\" / Path(key)\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.hub.download_url_to_file(url, path)\n",
    "    return str(path)\n",
    "\n",
    "url_prefix = \"https://huggingface.co/Zengwei/icefall-asr-librispeech-pruned-transducer-stateless7-ctc-2022-12-01\"\n",
    "model_link = f\"{url_prefix}/resolve/main/exp/cpu_jit.pt\"\n",
    "model_path = download_asset_external(model_link, \"cuda_ctc_decoder/cpu_jit.pt\")\n",
    "\n",
    "speech_file = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n",
    "waveform, sample_rate = torchaudio.load(speech_file)\n",
    "\n",
    "actual_transcript = \"i really was very much afraid of showing him how much shocked i was at some parts of what he said\"\n",
    "actual_transcript = actual_transcript.split()\n",
    "\n",
    "device = torch.device(\"cuda\", 0)\n",
    "acoustic_model = torch.jit.load(model_path)\n",
    "acoustic_model.to(device)\n",
    "acoustic_model.eval()\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "feat = torchaudio.compliance.kaldi.fbank(waveform, num_mel_bins=80, snip_edges=False)\n",
    "feat = feat.unsqueeze(0)\n",
    "feat_lens = torch.tensor(feat.size(1), device=device).unsqueeze(0)\n",
    "\n",
    "encoder_out, encoder_out_lens = acoustic_model.encoder(feat, feat_lens)\n",
    "nnet_output = acoustic_model.ctc_output(encoder_out)\n",
    "log_prob = torch.nn.functional.log_softmax(nnet_output, -1)\n",
    "\n",
    "print(f\"The shape of log_prob: {log_prob.shape}, the shape of encoder_out_lens: {encoder_out_lens.shape}\")\n",
    "print(encoder_out_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimum ASD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvrtc.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mk2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec/lib/python3.9/site-packages/k2/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     k2_torch_cuda_version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m!=\u001b[39m k2_torch_cuda_version\n\u001b[1;32m     17\u001b[0m ):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk2 was built using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk2_torch_cuda_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut you are using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to run it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeterminizeWeightPushingType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_ragged_index_select\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swoosh_l\n",
      "\u001b[0;31mImportError\u001b[0m: libnvrtc.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import k2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lattice(\n",
    "    nnet_output: torch.Tensor,\n",
    "    decoding_graph: k2.Fsa,\n",
    "    supervision_segments: torch.Tensor,\n",
    "    search_beam: float,\n",
    "    output_beam: float,\n",
    "    min_active_states: int,\n",
    "    max_active_states: int,\n",
    "    subsampling_factor: int = 1,\n",
    ") -> k2.Fsa:\n",
    "    \"\"\"Get the decoding lattice from a decoding graph and neural\n",
    "    network output.\n",
    "    Args:\n",
    "      nnet_output:\n",
    "        It is the output of a neural model of shape `(N, T, C)`.\n",
    "      decoding_graph:\n",
    "        An Fsa, the decoding graph. It can be either an HLG\n",
    "        (see `compile_HLG.py`) or an H (see `k2.ctc_topo`).\n",
    "      supervision_segments:\n",
    "        A 2-D **CPU** tensor of dtype `torch.int32` with 3 columns.\n",
    "        Each row contains information for a supervision segment. Column 0\n",
    "        is the `sequence_index` indicating which sequence this segment\n",
    "        comes from; column 1 specifies the `start_frame` of this segment\n",
    "        within the sequence; column 2 contains the `duration` of this\n",
    "        segment.\n",
    "      search_beam:\n",
    "        Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "        (less pruning). This is the default value; it may be modified by\n",
    "        `min_active_states` and `max_active_states`.\n",
    "      output_beam:\n",
    "         Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "         to best path of output.\n",
    "      min_active_states:\n",
    "        Minimum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to have fewer than this number active.\n",
    "        Set it to zero if there is no constraint.\n",
    "      max_active_states:\n",
    "        Maximum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to exceed that but may not always succeed.\n",
    "        You can use a very large number if no constraint is needed.\n",
    "      subsampling_factor:\n",
    "        The subsampling factor of the model.\n",
    "    Returns:\n",
    "      An FsaVec containing the decoding result. It has axes [utt][state][arc].\n",
    "    \"\"\"\n",
    "    dense_fsa_vec = k2.DenseFsaVec(\n",
    "        nnet_output,\n",
    "        supervision_segments,\n",
    "        allow_truncate=subsampling_factor - 1,\n",
    "    )\n",
    "\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        decoding_graph,\n",
    "        dense_fsa_vec,\n",
    "        search_beam=search_beam,\n",
    "        output_beam=output_beam,\n",
    "        min_active_states=min_active_states,\n",
    "        max_active_states=max_active_states,\n",
    "    )\n",
    "\n",
    "    return lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWERLoss(torch.nn.Module):\n",
    "    '''Minimum Word Error Rate Loss compuration in k2.\n",
    "\n",
    "    See equation 2 of https://arxiv.org/pdf/2106.02302.pdf about its definition.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        subsampling_factor: int,\n",
    "        search_beam: int = 20,\n",
    "        output_beam: int = 8,\n",
    "        min_active_states: int = 30,\n",
    "        max_active_states: int = 10000,\n",
    "        temperature: float = 1.0,\n",
    "        num_paths: int = 100,\n",
    "        use_double_scores: bool = True,\n",
    "        nbest_scale: float = 0.5,\n",
    "        reduction: Literal['none', 'mean', 'sum'] = 'sum'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          search_beam:\n",
    "            Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "            (less pruning). This is the default value; it may be modified by\n",
    "            `min_active_states` and `max_active_states`.\n",
    "          output_beam:\n",
    "             Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "             to best path of output.\n",
    "          min_active_states:\n",
    "            Minimum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to have fewer than this number active.\n",
    "            Set it to zero if there is no constraint.\n",
    "          max_active_states:\n",
    "            Maximum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to exceed that but may not always succeed.\n",
    "            You can use a very large number if no constraint is needed.\n",
    "          subsampling_factor:\n",
    "            The subsampling factor of the model.\n",
    "          temperature:\n",
    "            For long utterances, the dynamic range of scores will be too large\n",
    "            and the posteriors will be mostly 0 or 1.\n",
    "            To prevent this it might be a good idea to have an extra argument\n",
    "            that functions like a temperature.\n",
    "            We scale the logprobs by before doing the normalization.\n",
    "          use_double_scores:\n",
    "            True to use double precision floating point.\n",
    "            False to use single precision.\n",
    "          reduction:\n",
    "            Specifies the reduction to apply to the output:\n",
    "            'none' | 'sum' | 'mean'.\n",
    "            'none': no reduction will be applied.\n",
    "                    The returned 'loss' is a k2.RaggedTensor, with\n",
    "                    loss.tot_size(0) == batch_size.\n",
    "                    loss.tot_size(1) == total_num_paths_of_current_batch\n",
    "                    If you want the MWER loss for each utterance, just do:\n",
    "                    `loss_per_utt = loss.sum()`\n",
    "                    Then loss_per_utt.shape[0] should be batch_size.\n",
    "                    See more example usages in 'k2/python/tests/mwer_test.py'\n",
    "            'sum': sum loss of each path over the whole batch together.\n",
    "            'mean': divide above 'sum' by total num paths over the whole batch.\n",
    "          nbest_scale:\n",
    "            Scale `lattice.score` before passing it to :func:`k2.random_paths`.\n",
    "            A smaller value leads to more unique paths at the risk of being not\n",
    "            to sample the path with the best score.\n",
    "          num_paths:\n",
    "            Number of paths to **sample** from the lattice\n",
    "            using :func:`k2.random_paths`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.search_beam = search_beam\n",
    "        self.output_beam = output_beam\n",
    "        self.min_active_states = min_active_states\n",
    "        self.max_active_states = max_active_states\n",
    "\n",
    "        self.num_paths = num_paths\n",
    "        self.nbest_scale = nbest_scale\n",
    "        self.subsampling_factor = subsampling_factor\n",
    "\n",
    "        self.mwer_loss = k2.MWERLoss(\n",
    "            temperature=temperature,\n",
    "            use_double_scores=use_double_scores,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        emissions_lengths: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_length: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
    "                probability distribution over labels; output of acoustic model.\n",
    "            labels (torch.FloatTensor): CPU tensor of shape `(batch, label_len)` storing labels.\n",
    "            emissions_lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                in time axis of the output Tensor in each batch.\n",
    "            labels_length (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                label in each batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor:\n",
    "                Minimum Word Error Rate loss.\n",
    "        \"\"\"\n",
    "        H = k2.ctc_topo(\n",
    "            max_token=self.vocab_size-1,\n",
    "            modified=False,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "\n",
    "        supervision_segments = torch.stack(\n",
    "            (\n",
    "                torch.tensor(range(emissions_lengths.shape[0])),\n",
    "                torch.zeros(emissions_lengths.shape[0]),\n",
    "                emissions_lengths.cpu(),\n",
    "            ),\n",
    "            1,\n",
    "        ).to(torch.int32)\n",
    "\n",
    "        lattice = get_lattice(\n",
    "            nnet_output=emissions,\n",
    "            decoding_graph=H,\n",
    "            supervision_segments=supervision_segments,\n",
    "            search_beam=self.search_beam,\n",
    "            output_beam=self.output_beam,\n",
    "            min_active_states=self.min_active_states,\n",
    "            max_active_states=self.max_active_states,\n",
    "            subsampling_factor=self.subsampling_factor,\n",
    "        )\n",
    "\n",
    "        token_ids = []\n",
    "        for i in range(labels_length.size(0)):\n",
    "            token_ids.append(labels[i, : labels_length[i]].cpu().tolist())\n",
    "\n",
    "        loss = self.mwer_loss(\n",
    "            lattice, token_ids,\n",
    "            nbest_scale=self.nbest_scale,\n",
    "            num_paths=self.num_paths\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Gumbel Softmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "asd_scores = [0] * 10\n",
    "for i in range(10):\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=10, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_scores[i] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    print(\"asd:\", asd_scores[i], \"hyp:\", hyp_text)\n",
    "\n",
    "mean_asd = np.mean(asd_scores)\n",
    "loss[0] = mean_asd\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "def get_mass_prob(paths: Tensor,\n",
    "                        softmax_ctc: Tensor,\n",
    "                        model_pred_length: Tensor,\n",
    "                        eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    compute the path probability mass\n",
    "    :param paths: ctc alignments\n",
    "    :param softmax_ctc: model logits after softmax\n",
    "    :param model_pred_length:  max length of all given paths\n",
    "    :return: avg of the paths probability\n",
    "    \"\"\"\n",
    "    log_indexes_probs = softmax_ctc.gather(dim=1, index=paths) + eps\n",
    "    if len(log_indexes_probs) > model_pred_length:\n",
    "        log_indexes_probs[model_pred_length:, :] = torch.zeros((log_indexes_probs.shape[0] - model_pred_length, 0))\n",
    "    return torch.sum(log_indexes_probs, dim=0) / (model_pred_length.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: pågripelsen av toska gjorde altså denne pågripelsen mulig\n",
      "[0.037175211785556765, 0.037175211785556765, 0.1520948636867659, 0.24277221896598888]\n",
      "['pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåskha gjorde altså denne pågripelsen mulig']\n",
      "0\n",
      "tensor([0.2708, 0.1336, 0.1375], grad_fn=<CopySlices>)\n",
      "tensor(0.5420, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "input_lengths = model._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(output[\"logits\"][0], dim=-1, dtype=torch.float32)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "num_samples = 10\n",
    "non_zero_logits = []\n",
    "non_zero_asd = []\n",
    "non_zero_hyp = []\n",
    "# for i in range(num_samples):\n",
    "while len(non_zero_asd) < 4:\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=100, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    if asd_score != 0:\n",
    "        non_zero_logits.append(sampled_logits)\n",
    "        non_zero_asd.append(asd_score)\n",
    "        non_zero_hyp.append(hyp_text)\n",
    "\n",
    "print(non_zero_asd)\n",
    "print(non_zero_hyp)\n",
    "\n",
    "lowest_asd_idx = np.argmin(np.array(non_zero_asd))\n",
    "print(lowest_asd_idx)\n",
    "\n",
    "mass_prob_list = []\n",
    "for i in range(len(non_zero_asd)):\n",
    "    logits_selected = non_zero_logits[i].type(torch.LongTensor)\n",
    "    mass_prob_list.append(get_mass_prob(logits_selected, log_probs, input_lengths))\n",
    "\n",
    "loss = torch.zeros((len(mass_prob_list)-1))\n",
    "j=0\n",
    "for i in range(len(mass_prob_list)):\n",
    "    if i != lowest_asd_idx:\n",
    "        subtract_path_probs = mass_prob_list[i] - mass_prob_list[lowest_asd_idx]\n",
    "        loss[j] = torch.sum(torch.clamp((subtract_path_probs), min=0))\n",
    "        j += 1\n",
    "print(loss)\n",
    "print(torch.sum(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating aligned cosdist (1 batch, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    ref_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "\n",
    "    return ref_alignments\n",
    "\n",
    "\n",
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed\n",
    "\n",
    "\n",
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    return cosdist_for_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_register = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    input_feature = [{\"input_values\": input_values} for input_values in train_dataset[i*8:(i+1)*8][\"input_values\"]]\n",
    "    label_feature = [{\"input_ids\": labels} for labels in train_dataset[i*8:(i+1)*8][\"labels\"]]\n",
    "\n",
    "    batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "    label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = label\n",
    "\n",
    "    output = model(**batch)\n",
    "    output_logits = output[\"logits\"]\n",
    "    pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "    labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "    for i in range(len(pred_str)):\n",
    "        ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "        pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "        label_ids = labels[i]\n",
    "        labels_mask = label_ids >= 0\n",
    "        flattened_labels = label_ids.masked_select(labels_mask)\n",
    "        ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "        tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "        cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "        cosdist_register.append(cosdist_for_ctc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_976418/112888293.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cosdist_register_arr = np.asarray(cosdist_register)\n"
     ]
    }
   ],
   "source": [
    "cosdist_register_arr = np.asarray(cosdist_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(cosdist_register_arr.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    numphones = params.shape[0]  # number of labels, not even used in the code\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    # print(seqLen, numphones, L, T)\n",
    "\n",
    "    alphas = np.zeros((L,T))\n",
    "    betas = np.zeros((L,T))\n",
    "\n",
    "    # convert logits to log softmax\n",
    "    params = params - np.max(params,axis=0)\n",
    "    params = np.exp(params)\n",
    "    params = params / np.sum(params,axis=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = np.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = np.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = np.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward += np.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = np.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = np.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = np.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward += np.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = np.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = np.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = np.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or np.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(np.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_tensor(logits, seq, cosdist_for_ctc, blank=0):\n",
    "\n",
    "    cosdist_contribution_alphas = []\n",
    "    cosdist_contribution_betas = []\n",
    "\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    # print(\"label seq:\", seqLen)\n",
    "    # print(\"label seq length with blanks:\", L)\n",
    "    # print(\"utterance length:\", T)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # print(0, L-2*(T-t), \"time:\", t, \"start:\", start, \"L:\", L, \"s:\", s, \"l:\", l)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    # llDiff = torch.abs(llForward-llBackward)\n",
    "    # if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "    #     print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "    #     print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "    #     return (-llForward, grad)\n",
    "    # else:\n",
    "    #     grad = params - grad / (params * absum)\n",
    "    #     return (-llForward, grad)\n",
    "\n",
    "    for t in range(T):\n",
    "        for s in range(numphones):\n",
    "            tmp = (params[s,t]*absum[t])\n",
    "            if tmp > 0:\n",
    "                grad[s,t] = params[s,t] - grad[s,t] / tmp\n",
    "            else:\n",
    "                grad[s,t] = params[s,t]\n",
    "\n",
    "    return (-llForward, grad, sum(cosdist_contribution_alphas), sum(cosdist_contribution_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_newgrad(logits, seq, blank=0):\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient according to https://github.com/yehudabab/NumpyCTC/blob/main/ctc.py\n",
    "    padded_labels = torch.zeros((L))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(L):\n",
    "        if i%2 == 0:\n",
    "            padded_labels[i] = 0\n",
    "        else:\n",
    "            padded_labels[i] = seq[j]\n",
    "            j += 1\n",
    "\n",
    "    # print(len(seq), seq)\n",
    "    # print(len(padded_labels), padded_labels)\n",
    "\n",
    "    grad = torch.zeros(params.shape)\n",
    "\n",
    "    score_last = alphas[L-1, T-1]\n",
    "    score_before_last = betas[L-2, T-1]\n",
    "    p_l_given_ctc = score_last + score_before_last\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(numphones):\n",
    "            d_p_d_ytk = 0\n",
    "            lb_lk = np.nonzero(list(map(lambda x: 1 if k in x else 0, padded_labels)))[0]\n",
    "            for s in lb_lk:\n",
    "                d_p_d_ytk += alphas[s, t] * betas[s, t]\n",
    "\n",
    "            d_p_d_ytk /= (params[k, t] ** 2)\n",
    "            d_lnp_d_ytk = (1. / p_l_given_ctc) * d_p_d_ytk\n",
    "            grad[k, t] = d_lnp_d_ytk\n",
    "\n",
    "    return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Stanford CTC Loss script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].detach().cpu().numpy().transpose()\n",
    "seq = batch[\"labels\"][0].detach().cpu().numpy()\n",
    "\n",
    "loss, grad = ctc_loss(logits, seq.squeeze(), blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(output[\"logits\"][0], dim=1)\n",
    "print(token_ids)\n",
    "for token in token_ids:\n",
    "    if token == 33:\n",
    "        print(\"HUY\")\n",
    "\n",
    "decoded_tokens = []\n",
    "for token in token_ids:\n",
    "    decoded_tokens.append(model_tokenizer.decode(token))\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "targets = batch[\"labels\"][0]\n",
    "input_lengths = torch.tensor(output[\"logits\"][0].shape[0])\n",
    "target_lengths = torch.tensor(batch[\"labels\"][0].shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=targets, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].transpose(1,0)\n",
    "seq = batch[\"labels\"][0]\n",
    "\n",
    "loss, grad = ctc_loss_tensor(logits, seq, blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad.shape)\n",
    "transposed_grad = grad.transpose(1,0)\n",
    "print(transposed_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad.shape[0]):\n",
    "    print(i, grad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosdist alphas: 845.0103612840176\n",
      "cosdist betas: 972.7007383406162\n",
      "torch.Size([201, 34]) tensor(608.0905, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1854.4355, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 606.3643108587712\n",
      "cosdist betas: 550.9175247717649\n",
      "torch.Size([201, 34]) tensor(680.7924, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1999.4266, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1113.2689, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 307.15736174583435\n",
      "cosdist betas: 318.57188880443573\n",
      "torch.Size([201, 34]) tensor(2163.9688, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 2056.844472726263\n",
      "cosdist betas: 1902.4644071857585\n",
      "torch.Size([201, 34]) tensor(860.0428, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_to_inspect = []\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    loss, grad, cosdist_contribution_alphas, cosdist_contribution_betas = ctc_loss_tensor(logits, flattened_labels, cosdist_for_ctc)\n",
    "    grad_to_inspect.append(grad)\n",
    "\n",
    "    print(\"cosdist alphas:\", cosdist_contribution_alphas)\n",
    "    print(\"cosdist betas:\", cosdist_contribution_betas)\n",
    "    print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for logits in output_logits:\n",
    "    token_ids = torch.argmax(logits, dim=1)\n",
    "    print(token_ids)\n",
    "\n",
    "    decoded_tokens = []\n",
    "    for token in token_ids:\n",
    "        decoded_tokens.append(model_tokenizer.decode(token))\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing only the first utterance\n",
    "model.train()\n",
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "logits = output_logits[0]\n",
    "label_ids = labels[0]\n",
    "labels_mask = label_ids >= 0\n",
    "flattened_labels = label_ids.masked_select(labels_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([235, 34])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "# python-implemented CTC loss\n",
    "loss, grad = ctc_loss_tensor(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "loss, grad = ctc_loss_newgrad(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6440, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pytorch CTC loss\n",
    "import torch.nn.functional as F\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = labels_mask.sum(-1)\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=flattened_labels, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.grad)\n",
    "#         print(param.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating ASD into CTC Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: start running here! (when loading from pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOV-8 DATA\n",
    "# ref = \"wozniackis drøm om seier knust av russiske jekaterina\"\n",
    "# hyp = \"asnake drøm om sier knut av russiske karine\"\n",
    "\n",
    "# logit_frames_decoded = pickle.load(open(\"logit_frames_decoded_8Nov.pkl\", \"rb\"))\n",
    "\n",
    "# alignment_table = [logit_frames_decoded]\n",
    "# table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "# print(\"logits decoded\")\n",
    "# display(HTML(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    cosdist_alignment_tokens = []  # FOR PRINTING\n",
    "    ref_alignments = []\n",
    "    # hyp_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        cosdist_alignment_tokens.append(round(cosdist, 3))  # FOR PRINTING\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        # hyp_token = asd_tokenizer.convert_ids_to_tokens(hyp_input_ids[hyp_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "        # hyp_alignments.append((hyp_alignment_idxs[i], hyp_token, cosdist))\n",
    "\n",
    "    # FOR PRINTING\n",
    "    # ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(ref_alignment_idxs):\n",
    "    #     ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    # ref_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(ref_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(hyp_alignment_idxs):\n",
    "    #     hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    # hyp_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(hyp_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    # ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    # cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    # alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    # table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    # print(\"Token alignment table:\")\n",
    "    # display(HTML(table))\n",
    "\n",
    "    return ref_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    # print(tokens_compressed)\n",
    "    # if len(label_ids) == len(cosdist_for_ctc):\n",
    "    #     print(\"SAME\")\n",
    "    # else:\n",
    "    #     print(\"DIFF!\")\n",
    "    # for i, label in enumerate(label_ids):\n",
    "    #     print(label, cosdist_for_ctc[i])\n",
    "\n",
    "    # try normalizing\n",
    "    x = np.array(cosdist_for_ctc)\n",
    "    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    if np.isnan(np.sum(x_norm)):\n",
    "        print(\"boo!\")\n",
    "\n",
    "    return cosdist_for_ctc, x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07505041360855103, 0.07505041360855103, 0.07505041360855103, 0, 0.12715423107147217, 0.12715423107147217, 0.12715423107147217, 0, 0.029700636863708496, 0, 0.13597166538238525, 0.13597166538238525, 0, 0.5323134958744049, 0.5323134958744049, 0, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0, 0.6916015446186066, 0.6916015446186066, 0, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0, 0, 0.032095909118652344, 0.032095909118652344, 0, 0.018728435039520264, 0.018728435039520264, 0.018728435039520264, 0, 0.016046226024627686, 0.016046226024627686, 0, 0.016792714595794678, 0.016792714595794678, 0, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414]\n",
      "[0.10851684 0.10851684 0.10851684 0.         0.18385475 0.18385475\n",
      " 0.18385475 0.         0.04294472 0.         0.19660405 0.19660405\n",
      " 0.         0.76968234 0.76968234 0.         0.30526938 0.30526938\n",
      " 0.30526938 0.30526938 0.         0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.0306659  0.0306659  0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.         0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.         0.03278869 0.03278869 0.03278869\n",
      " 0.03278869 0.         1.         1.         0.         0.22142421\n",
      " 0.22142421 0.22142421 0.22142421 0.         0.         0.04640809\n",
      " 0.04640809 0.         0.02707981 0.02707981 0.02707981 0.\n",
      " 0.02320155 0.02320155 0.         0.02428091 0.02428091 0.\n",
      " 0.02478112 0.02478112 0.02478112 0.02478112 0.         0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4022911/3475542975.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007969975471496582, 0.007969975471496582, 0.007969975471496582, 0, 0.006086826324462891, 0.006086826324462891, 0, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0, 0.015287041664123535, 0.015287041664123535, 0, 0.0193021297454834, 0.0193021297454834, 0, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144]\n",
      "[0.02681489 0.02681489 0.02681489 0.         0.02047905 0.02047905\n",
      " 0.         0.02253498 0.02253498 0.02253498 0.02253498 0.02253498\n",
      " 0.         0.05143307 0.05143307 0.         0.06494178 0.06494178\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "[0.03979825973510742, 0.03979825973510742, 0.03979825973510742, 0, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0, 0.01816505193710327, 0.01816505193710327, 0.01816505193710327, 0, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0]\n",
      "[0.22152116 0.22152116 0.22152116 0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.         0.10110853 0.10110853 0.10110853 0.         0.04921078\n",
      " 0.04921078 0.04921078 0.04921078 0.04921078 0.        ]\n",
      "[0.1013750433921814, 0.1013750433921814, 0, 0.3244396448135376, 0.3244396448135376, 0.3244396448135376, 0, 0.03615701198577881, 0, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0, 0.018376052379608154, 0.018376052379608154, 0.018376052379608154, 0, 0.032556354999542236, 0, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0, 0.6256595551967621, 0.6256595551967621, 0, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566]\n",
      "[0.16202908 0.16202908 0.         0.51855621 0.51855621 0.51855621\n",
      " 0.         0.05779023 0.         0.03629764 0.03629764 0.03629764\n",
      " 0.03629764 0.03629764 0.         0.02937069 0.02937069 0.02937069\n",
      " 0.         0.05203526 0.         0.03731947 0.03731947 0.03731947\n",
      " 0.03731947 0.         1.         1.         0.         0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.         0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449]\n"
     ]
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    max_per_frame = torch.argmax(logits, dim=1)\n",
    "    relevant_frames = []\n",
    "    for max_id in max_per_frame:\n",
    "        if max_id in flattened_labels:\n",
    "            relevant_frames.append(1)\n",
    "        else:\n",
    "            relevant_frames.append(0)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc, x_norm = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    print(cosdist_for_ctc)\n",
    "    print(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CTC with ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCORPORATING ASD COSDIST VALUES TO THE CTC CALCULATION\n",
    "\n",
    "def ctc_loss_with_ASD(params, seq, cosdist_for_ctc, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0].clone() / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])  # scale 0 to 1\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone() + alphas[s-2,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t].clone() / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1].clone() / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone() + betas[s+2,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t].clone() / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = torch.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out custom ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import gradcheck\n",
    "\n",
    "# output = model(**batch)\n",
    "# ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "# label_ids = batch[\"labels\"][0]\n",
    "# logits = output[\"logits\"][0]\n",
    "# hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "# ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "# tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "# cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "# inputs = (logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "\n",
    "# test = gradcheck(ctc_loss_with_ASD, inputs)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "print(tokens_compressed)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "loss, grad = ctc_loss_with_ASD(logits.transpose(1,0), label_ids, cosdist_for_ctc, blank=0)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = torch.tensor(label_ids.shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=label_ids, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extending torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCTC(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, seq, cosdist_for_ctc, blank=0):\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "        T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "        alphas = torch.zeros((L,T)).double()\n",
    "        betas = torch.zeros((L,T)).double()\n",
    "\n",
    "        # convert logits to log probs\n",
    "        params = params - (torch.max(params, dim=0)[0])\n",
    "        params = torch.exp(params)\n",
    "        params = params / torch.sum(params, dim=0)\n",
    "\n",
    "        # initialize alphas and forward pass\n",
    "        alphas[0,0] = params[blank,0]\n",
    "        alphas[1,0] = params[seq[0],0]\n",
    "        c = torch.sum(alphas[:,0])\n",
    "        alphas[:,0] = alphas[:,0] / c\n",
    "        llForward = torch.log(c)\n",
    "\n",
    "        for t in range(1,T):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(start,L):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s==0:\n",
    "                        alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                    else:\n",
    "                        alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == 1 or seq[l] == seq[l-1]:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time (prevent underflow)\n",
    "            c = torch.sum(alphas[start:end,t])\n",
    "            alphas[start:end,t] = alphas[start:end,t] / c\n",
    "            llForward = llForward + torch.log(c)\n",
    "\n",
    "        # initialize betas and backwards pass\n",
    "        betas[-1,-1] = params[blank,-1]\n",
    "        betas[-2,-1] = params[seq[-1],-1]\n",
    "        c = torch.sum(betas[:,-1])\n",
    "        betas[:,-1] = betas[:,-1] / c\n",
    "        llBackward = torch.log(c)\n",
    "\n",
    "        for t in range(T-2,-1,-1):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(end-1,-1,-1):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s == L-1:\n",
    "                        betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                    else:\n",
    "                        betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time\n",
    "            c = torch.sum(betas[start:end,t])\n",
    "            betas[start:end,t] = betas[start:end,t] / c\n",
    "            llBackward = llBackward + torch.log(c)\n",
    "\n",
    "        ctx.save_for_backward(params, seq, alphas, betas, llForward, llBackward)\n",
    "\n",
    "        return -llForward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        params, seq, alphas, betas, llForward, llBackward = ctx.saved_tensors\n",
    "        blank = 0\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "\n",
    "        # Compute gradient with respect to unnormalized input parameters\n",
    "        grad = torch.zeros(params.shape)\n",
    "        ab = alphas*betas\n",
    "        for s in range(L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/params[blank,:]\n",
    "            else:\n",
    "                grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "        absum = torch.sum(ab,axis=0)\n",
    "\n",
    "        llDiff = torch.abs(llForward-llBackward)\n",
    "        if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "            return (grad, None, None)\n",
    "        else:\n",
    "            grad = params - grad / (params * absum)\n",
    "            return (grad, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "print(tokens_compressed)\n",
    "\n",
    "myctcloss = MyCTC.apply\n",
    "loss = myctcloss(logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS w/ FRAME NOS. & DISREGARDING ALL THE PADS\n",
    "\n",
    "clean_logit_frames = []\n",
    "for i, item in enumerate(logit_frames_decoded):\n",
    "    if item != \"[PAD]\":\n",
    "        clean_logit_frames.append((i, item))\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(clean_logit_frames):\n",
    "    if i == 0:\n",
    "        if char[1] == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if char[1] != \"\":\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if clean_logit_frames[i-1][1] == \"\" and char[1] != \"\":  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "            elif char[1] != \"\":\n",
    "                new_word.append(char)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(\" \".join([char[1] for char in item]))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS MATRIX INTO THEIR CORRESPONDING WORDS, WHILE RETAINING ROW NOS. OR FRAME NOS.\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(logit_frames_decoded):\n",
    "    if i == 0:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if logit_frames_decoded[i-1] in [\"\", \"[PAD]\"] and char not in [\"\", \"[PAD]\"]:\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if \"\" in logit_frames_decoded[i-2:i-1] and char not in [\"\", \"[PAD]\"]:  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "            elif logit_frames_decoded[i-1] == char and char != \"\":  # repeating characters\n",
    "                new_word.append((i, char))\n",
    "            elif logit_frames_decoded[i-1] != char and char != \"\":  # new character detected\n",
    "                new_word.append((i, char))\n",
    "\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # detecting applicable frames & caclulating average cosdist\n",
    "        # frame_group = []\n",
    "        # for token_char in token_char_list:\n",
    "        #     detect_count = 0\n",
    "        #     if token_char in [x[1] for x in char_group_copy]:\n",
    "        #         for i, char in enumerate(char_group_copy):\n",
    "        #             # print(char[1], token_char, [x[1] for x in char_group_copy])\n",
    "        #             if char[1] == token_char:\n",
    "        #                 frame_group.append(char)\n",
    "        #                 detect_count += 1\n",
    "        #             else:\n",
    "        #                 if detect_count > 0:\n",
    "        #                     for count in range(detect_count):\n",
    "        #                         char_group_copy.pop(0)\n",
    "        #                     break\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # cosdist_divided = token_cosdist / len(frame_group)  # divide ASD token cosdist by number of frames for word\n",
    "        # assign divided ASD token cosdist\n",
    "        # frame_cosdist = []\n",
    "        # for char in frame_group:\n",
    "        #     if char[1] in token_char_list:\n",
    "        #         frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING THE ASD COSDIST PER NO. FRAMES\n",
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
