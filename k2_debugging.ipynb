{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import k2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Union, List, Literal\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  2],\n",
       "        [ 5,  4,  4],\n",
       "        [ 8,  7,  7],\n",
       "        [10, 11, 10]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to understand torch gather function\n",
    "t = torch.tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9],[10, 11, 12]])\n",
    "torch.gather(t, 1, torch.tensor([[0, 0, 1],[1, 0, 0],[1, 0, 0],[0, 1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': tensor([[-0.0584, -0.0568, -0.0510,  ..., -0.0050, -0.0039, -0.0039]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32), 'labels': tensor([[11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
      "          4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
      "          1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0]])}\n",
      "CausalLMOutput(loss=tensor(0.0778, requires_grad=True), logits=tensor([[[  5.2293,  -0.6872,  -4.2354,  ...,   3.9036, -15.1366, -15.2423],\n",
      "         [  7.3447,  -1.7595,  -4.6908,  ...,   5.4498, -15.2072, -15.2022],\n",
      "         [ -1.0451,  -0.9015,  -3.7738,  ...,   1.9222, -12.4645, -12.4136],\n",
      "         ...,\n",
      "         [ -4.7462,  -2.5563,  -5.0150,  ...,   9.7356, -14.3328, -14.3965],\n",
      "         [ -3.1324,  -2.8448,  -5.0748,  ...,   9.3147, -14.6481, -14.7300],\n",
      "         [ 11.1722,  -2.1108,  -3.5456,  ...,   3.0746, -15.1404, -15.1994]]],\n",
      "       requires_grad=True), hidden_states=None, attentions=None)\n",
      "tensor([ 0,  0, 11, 31, 15, 14, 14, 31, 31, 31, 31, 20, 31, 18, 18, 15, 31, 12,\n",
      "        12, 31, 31, 12, 12, 31,  0,  0, 31, 13,  5,  4,  4, 31,  0,  0, 31, 18,\n",
      "        18,  5, 31, 31,  9, 31, 31, 31, 31, 31, 31, 31, 31, 31, 19, 31, 31,  5,\n",
      "        31, 31, 31, 31,  4,  4,  9, 31, 31, 31, 31, 31,  5,  5, 20, 20, 31, 31,\n",
      "        31, 31, 20, 31, 31,  0,  0, 15, 31,  7, 31, 31,  0,  0,  0, 18, 18,  5,\n",
      "        31, 31, 31, 31, 16, 31, 18,  5,  5, 31, 31, 31, 19, 19, 31,  5, 14, 14,\n",
      "        31, 31, 20, 20, 31,  1, 31, 31, 31, 31, 31, 19, 10, 10, 15, 31, 31, 31,\n",
      "        31, 14, 14, 14, 31, 19, 19, 31, 31, 31, 31, 18, 31,  5, 31,  7, 31, 31,\n",
      "        14, 14,  9, 14, 14,  7,  7, 31,  5, 31, 18, 31, 31, 31, 31, 31, 31, 31,\n",
      "        31, 31, 31,  0])\n",
      "torch.Size([166, 1])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_input.pkl', 'rb') as file:\n",
    "    sample_input = pickle.load(file)\n",
    "    print(sample_input)\n",
    "\n",
    "with open('sample_output.pkl', 'rb') as file:\n",
    "    sample_output = pickle.load(file)\n",
    "    print(sample_output)\n",
    "\n",
    "emissions = sample_output[\"logits\"]\n",
    "log_probs = F.log_softmax(emissions[0], dim=-1, dtype=torch.float32)\n",
    "print(torch.argmax(log_probs, dim=1))\n",
    "sampled_logits = F.gumbel_softmax(emissions[0], tau=10, hard=True, dim=-1)\n",
    "path = torch.zeros(sampled_logits.shape)\n",
    "index_list = torch.argmax(sampled_logits, dim=1)\n",
    "paths_list = []\n",
    "for i in index_list:\n",
    "    paths_list.append([i])\n",
    "print(torch.tensor(paths_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.2293,  -0.6872,  -4.2354,  ...,   3.9036, -15.1366, -15.2423],\n",
       "         [  7.3447,  -1.7595,  -4.6908,  ...,   5.4498, -15.2072, -15.2022],\n",
       "         [ -1.0451,  -0.9015,  -3.7738,  ...,   1.9222, -12.4645, -12.4136],\n",
       "         ...,\n",
       "         [ -4.7462,  -2.5563,  -5.0150,  ...,   9.7356, -14.3328, -14.3965],\n",
       "         [ -3.1324,  -2.8448,  -5.0748,  ...,   9.3147, -14.6481, -14.7300],\n",
       "         [ 11.1722,  -2.1108,  -3.5456,  ...,   3.0746, -15.1404, -15.1994]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: torch.Size([4, 3])\n",
      "path: tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]])\n",
      "paths shape: torch.Size([4, 1])\n",
      "after gather shape: torch.Size([4, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         log_indexes_probs[idx, pred_length:, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m     31\u001b[0m             (log_indexes_probs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m pred_length, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(log_indexes_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (model_pred_length\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 34\u001b[0m mass_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpaths_mass_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(mass_prob)\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mpaths_mass_prob\u001b[0;34m(paths, softmax_ctc, model_pred_length, eps)\u001b[0m\n\u001b[1;32m     27\u001b[0m log_indexes_probs \u001b[38;5;241m=\u001b[39m softmax_ctc\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, paths)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter gather shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_indexes_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, pred_length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_pred_length\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     30\u001b[0m     log_indexes_probs[idx, pred_length:, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m     31\u001b[0m         (log_indexes_probs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m pred_length, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(log_indexes_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (model_pred_length\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def sample_n_paths(num_of_paths, softmax_ctc):\n",
    "    \"\"\"\n",
    "    sample n paths with respect to the ctc probabilites\n",
    "    :param softmax_ctc: model output ctc after softmax\n",
    "    :return: N sampled paths\n",
    "    \"\"\"\n",
    "    ctc_swapped = softmax_ctc\n",
    "    paths = [list(torch.utils.data.WeightedRandomSampler(line, num_of_paths, replacement=True)) for line in\n",
    "                ctc_swapped]\n",
    "    return paths\n",
    "\n",
    "t = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [7,8,9]])\n",
    "print(\"input tensor shape:\", t.shape)\n",
    "paths = torch.tensor(sample_n_paths(1, t)).type(torch.LongTensor)\n",
    "paths_list = sample_n_paths(1, t)\n",
    "print(\"path:\", paths)\n",
    "print(\"paths shape:\", paths.shape)\n",
    "\n",
    "def paths_mass_prob(paths, softmax_ctc, model_pred_length, eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    compute the path probability mass\n",
    "    :param paths: ctc alignments\n",
    "    :param softmax_ctc: model logits after softmax\n",
    "    :param model_pred_length:  max length of all given paths\n",
    "    :return: avg of the paths probability\n",
    "    \"\"\"\n",
    "    log_indexes_probs = softmax_ctc.gather(1, paths)\n",
    "    print(\"after gather shape:\", log_indexes_probs.shape)\n",
    "    for idx, pred_length in enumerate(model_pred_length):\n",
    "        log_indexes_probs[idx, pred_length:, :] = torch.zeros(\n",
    "            (log_indexes_probs.shape[1] - pred_length, 1))\n",
    "    return torch.sum(log_indexes_probs, dim=1) / (model_pred_length.unsqueeze(-1))\n",
    "\n",
    "mass_prob = paths_mass_prob(paths, t, 3)\n",
    "print(mass_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1687.34it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k2 debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "\n",
      "k2 version: 1.24.4\n",
      "Build type: Release\n",
      "Git SHA1: 8f976a1e1407e330e2a233d68f81b1eb5269fdaa\n",
      "Git date: Thu Jun 6 02:13:08 2024\n",
      "Cuda used to build k2: 12.1\n",
      "cuDNN used to build k2: \n",
      "Python version used to build k2: 3.9\n",
      "OS used to build k2: CentOS Linux release 7.9.2009 (Core)\n",
      "CMake version: 3.29.3\n",
      "GCC version: 9.3.1\n",
      "CMAKE_CUDA_FLAGS: -Wno-deprecated-gpu-targets -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_50,code=sm_50 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_61,code=sm_61 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_75,code=sm_75 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -D_GLIBCXX_USE_CXX11_ABI=0 --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
      "CMAKE_CXX_FLAGS:  -D_GLIBCXX_USE_CXX11_ABI=0 -Wno-unused-variable  -Wno-strict-overflow \n",
      "PyTorch version used to build k2: 2.3.1+cu121\n",
      "PyTorch is using Cuda: 12.1\n",
      "NVTX enabled: True\n",
      "With CUDA: True\n",
      "Disable debug: True\n",
      "Sync kernels : False\n",
      "Disable checks: False\n",
      "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
      "k2 abort: False\n",
      "__file__: /home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/k2/version/version.py\n",
      "_k2.__file__: /home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/_k2.cpython-39-x86_64-linux-gnu.so\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python3 -m k2.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_texts = [ [0, 2], [1, 2, 3] ]\n",
    "\n",
    "refs = k2.levenshtein_graph(ref_texts, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = k2.levenshtein_graph([ [1, 2], [1, 2, 3] ], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2023 Lucky Wong\n",
    "#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\"\"\"CTC based Minimum Word Error Rate Loss definition.\"\"\"\n",
    "\n",
    "class MWERLoss(torch.nn.Module):\n",
    "    '''Minimum Word Error Rate Loss compuration in k2.\n",
    "\n",
    "    See equation 2 of https://arxiv.org/pdf/2106.02302.pdf about its definition.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        subsampling_factor: int,\n",
    "        search_beam: int = 20,\n",
    "        output_beam: int = 8,\n",
    "        min_active_states: int = 30,\n",
    "        max_active_states: int = 10000,\n",
    "        temperature: float = 1.0,\n",
    "        num_paths: int = 100,\n",
    "        use_double_scores: bool = True,\n",
    "        nbest_scale: float = 0.5,\n",
    "        reduction: Literal['none', 'mean', 'sum'] = 'none'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          search_beam:\n",
    "            Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "            (less pruning). This is the default value; it may be modified by\n",
    "            `min_active_states` and `max_active_states`.\n",
    "          output_beam:\n",
    "             Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "             to best path of output.\n",
    "          min_active_states:\n",
    "            Minimum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to have fewer than this number active.\n",
    "            Set it to zero if there is no constraint.\n",
    "          max_active_states:\n",
    "            Maximum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to exceed that but may not always succeed.\n",
    "            You can use a very large number if no constraint is needed.\n",
    "          subsampling_factor:\n",
    "            The subsampling factor of the model.\n",
    "          temperature:\n",
    "            For long utterances, the dynamic range of scores will be too large\n",
    "            and the posteriors will be mostly 0 or 1.\n",
    "            To prevent this it might be a good idea to have an extra argument\n",
    "            that functions like a temperature.\n",
    "            We scale the logprobs by before doing the normalization.\n",
    "          use_double_scores:\n",
    "            True to use double precision floating point.\n",
    "            False to use single precision.\n",
    "          reduction:\n",
    "            Specifies the reduction to apply to the output:\n",
    "            'none' | 'sum' | 'mean'.\n",
    "            'none': no reduction will be applied.\n",
    "                    The returned 'loss' is a k2.RaggedTensor, with\n",
    "                    loss.tot_size(0) == batch_size.\n",
    "                    loss.tot_size(1) == total_num_paths_of_current_batch\n",
    "                    If you want the MWER loss for each utterance, just do:\n",
    "                    `loss_per_utt = loss.sum()`\n",
    "                    Then loss_per_utt.shape[0] should be batch_size.\n",
    "                    See more example usages in 'k2/python/tests/mwer_test.py'\n",
    "            'sum': sum loss of each path over the whole batch together.\n",
    "            'mean': divide above 'sum' by total num paths over the whole batch.\n",
    "          nbest_scale:\n",
    "            Scale `lattice.score` before passing it to :func:`k2.random_paths`.\n",
    "            A smaller value leads to more unique paths at the risk of being not\n",
    "            to sample the path with the best score.\n",
    "          num_paths:\n",
    "            Number of paths to **sample** from the lattice\n",
    "            using :func:`k2.random_paths`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.search_beam = search_beam\n",
    "        self.output_beam = output_beam\n",
    "        self.min_active_states = min_active_states\n",
    "        self.max_active_states = max_active_states\n",
    "\n",
    "        self.num_paths = num_paths\n",
    "        self.nbest_scale = nbest_scale\n",
    "        self.subsampling_factor = subsampling_factor\n",
    "\n",
    "        self.mwer_loss = k2.MWERLoss(\n",
    "            temperature=temperature,\n",
    "            use_double_scores=use_double_scores,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        emissions_lengths: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_length: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
    "                probability distribution over labels; output of acoustic model.\n",
    "            labels (torch.FloatTensor): CPU tensor of shape `(batch, label_len)` storing labels.\n",
    "            emissions_lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                in time axis of the output Tensor in each batch.\n",
    "            labels_length (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                label in each batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor:\n",
    "                Minimum Word Error Rate loss.\n",
    "        \"\"\"\n",
    "        H = k2.ctc_topo(\n",
    "            max_token=self.vocab_size-1,\n",
    "            modified=False,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "\n",
    "        supervision_segments = torch.stack(\n",
    "            (\n",
    "                torch.tensor(range(emissions_lengths.shape[0])),\n",
    "                torch.zeros(emissions_lengths.shape[0]),\n",
    "                emissions_lengths.cpu(),\n",
    "            ),\n",
    "            1,\n",
    "        ).to(torch.int32)\n",
    "\n",
    "        lattice = get_lattice(\n",
    "            nnet_output=emissions,\n",
    "            decoding_graph=H,\n",
    "            supervision_segments=supervision_segments,\n",
    "            search_beam=self.search_beam,\n",
    "            output_beam=self.output_beam,\n",
    "            min_active_states=self.min_active_states,\n",
    "            max_active_states=self.max_active_states,\n",
    "            subsampling_factor=self.subsampling_factor,\n",
    "        )\n",
    "\n",
    "        token_ids = []\n",
    "        for i in range(labels_length.size(0)):\n",
    "            # token_ids.append(labels[i, : labels_length[i]].cpu().tolist())\n",
    "            temp = labels[i, : labels_length[i]].cpu().tolist()\n",
    "            token_ids.append(list(filter(lambda num: num != 0, temp)))\n",
    "\n",
    "        loss = self.mwer_loss(\n",
    "            lattice, token_ids,\n",
    "            nbest_scale=self.nbest_scale,\n",
    "            num_paths=self.num_paths\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lattice(\n",
    "    nnet_output: torch.Tensor,\n",
    "    decoding_graph: k2.Fsa,\n",
    "    supervision_segments: torch.Tensor,\n",
    "    search_beam: float,\n",
    "    output_beam: float,\n",
    "    min_active_states: int,\n",
    "    max_active_states: int,\n",
    "    subsampling_factor: int = 1,\n",
    ") -> k2.Fsa:\n",
    "    \"\"\"Get the decoding lattice from a decoding graph and neural\n",
    "    network output.\n",
    "    Args:\n",
    "      nnet_output:\n",
    "        It is the output of a neural model of shape `(N, T, C)`.\n",
    "      decoding_graph:\n",
    "        An Fsa, the decoding graph. It can be either an HLG\n",
    "        (see `compile_HLG.py`) or an H (see `k2.ctc_topo`).\n",
    "      supervision_segments:\n",
    "        A 2-D **CPU** tensor of dtype `torch.int32` with 3 columns.\n",
    "        Each row contains information for a supervision segment. Column 0\n",
    "        is the `sequence_index` indicating which sequence this segment\n",
    "        comes from; column 1 specifies the `start_frame` of this segment\n",
    "        within the sequence; column 2 contains the `duration` of this\n",
    "        segment.\n",
    "      search_beam:\n",
    "        Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "        (less pruning). This is the default value; it may be modified by\n",
    "        `min_active_states` and `max_active_states`.\n",
    "      output_beam:\n",
    "         Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "         to best path of output.\n",
    "      min_active_states:\n",
    "        Minimum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to have fewer than this number active.\n",
    "        Set it to zero if there is no constraint.\n",
    "      max_active_states:\n",
    "        Maximum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to exceed that but may not always succeed.\n",
    "        You can use a very large number if no constraint is needed.\n",
    "      subsampling_factor:\n",
    "        The subsampling factor of the model.\n",
    "    Returns:\n",
    "      An FsaVec containing the decoding result. It has axes [utt][state][arc].\n",
    "    \"\"\"\n",
    "    dense_fsa_vec = k2.DenseFsaVec(\n",
    "        nnet_output,\n",
    "        supervision_segments,\n",
    "        allow_truncate=subsampling_factor - 1,\n",
    "    )\n",
    "\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        decoding_graph,\n",
    "        dense_fsa_vec,\n",
    "        search_beam=search_beam,\n",
    "        output_beam=output_beam,\n",
    "        min_active_states=min_active_states,\n",
    "        max_active_states=max_active_states,\n",
    "    )\n",
    "\n",
    "    return lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': tensor([[-0.0584, -0.0568, -0.0510,  ..., -0.0050, -0.0039, -0.0039]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32), 'labels': tensor([[11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
      "          4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
      "          1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0]])}\n",
      "CausalLMOutput(loss=tensor(0.0778, requires_grad=True), logits=tensor([[[  5.2293,  -0.6872,  -4.2354,  ...,   3.9036, -15.1366, -15.2423],\n",
      "         [  7.3447,  -1.7595,  -4.6908,  ...,   5.4498, -15.2072, -15.2022],\n",
      "         [ -1.0451,  -0.9015,  -3.7738,  ...,   1.9222, -12.4645, -12.4136],\n",
      "         ...,\n",
      "         [ -4.7462,  -2.5563,  -5.0150,  ...,   9.7356, -14.3328, -14.3965],\n",
      "         [ -3.1324,  -2.8448,  -5.0748,  ...,   9.3147, -14.6481, -14.7300],\n",
      "         [ 11.1722,  -2.1108,  -3.5456,  ...,   3.0746, -15.1404, -15.1994]]],\n",
      "       requires_grad=True), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_input.pkl', 'rb') as file:\n",
    "    sample_input = pickle.load(file)\n",
    "    print(sample_input)\n",
    "\n",
    "with open('sample_output.pkl', 'rb') as file:\n",
    "    sample_output = pickle.load(file)\n",
    "    print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
      "          4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
      "          1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0]])\n"
     ]
    }
   ],
   "source": [
    "emissions = sample_output[\"logits\"]\n",
    "labels = sample_input[\"labels\"]\n",
    "\n",
    "labels_length = torch.zeros((labels.size(dim=0)))\n",
    "for i in range(labels.size(dim=0)):\n",
    "    labels_mask = labels[i] >= 0\n",
    "    labels_length[i] = len(labels[i].masked_select(labels_mask))\n",
    "labels_length = labels_length.to(torch.int)\n",
    "print(labels)\n",
    "\n",
    "emissions_lengths = torch.tensor([emissions.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(emissions, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = []\n",
    "for i in range(labels_length.size(0)):\n",
    "    temp = labels[i, : labels_length[i]].cpu().tolist()\n",
    "    token_ids.append(list(filter(lambda num: num != 0, temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = k2.ctc_topo(\n",
    "    max_token=33,\n",
    "    modified=False,\n",
    "    device=emissions.device,\n",
    ")\n",
    "\n",
    "# isym = k2.SymbolTable.from_str('''\n",
    "# | 0\n",
    "# a 1\n",
    "# b 2\n",
    "# c 3\n",
    "# d 4\n",
    "# e 5\n",
    "# f 6\n",
    "# g 7\n",
    "# h 8\n",
    "# i 9\n",
    "# j 10\n",
    "# k 11\n",
    "# l 12\n",
    "# m 13\n",
    "# n 14\n",
    "# o 15\n",
    "# p 16\n",
    "# q 17\n",
    "# r 18\n",
    "# s 19\n",
    "# t 20\n",
    "# u 21\n",
    "# v 22\n",
    "# w 23\n",
    "# x 24\n",
    "# y 25\n",
    "# z 26\n",
    "# å 27\n",
    "# æ 28\n",
    "# ø 29\n",
    "# ''')\n",
    "\n",
    "# osym = k2.SymbolTable.from_str('''\n",
    "# | 0\n",
    "# a 1\n",
    "# b 2\n",
    "# c 3\n",
    "# d 4\n",
    "# e 5\n",
    "# f 6\n",
    "# g 7\n",
    "# h 8\n",
    "# i 9\n",
    "# j 10\n",
    "# k 11\n",
    "# l 12\n",
    "# m 13\n",
    "# n 14\n",
    "# o 15\n",
    "# p 16\n",
    "# q 17\n",
    "# r 18\n",
    "# s 19\n",
    "# t 20\n",
    "# u 21\n",
    "# v 22\n",
    "# w 23\n",
    "# x 24\n",
    "# y 25\n",
    "# z 26\n",
    "# å 27\n",
    "# æ 28\n",
    "# ø 29\n",
    "# ''')\n",
    "\n",
    "# H.labels_sym = isym\n",
    "# H.aux_labels_sym = osym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 166, 34])\n",
      "tensor([166])\n",
      "tensor([[  0,   0, 166]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "supervision_segments = torch.stack(\n",
    "    (\n",
    "        torch.tensor(range(emissions_lengths.shape[0])),\n",
    "        torch.zeros(emissions_lengths.shape[0]),\n",
    "        emissions_lengths.cpu(),\n",
    "    ),\n",
    "    1,\n",
    ").to(torch.int32)\n",
    "\n",
    "print(log_probs.shape)\n",
    "print(emissions_lengths)\n",
    "print(supervision_segments)\n",
    "\n",
    "lattice = get_lattice(\n",
    "    nnet_output=log_probs,\n",
    "    decoding_graph=H,\n",
    "    supervision_segments=supervision_segments,\n",
    "    search_beam=20,\n",
    "    output_beam=8,\n",
    "    min_active_states=30,\n",
    "    max_active_states=10000,\n",
    "    subsampling_factor=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = k2.Nbest.from_lattice(\n",
    "            lattice=lattice,\n",
    "            num_paths=100,\n",
    "            use_double_scores=True,\n",
    "            nbest_scale=0.01,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_texts(\n",
    "    best_paths: k2.Fsa, return_ragged: bool = False\n",
    ") -> Union[List[List[int]], k2.RaggedTensor]:\n",
    "    \"\"\"Extract the texts (as word IDs) from the best-path FSAs.\n",
    "\n",
    "    Note:\n",
    "        Used by Nbest.build_levenshtein_graphs during MWER computation.\n",
    "        Copied from icefall.\n",
    "\n",
    "    Args:\n",
    "      best_paths:\n",
    "        A k2.Fsa with best_paths.arcs.num_axes() == 3, i.e.\n",
    "        containing multiple FSAs, which is expected to be the result\n",
    "        of k2.shortest_path (otherwise the returned values won't\n",
    "        be meaningful).\n",
    "      return_ragged:\n",
    "        True to return a ragged tensor with two axes [utt][word_id].\n",
    "        False to return a list-of-list word IDs.\n",
    "    Returns:\n",
    "      Returns a list of lists of int, containing the label sequences we\n",
    "      decoded.\n",
    "    \"\"\"\n",
    "    if isinstance(best_paths.aux_labels, k2.RaggedTensor):\n",
    "        # remove 0's and -1's.\n",
    "        aux_labels = best_paths.aux_labels.remove_values_leq(-1)\n",
    "        # TODO: change arcs.shape() to arcs.shape\n",
    "        aux_shape = best_paths.arcs.shape().compose(aux_labels.shape)\n",
    "\n",
    "        # remove the states and arcs axes.\n",
    "        aux_shape = aux_shape.remove_axis(1)\n",
    "        aux_shape = aux_shape.remove_axis(1)\n",
    "        aux_labels = k2.RaggedTensor(aux_shape, aux_labels.values)\n",
    "    else:\n",
    "        # remove axis corresponding to states.\n",
    "        aux_shape = best_paths.arcs.shape().remove_axis(1)\n",
    "        aux_labels = k2.RaggedTensor(aux_shape, best_paths.aux_labels)\n",
    "        # remove 0's and -1's.\n",
    "        aux_labels = aux_labels.remove_values_leq(-1)\n",
    "\n",
    "    assert aux_labels.num_axes == 2\n",
    "    if return_ragged:\n",
    "        return aux_labels\n",
    "    else:\n",
    "        return aux_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "word_ids = _get_texts(nbest.fsa, return_ragged=False)\n",
    "print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "a k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] le[PAD] m ed [PAD] r e [PAD]i[PAD] s[PAD]e r[PAD] d i [PAD] je[PAD] t [PAD] t[PAD] [PAD]o[PAD]g [PAD] r e[PAD] p[PAD]re[PAD] s[PAD]e n[PAD] t[PAD] a[PAD] s jo [PAD] n [PAD]s[PAD] r[PAD]eg [PAD] ni[PAD]n[PAD]g[PAD]e[PAD] \n",
      "166\n",
      "n k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l[PAD] r med [PAD] r e [PAD]i[PAD] s[PAD] di [PAD] j[PAD] t[PAD] d[PAD] [PAD]o[PAD]g [PAD] [PAD] r[PAD]e[PAD] p[PAD]re[PAD] s[PAD] en [PAD] t [PAD]a[PAD] sj o[PAD] n[PAD] s[PAD] r e[PAD]y[PAD] n[PAD]in g [PAD]e[PAD]r[PAD] \n",
      "166\n",
      "v k[PAD]on[PAD] t[PAD]r o[PAD]l[PAD] l n [PAD] med r a[PAD]i [PAD] s[PAD] er[PAD] d e[PAD] j[PAD]e[PAD]t [PAD] d[PAD]e o[PAD]g [PAD] r e[PAD] p[PAD]r[PAD]e[PAD] s[PAD]e n[PAD] t[PAD]a[PAD] s[PAD]jo[PAD] n [PAD] s[PAD] r[PAD]e[PAD]k[PAD] n i[PAD]n g[PAD] r[PAD] \n",
      "166\n",
      "h k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] l [PAD] m e[PAD]d[PAD] [PAD] r e[PAD] i[PAD] s[PAD]e [PAD] d i[PAD] h[PAD] et [PAD] t[PAD] o[PAD]g [PAD] r e[PAD] pr e[PAD] s[PAD] e n[PAD] t[PAD] a[PAD] s[PAD]jo [PAD] n[PAD] s[PAD] r e[PAD]n[PAD] ni [PAD]ng [PAD] \n",
      "166\n",
      " [PAD]k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] l e [PAD]me[PAD]d [PAD]r e[PAD]i [PAD] s[PAD]e [PAD] d i [PAD] j[PAD]e t[PAD] t o[PAD]g [PAD] r e[PAD] pr e [PAD] s [PAD]en [PAD] t [PAD]a[PAD] s[PAD]jo [PAD] n [PAD]s[PAD] r[PAD]e[PAD]y[PAD] ni n g[PAD] \n",
      "166\n",
      "a k[PAD]on [PAD] t[PAD] ro[PAD]l [PAD] le[PAD]t [PAD]me[PAD]d r[PAD]e i [PAD] s [PAD]e [PAD] d [PAD] h[PAD]e[PAD]t [PAD] d[PAD]e o[PAD]g[PAD] r[PAD]e[PAD] p[PAD]re [PAD] s [PAD]en [PAD] t[PAD] a[PAD] s[PAD]jo[PAD] n [PAD] s[PAD] r e g [PAD] ni [PAD]ng[PAD] \n",
      "166\n",
      "v k[PAD]on [PAD] t[PAD] ro[PAD]l[PAD] l n med[PAD] [PAD]r[PAD]e [PAD] s[PAD] et[PAD] d i [PAD] j[PAD]e[PAD]t[PAD] te[PAD] o[PAD]g [PAD] r e[PAD] pr e [PAD] s[PAD] en [PAD] t [PAD]a[PAD] sj o[PAD] n [PAD] s[PAD] r e[PAD]k[PAD] ni n g[PAD] r[PAD] \n",
      "166\n",
      "l k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l[PAD] me[PAD]d [PAD] r[PAD]e[PAD] i[PAD] s[PAD] e[PAD]r[PAD] du[PAD] he[PAD]t [PAD] t[PAD] o[PAD]g[PAD] r [PAD]e[PAD] pr e[PAD] s [PAD]e n[PAD] t[PAD]a[PAD] sj[PAD]o [PAD] n [PAD]s [PAD] r e[PAD]r[PAD] n[PAD]in g[PAD] e r[PAD] \n",
      "166\n",
      "o[PAD]k[PAD]on[PAD] t[PAD]r o[PAD]l[PAD] le[PAD] m ed[PAD] [PAD]r[PAD]e[PAD] i[PAD] s[PAD] e[PAD] [PAD] d[PAD]i [PAD] je t [PAD] d[PAD] o[PAD]g [PAD] r[PAD]e[PAD] pr e[PAD] s[PAD]e n[PAD] t[PAD] a[PAD] s j[PAD]o [PAD] n[PAD] s[PAD] r [PAD]e[PAD]i[PAD] ni [PAD]ng[PAD] \n",
      "166\n",
      "r k[PAD]on [PAD] t[PAD]r o[PAD]l[PAD] l e m e[PAD]d [PAD]r[PAD]e[PAD]i [PAD] s[PAD] [PAD] d[PAD]i[PAD] j[PAD] et [PAD] t [PAD] o[PAD]g[PAD] r e[PAD] pr e[PAD] s [PAD]e[PAD]n[PAD] t[PAD]a[PAD] s[PAD]jo[PAD] n [PAD] s[PAD] r e g [PAD] ni n g[PAD]e[PAD] \n",
      "166\n",
      "å k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] l [PAD]r med[PAD] r [PAD]e[PAD]i [PAD] s[PAD] dy[PAD] j[PAD] e[PAD]t [PAD] o[PAD]g[PAD] [PAD]r[PAD]e[PAD] p[PAD]re[PAD] s [PAD]e[PAD]n[PAD] t [PAD]a[PAD] s[PAD]jo[PAD] n [PAD] s[PAD] r[PAD]e[PAD] n i[PAD]ng [PAD]er [PAD] \n",
      "166\n",
      "r k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] le[PAD]t m ed[PAD] [PAD]r[PAD]e [PAD]i[PAD] s [PAD] e [PAD] d [PAD] j[PAD] t[PAD] t[PAD]e o[PAD]g [PAD] r [PAD]e[PAD] pr e[PAD] s[PAD]e[PAD]n[PAD] t[PAD] a[PAD] sj[PAD]o[PAD] n [PAD]s [PAD] r e[PAD]k[PAD] n[PAD]in g[PAD] e r[PAD] \n",
      "166\n",
      "t k[PAD]on [PAD] t[PAD] ro[PAD]l[PAD] l e m e[PAD]d[PAD] [PAD]r[PAD]e i [PAD] s[PAD]e [PAD] d [PAD] h[PAD]e [PAD]t [PAD] t [PAD]o[PAD]g[PAD] r [PAD]e[PAD] pr [PAD]e[PAD] s[PAD]e n[PAD] t [PAD]a[PAD] s jo [PAD] n [PAD] s[PAD] r[PAD]e[PAD]v[PAD] n in g[PAD]e[PAD]r[PAD] \n",
      "166\n",
      "o k[PAD]on [PAD] t[PAD] ro[PAD]l [PAD] l[PAD] t med[PAD] [PAD]r e [PAD] s [PAD] e [PAD] d[PAD]i [PAD] j[PAD] t[PAD] [PAD]o[PAD]g[PAD] r e[PAD] pr [PAD]e[PAD] s[PAD] e[PAD]n[PAD] t [PAD]a[PAD] s j[PAD]o[PAD] n [PAD] s [PAD] r e[PAD]y[PAD] n[PAD]i[PAD]n[PAD]g[PAD]e r[PAD] \n",
      "166\n",
      "h k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l [PAD]r [PAD]me[PAD]d[PAD] r [PAD]e[PAD]i [PAD] s [PAD] er[PAD] d i [PAD] j[PAD]e[PAD]t[PAD] t[PAD] [PAD]o[PAD]g[PAD] [PAD] r e[PAD] p[PAD]re[PAD] s[PAD] en [PAD] t [PAD]a[PAD] s[PAD]jo [PAD] n [PAD] s[PAD] re [PAD]g[PAD] ni [PAD]ng [PAD]er [PAD] \n",
      "166\n",
      "g[PAD]k[PAD]on [PAD] t[PAD] ro[PAD]l [PAD] l n[PAD] med [PAD] [PAD]r[PAD]e [PAD]i[PAD] s [PAD]e [PAD] d e[PAD] e[PAD]t [PAD] ta[PAD] o[PAD]g[PAD] r [PAD]e[PAD] pr [PAD]e[PAD] s[PAD] e n[PAD] t[PAD]a[PAD] sj[PAD]o[PAD] n [PAD]s [PAD] r e[PAD]t[PAD] n[PAD]i[PAD]ng[PAD] e r[PAD] \n",
      "166\n",
      "i k[PAD]on [PAD] t[PAD] ro[PAD]l [PAD] l [PAD] m ed [PAD] r e[PAD]i [PAD] s[PAD] d [PAD] j[PAD]e[PAD]t[PAD] t[PAD] [PAD]o[PAD]g[PAD] [PAD]r[PAD]e[PAD] p[PAD]re[PAD] s[PAD]e n[PAD] t [PAD]a[PAD] s jo [PAD] n [PAD]s[PAD] r [PAD]e[PAD]t[PAD] ni ng[PAD] e r[PAD] \n",
      "166\n",
      "o[PAD]k[PAD]on [PAD] t[PAD]r o[PAD]l[PAD] le[PAD] med [PAD]r e[PAD] i[PAD] s [PAD]e [PAD] [PAD] de[PAD] et [PAD] ta[PAD] [PAD]o[PAD]g[PAD] r e[PAD] pr [PAD]e[PAD] s [PAD]e n[PAD] t [PAD]a[PAD] s [PAD]jo [PAD] n[PAD] s[PAD] r e [PAD]d[PAD] ni ng [PAD]e[PAD]r[PAD] \n",
      "166\n",
      "r[PAD]k[PAD]on [PAD] t[PAD]r o[PAD]l[PAD] le[PAD]r me[PAD]d[PAD] r [PAD]e [PAD] s[PAD] er[PAD] d i[PAD] je t [PAD] e[PAD] o[PAD]g[PAD] r e[PAD] p[PAD]r[PAD]e[PAD] s [PAD]en [PAD] t [PAD]a[PAD] sj o [PAD] n [PAD] s[PAD] r e[PAD]n[PAD] n[PAD]i ng [PAD] \n",
      "166\n",
      "g[PAD]k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] l [PAD]e m ed [PAD] r a[PAD] i[PAD] s [PAD]e [PAD] d [PAD] et [PAD] d[PAD] o[PAD]g [PAD] r e[PAD] p[PAD]r e[PAD] s[PAD] en [PAD] t[PAD] a[PAD] sj o[PAD] n [PAD] s[PAD] r [PAD]e[PAD]r[PAD] n i[PAD]ng[PAD] er [PAD] \n",
      "166\n",
      "e k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l n[PAD] me[PAD]d r [PAD]e [PAD] s[PAD] et[PAD] d i[PAD] h[PAD] t[PAD] t[PAD]t [PAD]o[PAD]g[PAD] [PAD] r e[PAD] p[PAD]r[PAD]e[PAD] s [PAD]e n[PAD] t [PAD]a[PAD] s [PAD]jo [PAD] n [PAD]s[PAD] re ig[PAD] ni n g[PAD] \n",
      "166\n",
      "v k[PAD]on [PAD] t[PAD]r o[PAD]l[PAD] l n me[PAD]d[PAD] r e[PAD]i [PAD] s[PAD]e [PAD] [PAD]d u[PAD] j[PAD] t [PAD] e[PAD] o[PAD]g [PAD] r e[PAD] p[PAD]r[PAD]e[PAD] s[PAD] en [PAD] t [PAD]a[PAD] s jo[PAD] n[PAD] s[PAD] r [PAD]e[PAD]g[PAD] nin g[PAD] \n",
      "166\n",
      "l k[PAD]on [PAD] t[PAD] ro[PAD]l [PAD] l[PAD]e[PAD] [PAD]med [PAD] r a[PAD]i [PAD] s [PAD] e[PAD]r[PAD] d e[PAD] t[PAD] o[PAD]g[PAD] [PAD]r e[PAD] p[PAD]r[PAD]e[PAD] s[PAD] en [PAD] t[PAD] a[PAD] sj o[PAD] n [PAD]s[PAD] r[PAD]e[PAD] n in g[PAD]er [PAD] \n",
      "166\n",
      "h k[PAD]on [PAD] t[PAD] ro[PAD]l [PAD] l [PAD]n med[PAD] r e[PAD] i[PAD] s[PAD] e [PAD] d i[PAD] he[PAD]t[PAD] t[PAD]e o[PAD]g [PAD] [PAD]r[PAD]e[PAD] pr e[PAD] s[PAD]e n[PAD] t [PAD]a[PAD] s j o [PAD] n[PAD] s[PAD] r e g [PAD] n[PAD]i[PAD]ng[PAD] e r[PAD] \n",
      "166\n",
      "a k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l e[PAD] med [PAD] [PAD] r a[PAD]i [PAD] s [PAD] et[PAD] d e[PAD] e [PAD]t [PAD] ta[PAD] [PAD]o[PAD]g [PAD] r e[PAD] pr [PAD]e[PAD] s[PAD]e n[PAD] t[PAD] a[PAD] s j o [PAD] n [PAD]s[PAD] r e [PAD]r[PAD] n i[PAD]ng [PAD]er [PAD] \n",
      "166\n",
      "g[PAD]k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l [PAD]e m ed [PAD] [PAD] r a[PAD] i[PAD] s [PAD] e[PAD] [PAD] di[PAD] e[PAD]t[PAD] t [PAD] o[PAD]g[PAD] [PAD]r e[PAD] pr e [PAD] s [PAD]en [PAD] t[PAD] a[PAD] s j o [PAD] n [PAD]s [PAD] r [PAD]e[PAD]g[PAD] n[PAD]in[PAD]g[PAD] e r[PAD] \n",
      "166\n",
      "t k[PAD]on[PAD] t[PAD]r o[PAD]l[PAD] l e [PAD] med [PAD] r[PAD]e[PAD]i [PAD] s[PAD] r[PAD] d e[PAD] j[PAD] t[PAD] t[PAD]e o[PAD]g [PAD] r e[PAD] pr e[PAD] s[PAD]e n[PAD] t [PAD]a[PAD] s j o [PAD] n [PAD]s [PAD] re [PAD]t[PAD] n[PAD]in[PAD]g [PAD]e[PAD]r[PAD] \n",
      "166\n",
      "p k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] l e[PAD] m ed[PAD] [PAD]r e [PAD] s[PAD] d y[PAD] h[PAD] t[PAD] o[PAD]g[PAD] [PAD]r[PAD]e[PAD] p[PAD]re [PAD] s[PAD] e[PAD]n[PAD] t [PAD]a[PAD] sj o[PAD] n [PAD] s[PAD] r e [PAD]t[PAD] ni n[PAD]g[PAD]e r[PAD] \n",
      "166\n",
      "i k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l[PAD]e [PAD]me[PAD]d[PAD] [PAD]r a[PAD] i[PAD] s[PAD]e [PAD]r[PAD] d u[PAD] j[PAD] t[PAD] t[PAD]e o[PAD]g[PAD] [PAD]r e[PAD] p[PAD]re[PAD] s[PAD] en [PAD] t [PAD]a[PAD] s[PAD]jo [PAD] n [PAD]s[PAD] r[PAD]e[PAD] n i n[PAD]g[PAD] r[PAD] \n",
      "166\n",
      " k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] l [PAD]e m e[PAD]d[PAD] r a[PAD]i [PAD] s [PAD] er[PAD] d i [PAD] t [PAD] te[PAD] o[PAD]g[PAD] r [PAD]e[PAD] p[PAD]r e[PAD] s [PAD]en [PAD] t[PAD] a[PAD] s[PAD]jo[PAD] n[PAD] s[PAD] r [PAD]eig[PAD] ni [PAD]n[PAD]g[PAD]e[PAD]r[PAD] \n",
      "166\n",
      "r[PAD]k[PAD]on[PAD] t[PAD]r o[PAD]l[PAD] le[PAD]e m ed[PAD] [PAD] r[PAD]e[PAD] s [PAD]e t[PAD] d e[PAD] h[PAD]e[PAD]t[PAD] o[PAD]g [PAD] [PAD]r[PAD]e[PAD] pr [PAD]e[PAD] s[PAD]e[PAD]n[PAD] t[PAD]a[PAD] s[PAD]jo [PAD] n [PAD]s [PAD] r e[PAD]v[PAD] ni [PAD]ng [PAD] r[PAD] \n",
      "166\n",
      "t k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] l n [PAD]me[PAD]d[PAD] r[PAD]e [PAD]i[PAD] s[PAD] d i [PAD] je[PAD]t [PAD] e[PAD]e [PAD]o[PAD]g[PAD] r e[PAD] p[PAD]r e[PAD] s [PAD]e[PAD]n[PAD] t [PAD]a[PAD] s j o[PAD] n [PAD]s [PAD] r e [PAD] n[PAD]in g [PAD]e r[PAD] \n",
      "166\n",
      " k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l[PAD] n [PAD]med[PAD] [PAD]r a[PAD] i[PAD] s [PAD] et[PAD] d [PAD] he t[PAD] e[PAD] o[PAD]g[PAD] r[PAD]e[PAD] p[PAD]r[PAD]e[PAD] s[PAD]e n[PAD] t[PAD]a[PAD] s jo [PAD] n[PAD] s[PAD] r e ig[PAD] n i ng [PAD]e[PAD] \n",
      "166\n",
      "e[PAD]k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l [PAD] med [PAD]r[PAD]e[PAD] i[PAD] s[PAD] [PAD] di[PAD] he[PAD]t [PAD] d[PAD] o[PAD]g[PAD] r e[PAD] p[PAD]r e[PAD] s[PAD] e[PAD]n[PAD] t[PAD] a[PAD] s j[PAD]o[PAD] n[PAD] s[PAD] r e[PAD]k[PAD] n i[PAD]ng [PAD]e r[PAD] \n",
      "166\n",
      "k k[PAD]on [PAD] t[PAD]r o[PAD]l [PAD] l[PAD]e[PAD] m ed[PAD] [PAD]r[PAD]e[PAD] s [PAD] [PAD] d i [PAD] h[PAD]e t [PAD] te[PAD] o[PAD]g [PAD] r [PAD]e[PAD] pr e[PAD] s [PAD]e[PAD]n[PAD] t [PAD]a[PAD] s j o[PAD] n [PAD] s[PAD] r[PAD]e[PAD]d[PAD] ni ng [PAD] r[PAD] \n",
      "166\n",
      "r k[PAD]on [PAD] t[PAD] ro[PAD]l[PAD] le[PAD]t m ed[PAD] [PAD] r[PAD]e i [PAD] s[PAD] e[PAD] [PAD] d i[PAD] h[PAD] t [PAD] te[PAD] o[PAD]g [PAD] [PAD]r[PAD]e[PAD] pr e [PAD] s [PAD]en [PAD] t[PAD] a[PAD] s[PAD]jo[PAD] n [PAD] s[PAD] r [PAD]e[PAD]n[PAD] ni[PAD]n g[PAD]e[PAD] \n",
      "166\n",
      "v k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] l [PAD]e m ed[PAD] r [PAD]e [PAD] s[PAD]e r[PAD] [PAD] du[PAD] e t [PAD] t[PAD] o[PAD]g [PAD] [PAD] r e[PAD] p[PAD]r e[PAD] s[PAD] en [PAD] t[PAD] a[PAD] s jo[PAD] n[PAD] s[PAD] r [PAD]e[PAD]g[PAD] ni ng [PAD] r[PAD] \n",
      "166\n",
      "å k[PAD]on [PAD] t[PAD] ro[PAD]l[PAD] l [PAD] med [PAD] r[PAD]e[PAD]i [PAD] s [PAD] e[PAD] d u[PAD] he [PAD]t[PAD] t [PAD] [PAD]o[PAD]g [PAD] [PAD] r[PAD]e[PAD] p[PAD]r[PAD]e[PAD] s[PAD]en [PAD] t[PAD]a[PAD] s[PAD]jo[PAD] n [PAD]s [PAD] r e[PAD]k[PAD] ni[PAD]ng[PAD] er [PAD] \n",
      "166\n",
      "g[PAD]k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l [PAD] m ed[PAD] r [PAD]e [PAD]i[PAD] s [PAD]e r[PAD] d[PAD]i[PAD] e t [PAD] th[PAD] o[PAD]g [PAD] [PAD]r e[PAD] p[PAD]r e[PAD] s[PAD] e[PAD]n[PAD] t [PAD]a[PAD] s jo [PAD] n [PAD] s [PAD] r e [PAD]g[PAD] n i n[PAD]g[PAD] \n",
      "166\n",
      "a[PAD]k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] l e[PAD] med [PAD] r[PAD]e[PAD] s [PAD]e [PAD] [PAD] d i [PAD] j[PAD]et[PAD] d[PAD] [PAD]o[PAD]g[PAD] [PAD]r e[PAD] pr e[PAD] s[PAD] en [PAD] t[PAD]a[PAD] s [PAD]jo [PAD] n [PAD] s[PAD] r [PAD]e[PAD]d[PAD] n[PAD]in g [PAD]e[PAD] \n",
      "166\n",
      "k k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l[PAD]e me[PAD]d[PAD] [PAD]r a[PAD] i[PAD] s[PAD] e[PAD]r[PAD] de[PAD] e t[PAD] t [PAD] o[PAD]g[PAD] r e[PAD] p[PAD]r e[PAD] s[PAD] en [PAD] t[PAD] a[PAD] s j[PAD]o [PAD] n [PAD] s[PAD] r [PAD]e[PAD] n[PAD]i ng[PAD] e[PAD] \n",
      "166\n",
      "å k[PAD]on[PAD] t[PAD]r o[PAD]l[PAD] l [PAD] m ed [PAD] r e i [PAD] s[PAD] e [PAD] d [PAD] et[PAD] ta[PAD] o[PAD]g [PAD] [PAD]r[PAD]e[PAD] pr e[PAD] s [PAD]e[PAD]n[PAD] t [PAD]a[PAD] s j o [PAD] n[PAD] s[PAD] r e [PAD] ni n g[PAD] r[PAD] \n",
      "166\n",
      "p k[PAD]on [PAD] t[PAD]r o[PAD]l[PAD] le[PAD]t med[PAD] r a[PAD] i[PAD] s [PAD]e t[PAD] d i [PAD] et [PAD] t [PAD]o[PAD]g[PAD] r [PAD]e[PAD] p[PAD]r[PAD]e[PAD] s[PAD] en [PAD] t [PAD]a[PAD] s j[PAD]o [PAD] n [PAD] s[PAD] re [PAD]n[PAD] ni [PAD]n g[PAD]e r[PAD] \n",
      "166\n",
      "i k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l [PAD] [PAD]med [PAD] r a[PAD] i[PAD] s[PAD]e [PAD] [PAD]d [PAD] h[PAD] et [PAD] t[PAD] o[PAD]g[PAD] r e[PAD] pr [PAD]e[PAD] s[PAD] en [PAD] t[PAD] a[PAD] s jo [PAD] n[PAD] s[PAD] r[PAD]e[PAD]t[PAD] n[PAD]in[PAD]g[PAD] r[PAD] \n",
      "166\n",
      "r[PAD]k[PAD]on [PAD] t[PAD] ro[PAD]l[PAD] le[PAD]e m ed [PAD] r e[PAD]i [PAD] s[PAD] et[PAD] [PAD] dy[PAD] j[PAD] e[PAD]t[PAD] t [PAD]o[PAD]g [PAD] r e[PAD] p[PAD]r[PAD]e[PAD] s[PAD] e n[PAD] t[PAD] a[PAD] s[PAD]jo[PAD] n[PAD] s[PAD] re [PAD]i[PAD] nin g[PAD] e[PAD]r[PAD] \n",
      "166\n",
      "e[PAD]k[PAD]on[PAD] t[PAD] ro[PAD]l [PAD] l [PAD] [PAD] med r e [PAD] s [PAD]e [PAD] d y[PAD] e [PAD]t [PAD] e[PAD] o[PAD]g[PAD] r e[PAD] p[PAD]r e[PAD] s[PAD]e[PAD]n[PAD] t[PAD] a[PAD] sj[PAD]o[PAD] n [PAD]s[PAD] r [PAD]e[PAD]n[PAD] nin g[PAD] \n",
      "166\n",
      "r[PAD]k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] le[PAD]r [PAD]me[PAD]d[PAD] [PAD]r e[PAD] i[PAD] s [PAD] e [PAD] d u[PAD] je t[PAD] [PAD]o[PAD]g [PAD] r e[PAD] p[PAD]re [PAD] s[PAD] e[PAD]n[PAD] t[PAD]a[PAD] s jo [PAD] n[PAD] s[PAD] re [PAD]y[PAD] n[PAD]i n g[PAD] \n",
      "166\n",
      "e[PAD]k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l n[PAD] [PAD]me[PAD]d[PAD] r e [PAD]i[PAD] s[PAD]e r[PAD] [PAD]d [PAD] e [PAD]t[PAD] e[PAD]e [PAD]o[PAD]g[PAD] r[PAD]e[PAD] p[PAD]r[PAD]e[PAD] s[PAD]e n[PAD] t [PAD]a[PAD] s jo [PAD] n [PAD]s[PAD] r e[PAD]y[PAD] nin [PAD]g[PAD] \n",
      "166\n",
      "e k[PAD]on[PAD] t[PAD]r o[PAD]l [PAD] l e[PAD] me[PAD]d[PAD] [PAD]r e[PAD]i [PAD] s [PAD]e [PAD]r[PAD] d[PAD]i [PAD] h[PAD] t[PAD] t[PAD] o[PAD]g[PAD] [PAD]r e[PAD] pr e[PAD] s[PAD]en [PAD] t[PAD] a[PAD] sj o[PAD] n [PAD] s [PAD] r e[PAD]n[PAD] ni n[PAD]g [PAD] \n",
      "166\n",
      "s k[PAD]on[PAD] t[PAD] ro[PAD]l[PAD] l e[PAD] m e[PAD]d r e[PAD] i[PAD] s[PAD] r[PAD] d i [PAD] h[PAD] et[PAD] ta[PAD] o[PAD]g [PAD] [PAD] r[PAD]e[PAD] p[PAD]re [PAD] s [PAD]e n[PAD] t[PAD]a[PAD] s j o [PAD] n [PAD] s[PAD] r[PAD]e[PAD]r[PAD] n i n g[PAD]e[PAD] \n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    id_list = word_ids[i]\n",
    "    id_list_filtered = filter(lambda num: num != 31, id_list)\n",
    "    chars_temp = model_tokenizer.convert_ids_to_tokens(id_list)\n",
    "    print(len(id_list))\n",
    "    print(re.sub(\" +\", \" \", \"\".join(chars_temp).replace(\"|\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mwer = MWERLoss(vocab_size=32, subsampling_factor=1, reduction=\"mean\")\n",
    "masd_loss = mwer(emissions=log_probs, emissions_lengths=emissions_lengths, labels=labels, labels_length=labels_length)\n",
    "print(masd_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = sample_output[\"logits\"]\n",
    "\n",
    "H = k2.ctc_topo(\n",
    "    max_token=31,\n",
    "    modified=False,\n",
    "    device=emissions.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isym = k2.SymbolTable.from_str('''\n",
    "| 0\n",
    "a 1\n",
    "b 2\n",
    "[PAD] 3\n",
    "''')\n",
    "\n",
    "osym = k2.SymbolTable.from_str('''\n",
    "| 0\n",
    "a 1\n",
    "b 2\n",
    "[PAD] 3\n",
    "''')\n",
    "\n",
    "try_topo = k2.ctc_topo(max_token=3, modified=False)\n",
    "\n",
    "try_topo.labels_sym = isym\n",
    "try_topo.aux_labels_sym = osym\n",
    "\n",
    "try_topo.draw('try_topo.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_texts(\n",
    "    best_paths: k2.Fsa, return_ragged: bool = False\n",
    ") -> Union[List[List[int]], k2.RaggedTensor]:\n",
    "    \"\"\"Extract the texts (as word IDs) from the best-path FSAs.\n",
    "\n",
    "    Note:\n",
    "        Used by Nbest.build_levenshtein_graphs during MWER computation.\n",
    "        Copied from icefall.\n",
    "\n",
    "    Args:\n",
    "      best_paths:\n",
    "        A k2.Fsa with best_paths.arcs.num_axes() == 3, i.e.\n",
    "        containing multiple FSAs, which is expected to be the result\n",
    "        of k2.shortest_path (otherwise the returned values won't\n",
    "        be meaningful).\n",
    "      return_ragged:\n",
    "        True to return a ragged tensor with two axes [utt][word_id].\n",
    "        False to return a list-of-list word IDs.\n",
    "    Returns:\n",
    "      Returns a list of lists of int, containing the label sequences we\n",
    "      decoded.\n",
    "    \"\"\"\n",
    "    if isinstance(best_paths.aux_labels, k2.RaggedTensor):\n",
    "        # remove 0's and -1's.\n",
    "        aux_labels = best_paths.aux_labels.remove_values_leq(0)\n",
    "        # TODO: change arcs.shape() to arcs.shape\n",
    "        aux_shape = best_paths.arcs.shape().compose(aux_labels.shape)\n",
    "\n",
    "        # remove the states and arcs axes.\n",
    "        aux_shape = aux_shape.remove_axis(1)\n",
    "        aux_shape = aux_shape.remove_axis(1)\n",
    "        aux_labels = k2.RaggedTensor(aux_shape, aux_labels.values)\n",
    "    else:\n",
    "        # remove axis corresponding to states.\n",
    "        aux_shape = best_paths.arcs.shape().remove_axis(1)\n",
    "        aux_labels = k2.RaggedTensor(aux_shape, best_paths.aux_labels)\n",
    "        # remove 0's and -1's.\n",
    "        aux_labels = aux_labels.remove_values_leq(0)\n",
    "\n",
    "    assert aux_labels.num_axes == 2\n",
    "    if return_ragged:\n",
    "        return aux_labels\n",
    "    else:\n",
    "        return aux_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''used in MWER loss forward function to get the hyps, function copied from k2/python/k2/nbest.py'''\n",
    "def build_levenshtein_graphs(self) -> k2.Fsa:\n",
    "    \"\"\"Return an FsaVec with axes [utt][state][arc].\"\"\"\n",
    "    word_ids = _get_texts(self.fsa, return_ragged=True)  # get_texts function copied in above!!!\n",
    "    return k2.levenshtein_graph(word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Nbest CTC Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import ctc_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_CTCloss_nbest(reference_text, output_logits, input_lengths, asd_model, asd_tokenizer):\n",
    "    decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=100, blank_token=\"[PAD]\",\n",
    "                          sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "    targets = []\n",
    "    target_lengths = []\n",
    "    log_probs = F.log_softmax(output_logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n",
    "\n",
    "    for i in range(len(reference_text)):\n",
    "        ref_text = reference_text[i].replace(\"[UNK]\", \"\")\n",
    "        logits = output_logits[i]\n",
    "        # get nbest hypotheses and rank them\n",
    "        nbest_list = decoder(logits.type(torch.float32).detach().cpu()[None, :, :])\n",
    "        nbest_token_list = []\n",
    "        asd_score_list = [0] * len(nbest_list[0])\n",
    "        hyp_list = []\n",
    "        for j, item in enumerate(nbest_list[0]):\n",
    "            tokens = item.tokens\n",
    "            for k in range(len(tokens)):\n",
    "                if tokens[k] == 0:\n",
    "                    tokens_mod = tokens[k+1:]\n",
    "                else:\n",
    "                    break\n",
    "            chars = decoder.idxs_to_tokens(tokens_mod)\n",
    "            nbest_token_list.append(tokens_mod)\n",
    "            hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "            hyp_list.append(hyp_text)\n",
    "            asd_score_list[j] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "        targets.append(torch.tensor(nbest_token_list[np.argmin(asd_score_list)]))\n",
    "        target_lengths.append(torch.tensor(len(nbest_token_list[np.argmin(asd_score_list)])))\n",
    "\n",
    "    targets_tensor = torch.cat(targets, dim=0)\n",
    "    targets_len_tensor = torch.tensor(target_lengths)\n",
    "\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "        loss = F.ctc_loss(\n",
    "                log_probs,\n",
    "                targets_tensor,\n",
    "                input_lengths,\n",
    "                targets_len_tensor,\n",
    "                blank=31,\n",
    "                reduction=\"mean\",\n",
    "                zero_infinity=True,\n",
    "                )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=100, blank_token=\"[PAD]\",\n",
    "                          sil_token=\"|\", unk_word=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = sample_output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "nbest_list = decoder(logits.type(torch.float32).detach().cpu()[None, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-15.1994, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  5.2293,  -0.6872,  -4.2354,  ...,   3.9036, -15.1366, -15.2423],\n",
      "        [  7.3447,  -1.7595,  -4.6908,  ...,   5.4498, -15.2072, -15.2022],\n",
      "        [ -1.0451,  -0.9015,  -3.7738,  ...,   1.9222, -12.4645, -12.4136],\n",
      "        ...,\n",
      "        [ -4.7462,  -2.5563,  -5.0150,  ...,   9.7356, -14.3328, -14.3965],\n",
      "        [ -3.1324,  -2.8448,  -5.0748,  ...,   9.3147, -14.6481, -14.7300],\n",
      "        [ 11.1722,  -2.1108,  -3.5456,  ...,   3.0746, -15.1404, -15.1994]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([166, 34])\n",
      "tensor(11.1722, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(logits.shape)\n",
    "print(logits[165,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kontroll med reisediett og representasjonsregninger \n",
      " kontroll med reisediett og representasjonsregninger \n",
      " kontroll med reisediette og representasjonsregninger \n",
      " kontroll med reise diett og representasjonsregninger \n",
      " kontroll med reisediette og representasjonsregninger \n",
      " kontroll med reise diett og representasjonsregninger \n",
      " kontroll med reisediett og representasjonsreininger \n",
      " r kontroll med reisediett og representasjonsregninger \n",
      " kontroll med reisedeett og representasjonsregninger \n",
      " kontroll med reisedyett og representasjonsregninger \n",
      "tensor(2857.0664, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(285.7066, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nbest_path_probs = torch.zeros((len(nbest_list[0])), requires_grad=True, device=device).double()\n",
    "\n",
    "for j, item in enumerate(nbest_list[0]):\n",
    "    path_probs = torch.zeros((len(item.tokens)), requires_grad=True, device=device).double()\n",
    "    chars = decoder.idxs_to_tokens(item.tokens)\n",
    "    hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "    print(hyp_text)\n",
    "    for i, token in enumerate(item.tokens):\n",
    "        # if item.timesteps[i] < logits.shape[0]:\n",
    "        if i < len(item.timesteps) - 1:\n",
    "            start = item.timesteps[i]\n",
    "            end = item.timesteps[i+1]\n",
    "            path_probs[i] = logits[start:end,token].sum()\n",
    "        else:\n",
    "            path_probs[i] = logits[-1,token]\n",
    "    nbest_path_probs[j] = path_probs.sum()\n",
    "\n",
    "\n",
    "print(nbest_path_probs.sum())\n",
    "print(nbest_path_probs.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nbest_asd(reference_text, output_logits, asd_model, asd_tokenizer, reduction=\"mean\"):\n",
    "    decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=100, blank_token=\"[PAD]\",\n",
    "                          sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "    loss = torch.zeros((len(reference_text)), requires_grad=True, device=device).double()\n",
    "    for i in range(len(reference_text)):\n",
    "        ref_text = reference_text[i].replace(\"[UNK]\", \"\")\n",
    "        logits = output_logits[i]\n",
    "        # get nbest hypotheses and get path log probs * asd score\n",
    "        nbest_list = decoder(logits.type(torch.float32).detach().cpu()[None, :, :])\n",
    "        nbest_asd_loss = torch.zeros((len(nbest_list[0])), requires_grad=True, device=device).double()\n",
    "        for j, item in enumerate(nbest_list[0]):\n",
    "            path_probs = torch.zeros((len(item.tokens)), requires_grad=True, device=device).double()\n",
    "            chars = decoder.idxs_to_tokens(item.tokens)\n",
    "            hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "            for k, token in enumerate(item.tokens):\n",
    "                if item.timesteps[k] < logits.shape[0]:\n",
    "                    start = item.timesteps[k]\n",
    "                    end = item.timesteps[k+1]\n",
    "                    path_probs[k] = logits[start:end,token].sum()\n",
    "                else:\n",
    "                    path_probs[k] = logits[-1,token]\n",
    "            nbest_asd_loss[j] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text) * path_probs.sum()\n",
    "        loss[i] = nbest_asd_loss.mean()\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k2_trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
