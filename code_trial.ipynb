{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, concatenate_datasets\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM\n",
    "from datasets import Dataset, load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "# chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "def load_dataset_from_files(data_dir_list:list[str], csv_export_dir:str, split_ratio=0.1, csv_export=True):\n",
    "    frames = []\n",
    "    for path in data_dir_list:\n",
    "        source = os.path.basename(os.path.dirname(path))\n",
    "        wavfile_data = []\n",
    "        textfile_data = []\n",
    "        for (root, dirs, files) in os.walk(path, topdown=True):\n",
    "            if source == \"Rundkast\":  # to modify depending on Rundkast cuts folder name\n",
    "                for fn in files:\n",
    "                    if fn.endswith(\".wav\"):\n",
    "                        wav_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        path = os.path.join(root, fn)\n",
    "                        wavfile_data.append((wav_id, fn, path, source))\n",
    "                    elif fn.endswith(\".txt\"):\n",
    "                        text_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        with open(os.path.join(root, fn), encoding=\"utf-8\") as text_file:\n",
    "                            text = text_file.read()\n",
    "                        textfile_data.append((text_id, text))\n",
    "            else:\n",
    "                for fn in files:\n",
    "                    if fn.endswith(\".wav\"):\n",
    "                        wav_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        path = os.path.join(root, fn)\n",
    "                        wavfile_data.append((wav_id, fn, path, source))\n",
    "                    elif fn.endswith(\".txt-utf8\"):\n",
    "                        text_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        with open(os.path.join(root, fn), encoding=\"utf-8-sig\") as text_file:\n",
    "                            text = text_file.read()\n",
    "                        textfile_data.append((text_id, text))\n",
    "        df_wav = pd.DataFrame(wavfile_data, columns=[\"segment_id\", \"wav_file\", \"path\", \"source\"])\n",
    "        df_wav = df_wav.set_index(\"segment_id\")\n",
    "        df_text = pd.DataFrame(textfile_data, columns=[\"segment_id\", \"text\"])\n",
    "        df_text = df_text.set_index(\"segment_id\")\n",
    "        dataset_df = df_wav.merge(df_text, left_index=True, right_index=True)\n",
    "        frames.append(dataset_df)\n",
    "    # concat to full dataframe and convert to Dataset with special characters removed\n",
    "    full_dataset_df = pd.concat(frames)\n",
    "    raw_dataset = Dataset.from_pandas(full_dataset_df)\n",
    "    raw_dataset = raw_dataset.map(remove_special_characters)\n",
    "    # split dataset\n",
    "    raw_dataset = raw_dataset.train_test_split(test_size=split_ratio)\n",
    "    # save copy of dataset\n",
    "    if csv_export is True:\n",
    "        df_train = pd.DataFrame(raw_dataset[\"train\"])\n",
    "        df_train.to_csv(os.path.join(csv_export_dir, \"train_set.csv\"))\n",
    "        df_dev = pd.DataFrame(raw_dataset[\"test\"])\n",
    "        df_dev.to_csv(os.path.join(csv_export_dir, \"dev_set.csv\"))\n",
    "    # loading audio\n",
    "    dataset = raw_dataset.cast_column(\"path\", Audio())\n",
    "    dataset = dataset.rename_column(\"path\", \"audio\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    # preprocess dataset\n",
    "    # dataset = dataset.map(prepare_dataset,\n",
    "    #                       remove_columns=dataset.column_names[\"train\"],\n",
    "    #                       num_proc=4)\n",
    "    return raw_dataset, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_list = [\"../../datasets/NordTrans_TUL/train_small/Stortinget/\",\n",
    "                 \"../../datasets/NordTrans_TUL/train_small/NRK/\",\n",
    "                 \"../../datasets/NordTrans_TUL/train_small/Rundkast/\"]\n",
    "\n",
    "# data_dir_list = [\"../../datasets/NordTrans_TUL/train_small/Stortinget/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24300/24300 [00:01<00:00, 21744.88ex/s]\n"
     ]
    }
   ],
   "source": [
    "csv_export_dir = \"./code_trial/\"\n",
    "\n",
    "raw_dataset, dataset = load_dataset_from_files(data_dir_list, csv_export_dir, split_ratio=0.1, csv_export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21870/21870 [00:01<00:00, 15073.81ex/s]\n",
      "100%|██████████| 2430/2430 [00:00<00:00, 14730.58ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['wav_file', 'audio', 'source', 'text', 'segment_id'],\n",
       "        num_rows: 21870\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['wav_file', 'audio', 'source', 'text', 'segment_id'],\n",
       "        num_rows: 2430\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wav_file': '12232017_001.wav',\n",
       " 'audio': {'path': '../../datasets/NordTrans_TUL/train_small/NRK/12232017_001.wav',\n",
       "  'array': array([0.02038574, 0.01919556, 0.01751709, ..., 0.00527954, 0.00469971,\n",
       "         0.00375366], dtype=float32),\n",
       "  'sampling_rate': 16000},\n",
       " 'source': 'NRK',\n",
       " 'text': 'gjøre mine vurderinger ',\n",
       " 'segment_id': 'NRK_12232017_001'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "# model_name = \"KBLab/wav2vec2-large-voxrex\"\n",
    "# processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# dataset = dataset.map(prepare_dataset,\n",
    "#                         remove_columns=dataset.column_names[\"train\"],\n",
    "#                         num_proc=4)\n",
    "inputs = processor(sample[\"audio\"][\"array\"], sampling_rate=sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "label = sample[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/wav2vec2-large-voxrex were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at KBLab/wav2vec2-large-voxrex and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"KBLab/wav2vec2-large-voxrex\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1931, -0.2910, -0.3430,  ...,  0.3020,  0.2800, -0.1646],\n",
       "         [ 0.1936, -0.2956, -0.3336,  ...,  0.2900,  0.2811, -0.1608],\n",
       "         [ 0.1902, -0.2981, -0.3339,  ...,  0.2827,  0.2760, -0.1666],\n",
       "         ...,\n",
       "         [ 0.1883, -0.3031, -0.3290,  ...,  0.2887,  0.2919, -0.1822],\n",
       "         [ 0.1988, -0.3122, -0.3347,  ...,  0.2893,  0.2938, -0.1748],\n",
       "         [ 0.2029, -0.3056, -0.3337,  ...,  0.2898,  0.2984, -0.1819]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input logits of size 32, but vocabulary is size 34",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/pyctcdecode/decoder.py\", line 549, in _decode_beams_mp_safe\n    decoded_beams = self.decode_beams(\n  File \"/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/pyctcdecode/decoder.py\", line 514, in decode_beams\n    raise ValueError(\nValueError: Input logits of size 32, but vocabulary is size 34\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m      2\u001b[0m transcription[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:415\u001b[0m, in \u001b[0;36mWav2Vec2ProcessorWithLM.batch_decode\u001b[0;34m(self, logits, pool, num_processes, beam_width, beam_prune_logp, token_min_logp, hotwords, hotword_weight, alpha, beta, unk_score_offset, lm_score_boundary, output_word_offsets)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# pyctcdecode\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cm:\n\u001b[0;32m--> 415\u001b[0m     decoded_beams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_beams_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_prune_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_prune_logp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_min_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_min_logp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhotwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhotwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhotword_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhotword_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# extract text and scores\u001b[39;00m\n\u001b[1;32m    426\u001b[0m batch_texts, logit_scores, lm_scores, word_offsets \u001b[38;5;241m=\u001b[39m [], [], [], []\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/pyctcdecode/decoder.py:599\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC.decode_beams_batch\u001b[0;34m(self, pool, logits_list, beam_width, beam_prune_logp, token_min_logp, prune_history, hotwords, hotword_weight)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m\"\"\"Use multi processing pool to batch decode input logits.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m    List of list of beams of type OUTPUT_BEAM_MP_SAFE with various meta information\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    590\u001b[0m p_decode \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_beams_mp_safe,\n\u001b[1;32m    592\u001b[0m     beam_width\u001b[38;5;241m=\u001b[39mbeam_width,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    597\u001b[0m     hotword_weight\u001b[38;5;241m=\u001b[39mhotword_weight,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m decoded_beams_list: List[List[OutputBeamMPSafe]] \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_decode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_beams_list\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec_v2/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec_v2/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: Input logits of size 32, but vocabulary is size 34"
     ]
    }
   ],
   "source": [
    "transcription = processor.batch_decode(logits.numpy()).text\n",
    "transcription[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 10, 10, 10, 10, 10, 10, 10, 24, 10, 10, 10, 10, 10, 24, 10, 10, 10,\n",
      "         10, 10, 10, 24, 24, 24, 24, 24, 24, 24, 10, 10, 29, 10, 24, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 24, 10, 29, 29, 24, 10, 29, 24, 24, 24, 24, 24,\n",
      "         10, 10, 10, 10, 10, 10]])\n",
      "['xjxjxjxjøjxjxjøxjøxj']\n",
      "gjøre mine vurderinger \n"
     ]
    }
   ],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "print(predicted_ids)\n",
    "print(transcription)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.61ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.59ba/s]\n"
     ]
    }
   ],
   "source": [
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': 0,\n",
       " 'b': 1,\n",
       " '1': 2,\n",
       " 'k': 3,\n",
       " 'è': 4,\n",
       " \"'\": 5,\n",
       " '3': 6,\n",
       " 'ó': 7,\n",
       " 'r': 8,\n",
       " 'v': 9,\n",
       " 't': 10,\n",
       " 'u': 11,\n",
       " 'l': 12,\n",
       " 'å': 13,\n",
       " 'p': 14,\n",
       " 'm': 15,\n",
       " 'æ': 16,\n",
       " 'f': 17,\n",
       " 'w': 18,\n",
       " 'c': 19,\n",
       " 'o': 20,\n",
       " 'í': 21,\n",
       " 'n': 22,\n",
       " ' ': 23,\n",
       " '–': 24,\n",
       " '4': 25,\n",
       " 'ü': 26,\n",
       " 'd': 27,\n",
       " 'g': 28,\n",
       " 'z': 29,\n",
       " 'i': 30,\n",
       " 'e': 31,\n",
       " '`': 32,\n",
       " 'q': 33,\n",
       " 'ö': 34,\n",
       " 'y': 35,\n",
       " 'ä': 36,\n",
       " 'a': 37,\n",
       " 'x': 38,\n",
       " '6': 39,\n",
       " 'á': 40,\n",
       " 'h': 41,\n",
       " 'ø': 42,\n",
       " 'é': 43,\n",
       " 'j': 44,\n",
       " '9': 45,\n",
       " 'ò': 46,\n",
       " 's': 47}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(i, processor.tokenizer.convert_ids_to_tokens(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
