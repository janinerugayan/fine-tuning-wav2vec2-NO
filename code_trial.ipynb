{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, concatenate_datasets\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM\n",
    "from datasets import Dataset, load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "# chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "def load_dataset_from_files(data_dir_list:list[str], csv_export_dir:str, split_ratio=0.1, csv_export=True):\n",
    "    frames = []\n",
    "    for path in data_dir_list:\n",
    "        source = os.path.basename(os.path.dirname(path))\n",
    "        wavfile_data = []\n",
    "        textfile_data = []\n",
    "        for (root, dirs, files) in os.walk(path, topdown=True):\n",
    "            if source == \"Rundkast\":  # to modify depending on Rundkast cuts folder name\n",
    "                for fn in files:\n",
    "                    if fn.endswith(\".wav\"):\n",
    "                        wav_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        path = os.path.join(root, fn)\n",
    "                        wavfile_data.append((wav_id, fn, path, source))\n",
    "                    elif fn.endswith(\".txt\"):\n",
    "                        text_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        with open(os.path.join(root, fn), encoding=\"utf-8\") as text_file:\n",
    "                            text = text_file.read()\n",
    "                        textfile_data.append((text_id, text))\n",
    "            else:\n",
    "                for fn in files:\n",
    "                    if fn.endswith(\".wav\"):\n",
    "                        wav_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        path = os.path.join(root, fn)\n",
    "                        wavfile_data.append((wav_id, fn, path, source))\n",
    "                    elif fn.endswith(\".txt-utf8\"):\n",
    "                        text_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        with open(os.path.join(root, fn), encoding=\"utf-8-sig\") as text_file:\n",
    "                            text = text_file.read()\n",
    "                        textfile_data.append((text_id, text))\n",
    "        df_wav = pd.DataFrame(wavfile_data, columns=[\"segment_id\", \"wav_file\", \"path\", \"source\"])\n",
    "        df_wav = df_wav.set_index(\"segment_id\")\n",
    "        df_text = pd.DataFrame(textfile_data, columns=[\"segment_id\", \"text\"])\n",
    "        df_text = df_text.set_index(\"segment_id\")\n",
    "        dataset_df = df_wav.merge(df_text, left_index=True, right_index=True)\n",
    "        frames.append(dataset_df)\n",
    "    # concat to full dataframe and convert to Dataset with special characters removed\n",
    "    full_dataset_df = pd.concat(frames)\n",
    "    raw_dataset = Dataset.from_pandas(full_dataset_df)\n",
    "    raw_dataset = raw_dataset.map(remove_special_characters)\n",
    "    # split dataset\n",
    "    raw_dataset = raw_dataset.train_test_split(test_size=split_ratio)\n",
    "    # save copy of dataset\n",
    "    if csv_export is True:\n",
    "        df_train = pd.DataFrame(raw_dataset[\"train\"])\n",
    "        df_train.to_csv(os.path.join(csv_export_dir, \"train_set.csv\"))\n",
    "        df_dev = pd.DataFrame(raw_dataset[\"test\"])\n",
    "        df_dev.to_csv(os.path.join(csv_export_dir, \"dev_set.csv\"))\n",
    "    # loading audio\n",
    "    dataset = raw_dataset.cast_column(\"path\", Audio())\n",
    "    dataset = dataset.rename_column(\"path\", \"audio\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    # preprocess dataset\n",
    "    # dataset = dataset.map(prepare_dataset,\n",
    "    #                       remove_columns=dataset.column_names[\"train\"],\n",
    "    #                       num_proc=4)\n",
    "    return raw_dataset, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_list = [\"../../datasets/NordTrans_TUL/train_small/Stortinget/\",\n",
    "                 \"../../datasets/NordTrans_TUL/train_small/NRK/\",\n",
    "                 \"../../datasets/NordTrans_TUL/train_small/Rundkast/\"]\n",
    "\n",
    "# data_dir_list = [\"../../datasets/NordTrans_TUL/train_small/Stortinget/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24300/24300 [00:01<00:00, 21981.72ex/s]\n"
     ]
    }
   ],
   "source": [
    "csv_export_dir = \"./code_trial/\"\n",
    "\n",
    "raw_dataset, dataset = load_dataset_from_files(data_dir_list, csv_export_dir, split_ratio=0.1, csv_export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21870/21870 [00:01<00:00, 14693.13ex/s]\n",
      "100%|██████████| 2430/2430 [00:00<00:00, 15024.86ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['wav_file', 'audio', 'source', 'text', 'segment_id'],\n",
       "        num_rows: 21870\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['wav_file', 'audio', 'source', 'text', 'segment_id'],\n",
       "        num_rows: 2430\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1665.07it/s]\n",
      "\n",
      "#2:   0%|          | 0/5467 [00:00<?, ?ex/s]\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   0%|          | 1/5468 [00:00<1:22:33,  1.10ex/s]/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   2%|▏         | 123/5468 [00:01<00:32, 166.99ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   5%|▍         | 249/5468 [00:01<00:15, 344.40ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   7%|▋         | 373/5468 [00:01<00:09, 512.85ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   9%|▉         | 498/5468 [00:01<00:07, 668.24ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  11%|█▏        | 621/5468 [00:01<00:06, 798.23ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  14%|█▎        | 749/5468 [00:01<00:05, 916.63ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  16%|█▌        | 878/5468 [00:01<00:04, 1013.68ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  18%|█▊        | 1001/5468 [00:02<00:09, 476.13ex/s]\n",
      "#0:  21%|██        | 1141/5468 [00:02<00:07, 611.93ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  24%|██▎       | 1286/5468 [00:02<00:05, 757.02ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  26%|██▌       | 1424/5468 [00:02<00:04, 880.03ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  29%|██▊       | 1568/5468 [00:02<00:03, 1002.50ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  31%|███▏      | 1711/5468 [00:02<00:03, 1103.90ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  34%|███▍      | 1855/5468 [00:02<00:03, 1187.89ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  37%|███▋      | 1997/5468 [00:02<00:02, 1246.87ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  39%|███▉      | 2135/5468 [00:03<00:05, 660.32ex/s] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  42%|████▏     | 2277/5468 [00:03<00:04, 788.06ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  44%|████▍     | 2420/5468 [00:03<00:03, 911.78ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  47%|████▋     | 2559/5468 [00:03<00:02, 1014.58ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  49%|████▉     | 2695/5468 [00:03<00:02, 1095.70ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  52%|█████▏    | 2828/5468 [00:03<00:02, 1154.33ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  54%|█████▍    | 2965/5468 [00:03<00:02, 1210.64ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  57%|█████▋    | 3098/5468 [00:04<00:04, 564.35ex/s] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  59%|█████▉    | 3240/5468 [00:04<00:03, 693.55ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  62%|██████▏   | 3386/5468 [00:04<00:02, 828.74ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  65%|██████▍   | 3532/5468 [00:04<00:02, 955.91ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  67%|██████▋   | 3675/5468 [00:04<00:01, 1060.42ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  70%|██████▉   | 3818/5468 [00:04<00:01, 1148.40ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  72%|███████▏  | 3964/5468 [00:05<00:01, 1227.97ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  75%|███████▌  | 4103/5468 [00:05<00:01, 794.27ex/s] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  78%|███████▊  | 4243/5468 [00:05<00:01, 910.91ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  80%|████████  | 4385/5468 [00:05<00:01, 1020.52ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  83%|████████▎ | 4528/5468 [00:05<00:00, 1115.12ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  85%|████████▌ | 4674/5468 [00:05<00:00, 1200.72ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  88%|████████▊ | 4811/5468 [00:05<00:00, 1245.26ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  91%|█████████ | 4950/5468 [00:05<00:00, 1283.13ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  93%|█████████▎| 5087/5468 [00:06<00:00, 815.68ex/s] \n",
      "#0:  96%|█████████▌| 5225/5468 [00:06<00:00, 928.31ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  98%|█████████▊| 5368/5468 [00:06<00:00, 1039.31ex/s]\n",
      "\n",
      "#0: 100%|██████████| 5468/5468 [00:06<00:00, 832.40ex/s] \n",
      "#2: 100%|██████████| 5467/5467 [00:06<00:00, 851.18ex/s] \n",
      "\n",
      "\n",
      "#1: 100%|██████████| 5468/5468 [00:06<00:00, 842.03ex/s] \n",
      "#3: 100%|██████████| 5467/5467 [00:06<00:00, 851.96ex/s] \n",
      "#0:   0%|          | 0/608 [00:00<?, ?ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:  19%|█▉        | 118/608 [00:00<00:02, 239.79ex/s]/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:  41%|████      | 248/608 [00:00<00:00, 478.16ex/s]/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  62%|██████▏   | 377/608 [00:00<00:00, 674.80ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  83%|████████▎ | 506/608 [00:00<00:00, 833.07ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0: 100%|██████████| 608/608 [00:01<00:00, 584.36ex/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#2: 100%|██████████| 607/607 [00:01<00:00, 589.98ex/s]\n",
      "#1: 100%|██████████| 608/608 [00:01<00:00, 545.01ex/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 607/607 [00:01<00:00, 556.50ex/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "\n",
    "dataset = dataset.map(prepare_dataset,\n",
    "                        remove_columns=dataset.column_names[\"train\"],\n",
    "                        num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 21870\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 2430\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.61ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.59ba/s]\n"
     ]
    }
   ],
   "source": [
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': 0,\n",
       " 'b': 1,\n",
       " '1': 2,\n",
       " 'k': 3,\n",
       " 'è': 4,\n",
       " \"'\": 5,\n",
       " '3': 6,\n",
       " 'ó': 7,\n",
       " 'r': 8,\n",
       " 'v': 9,\n",
       " 't': 10,\n",
       " 'u': 11,\n",
       " 'l': 12,\n",
       " 'å': 13,\n",
       " 'p': 14,\n",
       " 'm': 15,\n",
       " 'æ': 16,\n",
       " 'f': 17,\n",
       " 'w': 18,\n",
       " 'c': 19,\n",
       " 'o': 20,\n",
       " 'í': 21,\n",
       " 'n': 22,\n",
       " ' ': 23,\n",
       " '–': 24,\n",
       " '4': 25,\n",
       " 'ü': 26,\n",
       " 'd': 27,\n",
       " 'g': 28,\n",
       " 'z': 29,\n",
       " 'i': 30,\n",
       " 'e': 31,\n",
       " '`': 32,\n",
       " 'q': 33,\n",
       " 'ö': 34,\n",
       " 'y': 35,\n",
       " 'ä': 36,\n",
       " 'a': 37,\n",
       " 'x': 38,\n",
       " '6': 39,\n",
       " 'á': 40,\n",
       " 'h': 41,\n",
       " 'ø': 42,\n",
       " 'é': 43,\n",
       " 'j': 44,\n",
       " '9': 45,\n",
       " 'ò': 46,\n",
       " 's': 47}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(i, processor.tokenizer.convert_ids_to_tokens(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
