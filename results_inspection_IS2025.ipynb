{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"nbest_log_dist.pkl\", \"rb\") as file:\n",
    "    nbest_log_distribution = pickle.load(file)\n",
    "\n",
    "with open(\"asd_scores.pkl\", \"rb\") as file:\n",
    "    asd_scores = pickle.load(file)\n",
    "\n",
    "def compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=True):\n",
    "    # Computes log distribution\n",
    "    # (n, b) -> (b,): log( p1+p2+...+pn ) = log( exp(log_p1)+exp(log_p2)+...+exp(log_pn) )\n",
    "    sum_nbest_log_distribution = torch.logsumexp(nbest_log_distribution, 0)\n",
    "    print(\"nbest_log_distribution:\", nbest_log_distribution.size(), nbest_log_distribution)\n",
    "    print(\"sum_nbest_log_distribution:\", sum_nbest_log_distribution.size(), sum_nbest_log_distribution)\n",
    "\n",
    "    # Re-normalized over just the N-best hypotheses.\n",
    "    # (n, b) - (b,) -> (n, b): exp(log_p)/exp(log_p_sum) = exp(log_p-log_p_sum)\n",
    "    normal_nbest_distribution = torch.exp(nbest_log_distribution - sum_nbest_log_distribution)\n",
    "    print(\"normal_nbest_distribution:\", normal_nbest_distribution.size(), normal_nbest_distribution)\n",
    "\n",
    "    if normalized_asd == True:\n",
    "        mean_asd = torch.mean(asd_scores, 0)\n",
    "        asd_norm = asd_scores - mean_asd\n",
    "        print(\"mean_asd:\", mean_asd.size(), mean_asd)\n",
    "        print(\"asd_norm:\", asd_norm.size(), asd_norm)\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_norm, 0)\n",
    "    else:\n",
    "        asd_loss = torch.sum(normal_nbest_distribution * asd_scores, 0)\n",
    "\n",
    "    return asd_loss\n",
    "\n",
    "asd_loss = compute_masd_loss_ver2(nbest_log_distribution, asd_scores, normalized_asd=False)\n",
    "print(\"asd_loss:\", asd_loss)\n",
    "print(\"asd_loss_mean:\", torch.mean(asd_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Set Average Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuning_wav2vec2_asd_trial74_masd_6ep_allDataSmall_aulus6\n",
      "WER: 0.11566186522201408\n",
      "ASD: 0.0912799769379054\n",
      "\n",
      "\n",
      "fine-tuning_wav2vec2_asd_trial71_masd_6ep_allDataSmall_aulus7\n",
      "WER: 0.11452481871106118\n",
      "ASD: 0.08825652911269921\n",
      "\n",
      "\n",
      "fine-tuning_wav2vec2_asd_trial68_masd_6ep_allDataSmall_aulus6\n",
      "WER: 0.11638675984370303\n",
      "ASD: 0.08855505280725691\n",
      "\n",
      "\n",
      "fine-tuning_wav2vec2_asd_trial60_masd_6ep_allDataSmall_titan1\n",
      "WER: 0.10980625643666322\n",
      "ASD: 0.0863335966812446\n",
      "\n",
      "\n",
      "fine-tuning_wav2vec2_asd_trial69_masd_6ep_allDataSmall_aulus6\n",
      "WER: 0.1111965325234118\n",
      "ASD: 0.08987302913503997\n",
      "\n",
      "\n",
      "fine-tuning_wav2vec2_asd_origloss_6ep_allDataSmall_aulus7\n",
      "WER: 0.10637592788971369\n",
      "ASD: 0.08539109747078852\n",
      "\n",
      "\n",
      "fine-tuning_wav2vec2_asd_trial76_masd_6ep_allDataSmall_aulus6\n",
      "WER: 0.9235293371231347\n",
      "ASD: 0.5762788391343856\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df1 = pd.read_csv(\"../../datasets/IS25_devset/dev_set.csv\")\n",
    "# df2 = pd.read_csv(\"../../datasets/IS25_devset/trial74_dev_set.csv\")\n",
    "# diff = df1.compare(df2)\n",
    "# print(diff)\n",
    "\n",
    "def get_log_info(log_history_path):\n",
    "    wer_list = []\n",
    "    asd_list = []\n",
    "    with open(log_history_path, \"r\") as f:\n",
    "        data = f.read().split(\"}, {\")\n",
    "    for item in data:\n",
    "        log_item = (item.split(\", \"))\n",
    "        for x in log_item:\n",
    "            if \"eval_wer\" in x:\n",
    "                wer_list.append(float(x.split(\":\")[1].strip()))\n",
    "            elif \"eval_asd\" in x:\n",
    "                asd_list.append(float(x.split(\":\")[1].strip()))\n",
    "\n",
    "    wer_arr = np.array(wer_list)\n",
    "    asd_arr = np.array(asd_list)\n",
    "    print(\"WER:\", np.mean(wer_arr))\n",
    "    print(\"ASD:\", np.mean(asd_arr))\n",
    "\n",
    "log_history_paths = [\"../../model_ckpts/fine-tuning_wav2vec2_asd_trial74_masd_6ep_allDataSmall_aulus6/runs/log_history.txt\",\n",
    "                     \"../../model_ckpts/fine-tuning_wav2vec2_asd_trial71_masd_6ep_allDataSmall_aulus7/runs/log_history.txt\",\n",
    "                     \"../../model_ckpts/fine-tuning_wav2vec2_asd_trial68_masd_6ep_allDataSmall_aulus6/runs/log_history.txt\",\n",
    "                     \"../../model_ckpts/fine-tuning_wav2vec2_asd_trial60_masd_6ep_allDataSmall_titan1/runs/log_history.txt\",\n",
    "                     \"../../model_ckpts/fine-tuning_wav2vec2_asd_trial69_masd_6ep_allDataSmall_aulus6/runs/log_history.txt\",\n",
    "                     \"../../model_ckpts/fine-tuning_wav2vec2_asd_origloss_6ep_allDataSmall_aulus7/runs/log_history.txt\",\n",
    "                     \"../../model_ckpts/fine-tuning_wav2vec2_asd_trial76_masd_6ep_allDataSmall_aulus6/runs/log_history.txt\"]\n",
    "\n",
    "for path in log_history_paths:\n",
    "    print(path.split(\"/\")[3])\n",
    "    get_log_info(path)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB Tale Datset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_0</td>\n",
       "      <td>p1_g02_m2_5_x-free_cut_0.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>jeg er en førtifire år gammel mann som er veld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_1</td>\n",
       "      <td>p1_g02_m2_5_x-free_cut_1.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>jeg trener ungdommer og jeg er også spinningin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_10</td>\n",
       "      <td>p1_g02_m2_5_x-free_cut_10.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>ei ut av de dalsluktene så plutselig kom vi ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_11</td>\n",
       "      <td>p1_g02_m2_5_x-free_cut_11.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>over i motsatt kjørebane og jeg er ikke så lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p1_g02_m2_5_x-free_cut_12</td>\n",
       "      <td>p1_g02_m2_5_x-free_cut_12.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>kjørefelt sånn at jeg måtte gå av sykkelen leg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_5</td>\n",
       "      <td>p1_g08_m2_5_x-free_cut_5.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>sendt fremragende senter for forskning som ble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_6</td>\n",
       "      <td>p1_g08_m2_5_x-free_cut_6.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>prøve å disaggregere konfliktstudiene fra å væ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_7</td>\n",
       "      <td>p1_g08_m2_5_x-free_cut_7.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>analysenivå som var mer relevant for konflikte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_8</td>\n",
       "      <td>p1_g08_m2_5_x-free_cut_8.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>innenfor  et land og gjerne ofte bare en veldi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>p1_g08_m2_5_x-free_cut_9</td>\n",
       "      <td>p1_g08_m2_5_x-free_cut_9.wav</td>\n",
       "      <td>../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...</td>\n",
       "      <td>landnivå vil være veldig feil for å studere et...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   segment_id                       wav_file  \\\n",
       "0    p1_g02_m2_5_x-free_cut_0   p1_g02_m2_5_x-free_cut_0.wav   \n",
       "1    p1_g02_m2_5_x-free_cut_1   p1_g02_m2_5_x-free_cut_1.wav   \n",
       "2   p1_g02_m2_5_x-free_cut_10  p1_g02_m2_5_x-free_cut_10.wav   \n",
       "3   p1_g02_m2_5_x-free_cut_11  p1_g02_m2_5_x-free_cut_11.wav   \n",
       "4   p1_g02_m2_5_x-free_cut_12  p1_g02_m2_5_x-free_cut_12.wav   \n",
       "..                        ...                            ...   \n",
       "62   p1_g08_m2_5_x-free_cut_5   p1_g08_m2_5_x-free_cut_5.wav   \n",
       "63   p1_g08_m2_5_x-free_cut_6   p1_g08_m2_5_x-free_cut_6.wav   \n",
       "64   p1_g08_m2_5_x-free_cut_7   p1_g08_m2_5_x-free_cut_7.wav   \n",
       "65   p1_g08_m2_5_x-free_cut_8   p1_g08_m2_5_x-free_cut_8.wav   \n",
       "66   p1_g08_m2_5_x-free_cut_9   p1_g08_m2_5_x-free_cut_9.wav   \n",
       "\n",
       "                                                 path  \\\n",
       "0   ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "1   ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "2   ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "3   ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "4   ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "..                                                ...   \n",
       "62  ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "63  ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "64  ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "65  ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "66  ../../datasets/NordTrans_TUL/test/NB_Tale/p1_g...   \n",
       "\n",
       "                                                 text  \n",
       "0   jeg er en førtifire år gammel mann som er veld...  \n",
       "1   jeg trener ungdommer og jeg er også spinningin...  \n",
       "2   ei ut av de dalsluktene så plutselig kom vi ru...  \n",
       "3   over i motsatt kjørebane og jeg er ikke så lit...  \n",
       "4   kjørefelt sånn at jeg måtte gå av sykkelen leg...  \n",
       "..                                                ...  \n",
       "62  sendt fremragende senter for forskning som ble...  \n",
       "63  prøve å disaggregere konfliktstudiene fra å væ...  \n",
       "64  analysenivå som var mer relevant for konflikte...  \n",
       "65  innenfor  et land og gjerne ofte bare en veldi...  \n",
       "66  landnivå vil være veldig feil for å studere et...  \n",
       "\n",
       "[67 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def load_test_dataset(data_dir_list: list[str]):\n",
    "    frames = []\n",
    "    for path in data_dir_list:\n",
    "        wavfile_data = []\n",
    "        textfile_data = []\n",
    "        for (root, dirs, files) in os.walk(path, topdown=True):\n",
    "            for fn in files:\n",
    "                if fn.endswith(\".wav\"):\n",
    "                    wav_id = os.path.splitext(fn)[0]\n",
    "                    path = os.path.join(root, fn)\n",
    "                    wavfile_data.append((wav_id, fn, path))\n",
    "                elif fn.endswith(\".txt\"):\n",
    "                    text_id = os.path.splitext(fn)[0]\n",
    "                    with open(os.path.join(root, fn), encoding=\"utf-8\") as text_file:\n",
    "                        text = text_file.read()\n",
    "                    textfile_data.append((text_id, text))\n",
    "                elif fn.endswith(\".txt-utf8\"):\n",
    "                    text_id = os.path.splitext(fn)[0]\n",
    "                    with open(os.path.join(root, fn), encoding=\"utf-8-sig\") as text_file:\n",
    "                        text = text_file.read()\n",
    "                    textfile_data.append((text_id, text))\n",
    "        df_wav = pd.DataFrame(wavfile_data, columns=[\"segment_id\", \"wav_file\", \"path\"])\n",
    "        df_wav = df_wav.set_index(\"segment_id\")\n",
    "        df_text = pd.DataFrame(textfile_data, columns=[\"segment_id\", \"text\"])\n",
    "        df_text = df_text.set_index(\"segment_id\")\n",
    "        dataset_df = df_wav.merge(df_text, left_index=True, right_index=True)\n",
    "        frames.append(dataset_df)\n",
    "    # concat to full dataframe\n",
    "    full_dataset_df = pd.concat(frames)\n",
    "    dataset = Dataset.from_pandas(full_dataset_df)\n",
    "    return dataset, full_dataset_df\n",
    "\n",
    "\n",
    "nbtale_dir = [\"../../datasets/NordTrans_TUL/test/NB_Tale/\"]\n",
    "\n",
    "dataset, full_dataset_df = load_test_dataset(nbtale_dir)\n",
    "full_dataset_df.reset_index(inplace=True)\n",
    "full_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p1_g07_m2_5_x-free', 'p1_g03_m2_4_x-free', 'p1_g04_m2_5_x-free', 'p1_g08_m2_5_x-free', 'p1_g02_m2_5_x-free'}\n"
     ]
    }
   ],
   "source": [
    "part_group_list = []\n",
    "for row in full_dataset_df.itertuples():\n",
    "    segment_id = row.segment_id.split(\"_\")\n",
    "    part_group_list.append(\"_\".join(segment_id[:-2]))\n",
    "print(set(part_group_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - mASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import jiwer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import BertModel, AutoModel, AutoModelForSequenceClassification\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from dtw import *\n",
    "from scipy.spatial import distance\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, display\n",
    "from scipy import stats\n",
    "# pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "metric_modelname = 'ltg/norbert2'  # changed to latest version of NorBERT (20-Mar-2023)\n",
    "metric_model = BertModel.from_pretrained(metric_modelname)\n",
    "metric_tokenizer = AutoTokenizer.from_pretrained(metric_modelname)\n",
    "\n",
    "# this is not working at the moment:\n",
    "# metric_modelname = \"ltg/norbert3-small\"\n",
    "# metric_model = AutoModel.from_pretrained(metric_modelname, trust_remote_code=True)\n",
    "# metric_tokenizer = AutoTokenizer.from_pretrained(metric_modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wordpieces: 17087\n",
      "Total whole words: 32917\n",
      "Total unused tokens: 100\n",
      "vocab dict length: 50104\n",
      "counter sum: 50104\n"
     ]
    }
   ],
   "source": [
    "vocabulary = metric_tokenizer.vocab\n",
    "\n",
    "wordpiece_counter = 0\n",
    "word_counter = 0\n",
    "unused_token_counter = 0\n",
    "for key, value in vocabulary.items():\n",
    "    if \"##\" in key:\n",
    "        wordpiece_counter += 1\n",
    "    elif \"unused\" in key:\n",
    "        unused_token_counter += 1\n",
    "    elif \"##\" not in key:\n",
    "        word_counter += 1\n",
    "print(f\"Total wordpieces: {wordpiece_counter}\")\n",
    "print(f\"Total whole words: {word_counter}\")\n",
    "print(f\"Total unused tokens: {unused_token_counter}\")\n",
    "\n",
    "print(\"vocab dict length:\", len(vocabulary))\n",
    "\n",
    "total_count = word_counter + wordpiece_counter + unused_token_counter\n",
    "print(\"counter sum:\", total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VISUAL INSPECTION OF ALIGNMENTS\n",
    "\n",
    "def get_asd_alignment(tokenized_ref, tokenized_hyp, model):\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                             hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                             hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(x=hyp_embedding_sequence, y=ref_embedding_sequence, dist_method=\"cosine\", keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "    return alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score\n",
    "\n",
    "\n",
    "def print_token_alignment(tokenizer, alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids):\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "\n",
    "    hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(hyp_alignment_idxs):\n",
    "        hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    hyp_alignment_input_ids_tensor = torch.from_numpy(hyp_alignment_input_ids)\n",
    "\n",
    "    ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    for i, index in enumerate(ref_alignment_idxs):\n",
    "        ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    ref_alignment_inpud_ids_tensor = torch.from_numpy(ref_alignment_input_ids)\n",
    "\n",
    "    hyp_alignment_tokens = tokenizer.convert_ids_to_tokens(hyp_alignment_input_ids_tensor)\n",
    "    ref_alignment_tokens = tokenizer.convert_ids_to_tokens(ref_alignment_inpud_ids_tensor)\n",
    "\n",
    "    ref_alignment_token_embeddings = []\n",
    "    for index in ref_alignment_idxs:\n",
    "        ref_alignment_token_embeddings.append(ref_embedding_sequence[index])\n",
    "\n",
    "    hyp_alignment_token_embeddings = []\n",
    "    for index in hyp_alignment_idxs:\n",
    "        hyp_alignment_token_embeddings.append(hyp_embedding_sequence[index])\n",
    "\n",
    "    cosdist_alignment_tokens = []\n",
    "    for i in range(len(ref_alignment_token_embeddings)):\n",
    "        ref_embedding = ref_alignment_token_embeddings[i]\n",
    "        hyp_embedding = hyp_alignment_token_embeddings[i]\n",
    "        cosdist_alignment_tokens.append(round((distance.cosine(ref_embedding, hyp_embedding)), 3))\n",
    "    hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    print(\"Token alignment table:\")\n",
    "    display(HTML(table))\n",
    "\n",
    "    # print(\"ASD score from alignment:\", total_dist/len(ref_alignment_token_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_modelname = \"bert-base-multilingual-cased\"\n",
    "# metric_model_multi = BertModel.from_pretrained(metric_modelname)\n",
    "# metric_tokenizer_multi = AutoTokenizer.from_pretrained(metric_modelname)\n",
    "\n",
    "# def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis, normalized=True):\n",
    "#     ref_text = re.sub(r\"\\s+\", \" \", reference.replace(\"[UNK]\", \"\"))\n",
    "#     hyp_text = re.sub(r\"\\s+\", \" \", hypothesis.replace(\"[UNK]\", \"\").replace(\"</s>\", \"\"))\n",
    "#     tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "#         model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "#     hidden_states_ref = model_output_ref.hidden_states\n",
    "#     hidden_states_hyp = model_output_hyp.hidden_states\n",
    "#     all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "#                             hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "#                             hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "#     all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "#                                 hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "#                                 hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "#     output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "#     output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "#     alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "#     num_tokens = len(output_mean_reference)\n",
    "#     if normalized == True:\n",
    "#         asd_score = alignment.distance / num_tokens\n",
    "#     else:\n",
    "#         asd_score = alignment.distance\n",
    "#     return asd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_results(mod_loss_results, orig_loss_results):\n",
    "    merged_results = orig_loss_results.join(mod_loss_results.set_index(\"segment_id\"), on=\"segment_id\")\n",
    "    merged_results = merged_results[['segment_id', 'ref_str', 'orig_asr', 'orig_cer', 'orig_wer', 'orig_asd', 'mod_asr', 'mod_cer', 'mod_wer', 'mod_asd']]\n",
    "    return merged_results\n",
    "\n",
    "def load_csv_to_df(csv_file, mod_loss):\n",
    "    results_df = pd.read_csv(csv_file, index_col=0)\n",
    "    if mod_loss == True:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\", \"ref_str\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"mod_asr\", \"wer\":\"mod_wer\", \"asd\":\"mod_asd\", \"cer\":\"mod_cer\"})\n",
    "    else:\n",
    "        results_df = results_df.drop(labels=[\"wav_file\", \"path\", \"text\"], axis=\"columns\")\n",
    "        results_df = results_df.rename(columns={\"asr_str\":\"orig_asr\", \"wer\":\"orig_wer\", \"asd\":\"orig_asd\", \"cer\":\"orig_cer\"})\n",
    "    return results_df\n",
    "\n",
    "def load_results_to_df(dir_path, mod_loss):\n",
    "    df_list = []\n",
    "    for (root, dirs, files) in os.walk(dir_path, topdown=True):\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(dir_path, fn)\n",
    "                df_list.append(load_csv_to_df(csv_path, mod_loss))\n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_dir = \"../../nlp_models/ner_pos/finetuned_bert_pos_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pos_model_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(pos_model_dir)\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "def get_pos_tags(nlp_pipeline, text, label_list_pos):\n",
    "    pos_tags = nlp_pipeline(text)\n",
    "\n",
    "    # merging subword tokens and making word list wiht pos tag list\n",
    "    labels = [label_list_pos[int(x[\"entity\"].split(\"_\")[1])] for x in pos_tags]\n",
    "    sub_words = [x[\"word\"] for x in pos_tags]\n",
    "    idx_for_labels = []\n",
    "    for i, item in enumerate(sub_words):\n",
    "        if item[0:2] != \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                idx_for_labels.append(i)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                idx_for_labels.append(i)\n",
    "        if item[0:2] == \"##\" and sub_words[i-1][0:2] != \"##\":\n",
    "            sub_word_combined = []\n",
    "            sub_word_combined.append(i-1)\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "        elif item[0:2] == \"##\" and sub_words[i-1][0:2] == \"##\":\n",
    "            if i < len(sub_words) - 1 and sub_words[i+1][0:2] != \"##\":\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "            elif i == len(sub_words) - 1:\n",
    "                sub_word_combined.append(i+1)\n",
    "                idx_for_labels.append(sub_word_combined)\n",
    "    new_word_list = []\n",
    "    new_label_list = []\n",
    "    for idx in idx_for_labels:\n",
    "        if type(idx) is int:\n",
    "            new_word_list.append(sub_words[idx])\n",
    "            new_label_list.append(labels[idx])\n",
    "        else:\n",
    "            new_word_list.append(\"\".join(sub_words[idx[0]:idx[1]]).replace(\"##\", \"\"))\n",
    "            new_label_list.append(labels[idx[0]])\n",
    "\n",
    "    return new_word_list, new_label_list\n",
    "\n",
    "\n",
    "def extract_errors_pos_tags(sub_count, ins_count, del_count, pos_tags, alignment, ref_labels, hyp_labels,\n",
    "                            ref_words, hyp_words):\n",
    "\n",
    "    label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "    sub = 0\n",
    "    ins = 0\n",
    "    dele = 0\n",
    "    pos = []\n",
    "    # ref_error_words = []\n",
    "    # asr_error_words = []\n",
    "    error_dict = []\n",
    "    pos_err_count = np.zeros(len(label_list_pos))\n",
    "\n",
    "    for i in range(len(alignment)):\n",
    "        ref_start = alignment[i].ref_start_idx\n",
    "        ref_end = alignment[i].ref_end_idx\n",
    "        hyp_start = alignment[i].hyp_start_idx\n",
    "        hyp_end = alignment[i].hyp_end_idx\n",
    "\n",
    "        if alignment[i].type != \"equal\":\n",
    "            errors = {\n",
    "                \"type\": alignment[i].type,\n",
    "                \"ref\": ref_words[ref_start:ref_end],\n",
    "                \"ref_pos\": ref_labels[ref_start:ref_end],\n",
    "                \"hyp\": hyp_words[hyp_start:hyp_end],\n",
    "                \"hyp_pos\": hyp_labels[hyp_start:hyp_end]\n",
    "            }\n",
    "            error_dict.append(errors)\n",
    "            if alignment[i].type == \"insert\":\n",
    "                hyp_pos = hyp_labels[hyp_start:hyp_end]\n",
    "                for tag in hyp_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "            elif alignment[i].type == \"substitute\":\n",
    "                ref_pos = ref_labels[ref_start:ref_end]\n",
    "                for tag in ref_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "                hyp_pos = hyp_labels[hyp_start:hyp_end]\n",
    "                for tag in hyp_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "            elif alignment[i].type == \"insert\":\n",
    "                hyp_pos = hyp_labels[hyp_start:hyp_end]\n",
    "                for tag in hyp_pos:\n",
    "                    pos_err_count[label_list_pos.index(tag)] += 1\n",
    "\n",
    "        if alignment[i].type == \"substitute\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                sub += 1\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "            # for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "            #     ref_words[ref_start+i] = word.upper()\n",
    "            #     ref_error_words.append(word)\n",
    "            # for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "            #     hyp_words[ref_start+i] = word.upper()\n",
    "            #     asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"insert\":\n",
    "            for item in hyp_labels[hyp_start:hyp_end]:\n",
    "                pos.append(item)\n",
    "                ins += 1\n",
    "            # for i, word in enumerate(hyp_words[ref_start:ref_end]):\n",
    "            #     hyp_words[ref_start+i] = word.upper()\n",
    "            #     asr_error_words.append(word)\n",
    "        elif alignment[i].type == \"delete\":\n",
    "            for item in ref_labels[ref_start:ref_end]:\n",
    "                pos.append(item)\n",
    "                dele += 1\n",
    "            # for i, word in enumerate(ref_words[ref_start:ref_end]):\n",
    "            #     ref_words[ref_start+i] = word.upper()\n",
    "            #     ref_error_words.append(word)\n",
    "\n",
    "    sub_count.append(sub)\n",
    "    ins_count.append(ins)\n",
    "    del_count.append(dele)\n",
    "    pos_tags.append(pos)\n",
    "\n",
    "    error_ref_text = \" \".join(ref_words)\n",
    "    error_asr_text = \" \".join(hyp_words)\n",
    "\n",
    "    return error_ref_text, error_asr_text, error_dict, pos_err_count\n",
    "\n",
    "\n",
    "def merge_df_wer_pos_tags(df, nlp_pipeline, label_list_pos):\n",
    "    mod_error_ref = []\n",
    "    mod_error_asr = []\n",
    "    # mod_ref_error_words = []\n",
    "    # mod_asr_error_words = []\n",
    "    mod_error_dict_list = []\n",
    "    mod_pos_err_count_list = []\n",
    "\n",
    "    orig_error_ref = []\n",
    "    orig_error_asr = []\n",
    "    # orig_ref_error_words = []\n",
    "    # orig_asr_error_words = []\n",
    "    orig_error_dict_list = []\n",
    "    orig_pos_err_count_list = []\n",
    "\n",
    "    mod_sub_count = []\n",
    "    mod_ins_count = []\n",
    "    mod_del_count = []\n",
    "    mod_pos_tags = []\n",
    "\n",
    "    orig_sub_count = []\n",
    "    orig_ins_count = []\n",
    "    orig_del_count = []\n",
    "    orig_pos_tags = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Error Tagging\"):\n",
    "        ref_words, ref_labels = get_pos_tags(nlp_pipeline, row.ref_str, label_list_pos)\n",
    "        orig_words, orig_labels = get_pos_tags(nlp_pipeline, row.orig_asr, label_list_pos)\n",
    "        mod_words, mod_labels = get_pos_tags(nlp_pipeline, row.mod_asr, label_list_pos)\n",
    "        out = jiwer.process_words([row.ref_str, row.ref_str], [row.orig_asr, row.mod_asr])\n",
    "        orig_alignment = out.alignments[0]\n",
    "        mod_alignment = out.alignments[1]\n",
    "\n",
    "        # substitute, insert, delete\n",
    "        mod_ref_text, mod_asr_text, mod_error_dict, mod_pos_err_count = extract_errors_pos_tags(mod_sub_count, mod_ins_count,\n",
    "                                                                             mod_del_count, mod_pos_tags, mod_alignment,\n",
    "                                                                             ref_labels, mod_labels, ref_words, mod_words)\n",
    "        orig_ref_text, orig_asr_text, orig_error_dict, orig_pos_err_count = extract_errors_pos_tags(orig_sub_count, orig_ins_count,\n",
    "                                                                                orig_del_count, orig_pos_tags,\n",
    "                                                                                orig_alignment, ref_labels,\n",
    "                                                                                orig_labels, ref_words, orig_words)\n",
    "\n",
    "        # whole text of the utterances\n",
    "        mod_error_ref.append(mod_ref_text)\n",
    "        mod_error_asr.append(mod_asr_text)\n",
    "        orig_error_ref.append(orig_ref_text)\n",
    "        orig_error_asr.append(orig_asr_text)\n",
    "\n",
    "        # specific words that are mistaken\n",
    "        # mod_ref_error_words.append(mod_ref_error)\n",
    "        # mod_asr_error_words.append(mod_asr_error)\n",
    "        # orig_ref_error_words.append(orig_ref_error)\n",
    "        # orig_asr_error_words.append(orig_asr_error)\n",
    "\n",
    "        # dictionary of wrong words\n",
    "        mod_error_dict_list.append(mod_error_dict)\n",
    "        mod_pos_err_count_list.append(mod_pos_err_count)\n",
    "        orig_error_dict_list.append(orig_error_dict)\n",
    "        orig_pos_err_count_list.append(orig_pos_err_count)\n",
    "\n",
    "    df.drop(labels=[\"ref_str\", \"orig_asr\", \"mod_asr\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    df[\"orig_ref\"] = orig_error_ref  # text/sentence form with uppercase errors\n",
    "    df[\"orig_asr\"] = orig_error_asr  # text/sentence form with uppercase errors\n",
    "    # df[\"orig_ref_error_words\"] = orig_ref_error_words\n",
    "    # df[\"orig_asr_error_words\"] = orig_asr_error_words\n",
    "    df[\"orig_error_dict\"] = orig_error_dict_list\n",
    "    df[\"orig_pos_err_count\"] = orig_pos_err_count_list\n",
    "\n",
    "    df[\"orig_sub\"] = orig_sub_count\n",
    "    df[\"orig_ins\"] = orig_ins_count\n",
    "    df[\"orig_del\"] = orig_del_count\n",
    "    df[\"orig_pos_tags\"] = orig_pos_tags\n",
    "\n",
    "    df[\"mod_ref\"] = mod_error_ref  # text/sentence form with uppercase errors\n",
    "    df[\"mod_asr\"] = mod_error_asr  # text/sentence form with uppercase errors\n",
    "    # df[\"mod_ref_error_words\"] = mod_ref_error_words\n",
    "    # df[\"mod_asr_error_words\"] = mod_asr_error_words\n",
    "    df[\"mod_error_dict\"] = mod_error_dict_list\n",
    "    df[\"mod_pos_err_count\"] = mod_pos_err_count_list\n",
    "\n",
    "    df[\"mod_sub\"] = mod_sub_count\n",
    "    df[\"mod_ins\"] = mod_ins_count\n",
    "    df[\"mod_del\"] = mod_del_count\n",
    "    df[\"mod_pos_tags\"] = mod_pos_tags\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the test transcriptions\n",
    "# path = \"./logs/trial35_masd_RundkastOnly_aulus7\"\n",
    "# mod_results = load_results_to_df(path, mod_loss=True)\n",
    "# path = \"./logs/trial_origloss_RundkastOnly_aulus7\"\n",
    "# orig_results = load_results_to_df(path, mod_loss=False)\n",
    "\n",
    "# merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# # higher asd score but lower wer when using modified loss\n",
    "# mod_higher_asd = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "# mod_higher_asd = mod_higher_asd[mod_higher_asd[\"mod_wer\"] < mod_higher_asd[\"orig_wer\"]]\n",
    "# mod_higher_asd = mod_higher_asd[mod_higher_asd[\"orig_wer\"] < 1]\n",
    "# print(\"higher ASD with mod loss:\", len(mod_higher_asd))\n",
    "\n",
    "# # lower asd score but higher wer when using modified loss\n",
    "# mod_lower_asd = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "# mod_lower_asd = mod_lower_asd[mod_lower_asd[\"mod_wer\"] >= mod_lower_asd[\"orig_wer\"]]\n",
    "# mod_lower_asd = mod_lower_asd[mod_lower_asd[\"orig_wer\"] < 1]\n",
    "# print(\"lower ASD with mod loss:\", len(mod_lower_asd))\n",
    "\n",
    "# # merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "\n",
    "# df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "# df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('mod_pos_tags').reset_index(name='count')\n",
    "# print(\"mod loss pos errors:\", df_mod_pos_count[\"count\"].sum(axis=0))\n",
    "# print(df_mod_pos_count)\n",
    "\n",
    "# df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "# df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('orig_pos_tags').reset_index(name='count')\n",
    "# print(\"orig loss pos errors:\", df_orig_pos_count[\"count\"].sum(axis=0))\n",
    "# print(df_orig_pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results & POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD but lower WER - using mod loss: 7\n",
      "lower ASD but higher or equal WER - using mod loss: 222\n"
     ]
    }
   ],
   "source": [
    "# loading the test transcriptions\n",
    "mod_results = load_results_to_df(\"./logs/trial60_masd_6ep_allDataSmall_titan1\", mod_loss=True)\n",
    "orig_results = load_results_to_df(\"./logs/trial_origloss_6ep_allDataSmall_aulus7\", mod_loss=False)\n",
    "\n",
    "merged_results = merge_test_results(mod_results, orig_results)\n",
    "\n",
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd_df = merged_results[merged_results[\"orig_asd\"] < merged_results[\"mod_asd\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"mod_wer\"] < mod_higher_asd_df[\"orig_wer\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD but lower WER - using mod loss:\", len(mod_higher_asd_df))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd_df = merged_results[merged_results[\"mod_asd\"] < merged_results[\"orig_asd\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"mod_wer\"] >= mod_lower_asd_df[\"orig_wer\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD but higher or equal WER - using mod loss:\", len(mod_lower_asd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_mod = []\n",
    "# multi_orig = []\n",
    "\n",
    "# for row in tqdm(merged_results.itertuples(), total=merged_results.shape[0], desc=\"multi-ASD scoring\"):\n",
    "#     multi_orig.append(compute_asd_score_single_utt(metric_model_multi, metric_tokenizer_multi, row.ref_str, row.orig_asr, normalized=True))\n",
    "#     multi_mod.append(compute_asd_score_single_utt(metric_model_multi, metric_tokenizer_multi, row.ref_str, row.mod_asr, normalized=True))\n",
    "\n",
    "# merged_results[\"orig_multi-asd\"] = multi_orig\n",
    "# merged_results[\"mod_multi-asd\"] = multi_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS error count (NOT equal CER utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging:   0%|          | 0/382 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Error Tagging: 100%|██████████| 382/382 [00:34<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances for POS analysis: 382\n",
      "mod loss pos errors: 2817\n",
      "orig loss pos errors: 2885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# merged full results to compare POS tags of orig loss & mod loss transcriptions\n",
    "\n",
    "not_equal_cer = merged_results[merged_results[\"mod_cer\"] != merged_results[\"orig_cer\"]]\n",
    "df_pos = merge_df_wer_pos_tags(not_equal_cer.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "print(\"number of utterances for POS analysis:\", len(df_pos))\n",
    "\n",
    "# df_pos = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)\n",
    "# print(\"number of utterances for POS analysis:\", len(df_pos))\n",
    "\n",
    "df_a = pd.Series(np.concatenate(df_pos.mod_pos_tags))\n",
    "df_mod_pos_count = df_a.groupby(df_a).size().rename_axis('pos_tags').reset_index(name='CTC+ASD')\n",
    "print(\"mod loss pos errors:\", df_mod_pos_count[\"CTC+ASD\"].sum(axis=0))\n",
    "\n",
    "df_b = pd.Series(np.concatenate(df_pos.orig_pos_tags))\n",
    "df_orig_pos_count = df_b.groupby(df_b).size().rename_axis('pos_tags').reset_index(name='CTC')\n",
    "print(\"orig loss pos errors:\", df_orig_pos_count[\"CTC\"].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import rcParams\n",
    "plt.rcParams.update({'font.size': 14, 'pdf.fonttype':42})\n",
    "\n",
    "merged_pos_count = pd.merge(df_orig_pos_count, df_mod_pos_count, on=[\"pos_tags\"])\n",
    "merged_pos_count = pd.melt(merged_pos_count, id_vars=[\"pos_tags\"], value_vars=[\"CTC\", \"CTC+ASD\"])\n",
    "merged_pos_count = merged_pos_count.rename(columns={\"variable\":\"model\", \"value\":\"count\"})\n",
    "# merged_pos_count = merged_pos_count.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "deltas = (df_mod_pos_count[\"CTC+ASD\"] - df_orig_pos_count[\"CTC\"]).to_list()\n",
    "deltas_df = pd.DataFrame(df_mod_pos_count[\"pos_tags\"])\n",
    "deltas_df[\"delta\"] = deltas\n",
    "deltas_df[\"y_pos\"] = df_mod_pos_count[\"CTC+ASD\"] + 20\n",
    "deltas_df[\"x_pos\"] = [x+0.2 for x in range(16)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax = sns.barplot(data=merged_pos_count, x=\"pos_tags\", y=\"count\", hue=\"model\", palette=\"dark\", alpha=0.7)\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.grid(axis='y')\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"POS Tags\")\n",
    "for row in deltas_df.itertuples():\n",
    "    ax.text(x=row.x_pos, y=row.y_pos, s=row.delta, color=\"black\", ha=\"center\")\n",
    "# sns.move_legend(ax, bbox_to_anchor=(1, 0.5), loc='center left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- among the nouns that were wrong, look at the lemmas\n",
    "- look at the adpositions and pronouns as well\n",
    "- is ASD making improvements, where are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"./plots/pos_hist.pdf\")\n",
    "# merged_pos_count.to_csv(\"./plots/pos_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher ASD but lower WER - using mod loss: 6\n",
      "lower ASD but higher or equal WER - using mod loss: 47\n"
     ]
    }
   ],
   "source": [
    "# higher asd score but lower wer when using modified loss\n",
    "mod_higher_asd_df = df_pos[df_pos[\"orig_asd\"] < df_pos[\"mod_asd\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"mod_wer\"] < mod_higher_asd_df[\"orig_wer\"]]\n",
    "mod_higher_asd_df = mod_higher_asd_df[mod_higher_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"higher ASD but lower WER - using mod loss:\", len(mod_higher_asd_df))\n",
    "\n",
    "# lower asd score but higher wer when using modified loss\n",
    "mod_lower_asd_df = df_pos[df_pos[\"mod_asd\"] < df_pos[\"orig_asd\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"mod_wer\"] >= mod_lower_asd_df[\"orig_wer\"]]\n",
    "mod_lower_asd_df = mod_lower_asd_df[mod_lower_asd_df[\"orig_wer\"] < 1]\n",
    "print(\"lower ASD but higher or equal WER - using mod loss:\", len(mod_lower_asd_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression between POS errors & ASD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- instead of a correlation analysis, perform regression analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Tagging: 100%|██████████| 829/829 [01:16<00:00, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "linreg_df = merge_df_wer_pos_tags(merged_results.copy(deep=True), nlp_pipeline, label_list_pos).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGRESSION MODEL TO PREDICT ASD:\n",
      "Results on TRAIN set\n",
      "MSE: 0.0065\n",
      "MAE: 0.0573\n",
      "R^2: 0.9327\n",
      "Results on TEST set\n",
      "MSE: 0.0048\n",
      "MAE: 0.0522\n",
      "R^2: 0.7026\n"
     ]
    }
   ],
   "source": [
    "error_data = linreg_df.mod_pos_err_count.tolist()\n",
    "label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "X = pd.DataFrame(error_data, columns=label_list_pos)\n",
    "# X = X.drop(labels=['PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'X', 'ADV', 'INTJ', 'AUX'], axis=\"columns\")\n",
    "Y = linreg_df.mod_asd\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print('REGRESSION MODEL TO PREDICT ASD:')\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "print('Results on TRAIN set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_train, Y_pred_train))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "print('R^2: %.4f' % r2_score(Y_train, Y_pred_train))\n",
    "\n",
    "Y_pred_test = model.predict(X_test)\n",
    "print('Results on TEST set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_test, Y_pred_test))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_test, Y_pred_test))\n",
    "print('R^2: %.4f' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGRESSION MODEL TO PREDICT WER:\n",
      "Results on TRAIN set\n",
      "MSE: 0.0063\n",
      "MAE: 0.0523\n",
      "R^2: 0.9606\n",
      "Results on TEST set\n",
      "MSE: 0.0056\n",
      "MAE: 0.0544\n",
      "R^2: 0.7010\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(error_data, columns=label_list_pos)\n",
    "# X = X.drop(labels=['PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'X', 'ADV', 'INTJ', 'AUX'], axis=\"columns\")\n",
    "Y = linreg_df.mod_wer\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print('REGRESSION MODEL TO PREDICT WER:')\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "print('Results on TRAIN set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_train, Y_pred_train))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_train, Y_pred_train))\n",
    "print('R^2: %.4f' % r2_score(Y_train, Y_pred_train))\n",
    "\n",
    "Y_pred_test = model.predict(X_test)\n",
    "print('Results on TEST set')\n",
    "print('MSE: %.4f' % mean_squared_error(Y_test, Y_pred_test))\n",
    "print('MAE: %.4f' % mean_absolute_error(Y_test, Y_pred_test))\n",
    "print('R^2: %.4f' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NOUN  PROPN  PRON  VERB\n",
       "0     0.0    0.0   0.0   0.0\n",
       "1     0.0    2.0   0.0   0.0\n",
       "2     2.0    0.0   0.0   0.0\n",
       "3     0.0    5.0   0.0   0.0\n",
       "4     2.0    0.0   0.0   0.0\n",
       "..    ...    ...   ...   ...\n",
       "824   2.0    0.0   0.0   0.0\n",
       "825   0.0    0.0   0.0   0.0\n",
       "826   2.0    0.0   1.0   0.0\n",
       "827   0.0    0.0   0.0   3.0\n",
       "828   3.0    4.0   2.0   0.0\n",
       "\n",
       "[829 rows x 4 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list_pos = ['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', 'ADV', 'INTJ', 'VERB', 'AUX']\n",
    "\n",
    "# data_source = df_pos.copy(deep=True)\n",
    "# data_source = data_source[data_source[\"mod_wer\"] <= 0.2]\n",
    "# data_source = data_source.reset_index(drop=True)\n",
    "\n",
    "# data_matrix = np.zeros((len(label_list_pos),len(data_source)), dtype=int)\n",
    "# for row_num, row in data_source.iterrows():\n",
    "#     for idx, count in enumerate(row.mod_pos_err_count):\n",
    "#         data_matrix[idx][row_num] += count\n",
    "\n",
    "# asd_values = data_source.mod_asd\n",
    "# wer_values = data_source.mod_wer\n",
    "\n",
    "# pos_asd_pearson_stat = []\n",
    "# pos_wer_pearson_stat = []\n",
    "# for row_num, row in enumerate(data_matrix):\n",
    "#     pos_item = label_list_pos[row_num]\n",
    "#     pos_error = row\n",
    "#     pos_asd_pearson_stat.append(stats.pearsonr(pos_error, asd_values)[0])\n",
    "#     pos_wer_pearson_stat.append(stats.pearsonr(pos_error, wer_values)[0])\n",
    "\n",
    "# pos_corr_df = pd.DataFrame(data=zip(label_list_pos, pos_asd_pearson_stat, pos_wer_pearson_stat), columns=[\"POS\", \"ASD_Pearson_r\", \"WER_Pearson_r\"])\n",
    "# pos_corr_ASDsorted_df = pos_corr_df.sort_values(by=\"ASD_Pearson_r\", ascending=False)\n",
    "# pos_corr_ASDsorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "import subprocess\n",
    "\n",
    "finetuned_bert_ner = \"../../nlp_models/ner_pos/finetuned_bert_ner_v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_bert_ner)\n",
    "model = AutoModelForTokenClassification.from_pretrained(finetuned_bert_ner)\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclite_path = \"../../aulus6_janinelr/kaldi/tools/sctk/bin/sclite\"\n",
    "sclite_output_dir = \"./sclite_outputs/Interspeech2025\"\n",
    "\n",
    "label_list_ner = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-GPE_LOC', 'I-GPE_LOC', 'B-PROD', 'I-PROD', 'B-LOC', 'I-LOC', 'B-GPE_ORG', 'I-GPE_ORG', 'B-DRV', 'I-DRV', 'B-EVT', 'I-EVT', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_tags(df, ner_pipeline):\n",
    "    ref_tags = []\n",
    "    mod_asr_tags = []\n",
    "    orig_asr_tags = []\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"NLP Tagging - ORIG ASR\"):\n",
    "        if (len(ner_pipeline(row.ref_str)) == 0) & (len(ner_pipeline(row.orig_asr)) == 0):\n",
    "            ref_tags.append(None)\n",
    "            mod_asr_tags.append(None)\n",
    "        else:\n",
    "            ref_tags.append(ner_pipeline(row.ref_str))\n",
    "            mod_asr_tags.append(ner_pipeline(row.orig_asr))\n",
    "    df[\"ref_ner_tags\"] = ref_tags\n",
    "    df[\"orig_asr_ner_tags\"] = mod_asr_tags\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"NLP Tagging - MOD ASR\"):\n",
    "        if (len(ner_pipeline(row.ref_str)) == 0) & (len(ner_pipeline(row.mod_asr)) == 0):\n",
    "            ref_tags.append(None)\n",
    "            orig_asr_tags.append(None)\n",
    "        else:\n",
    "            ref_tags.append(ner_pipeline(row.ref_str))\n",
    "            orig_asr_tags.append(ner_pipeline(row.mod_asr))\n",
    "    df[\"mod_asr_ner_tags\"] = orig_asr_tags\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# FUNCTION FOR NER ALIGNMENT\n",
    "def get_ner_alignment(input_df, sclite_output_dir, sclite_path, label_list, mod_asr=True):\n",
    "    df = input_df.copy(deep=True)\n",
    "    # combining words and tags into string of text\n",
    "    ref_tokenizedtext_list = []\n",
    "    asr_tokenizedtext_list = []\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Extracting NER tags\"):\n",
    "        ref_labels = []\n",
    "        asr_labels = []\n",
    "        for item in row.ref_ner_tags:\n",
    "            word_list = item['word'].split()\n",
    "            for word in word_list:\n",
    "                ref_labels.append(word + \"_\" + label_list[int(item['entity_group'].replace(\"LABEL_\", \"\"))])\n",
    "        if mod_asr == True:\n",
    "            for item in row.mod_asr_ner_tags:\n",
    "                word_list = item['word'].split()\n",
    "                for word in word_list:\n",
    "                    asr_labels.append(word + \"_\" + label_list[int(item['entity_group'].replace(\"LABEL_\", \"\"))])\n",
    "        else:\n",
    "            for item in row.orig_asr_ner_tags:\n",
    "                word_list = item['word'].split()\n",
    "                for word in word_list:\n",
    "                    asr_labels.append(word + \"_\" + label_list[int(item['entity_group'].replace(\"LABEL_\", \"\"))])\n",
    "\n",
    "        ref_tokenizedtext_list.append(' '.join([str(elem) for elem in ref_labels]))\n",
    "        asr_tokenizedtext_list.append(' '.join([str(elem) for elem in asr_labels]))\n",
    "\n",
    "    # using sclite to find the alignment\n",
    "    with open(os.path.join(sclite_output_dir, \"references.txt\"), \"w\") as f:\n",
    "        for idx, utterance in enumerate(ref_tokenizedtext_list):\n",
    "            f.write(utterance + \" (utt_\" + str(idx) + \")\" + \"\\n\")\n",
    "    with open(os.path.join(sclite_output_dir, \"hypothesis.txt\"), \"w\") as f:\n",
    "        for idx, utterance in enumerate(asr_tokenizedtext_list):\n",
    "            f.write(utterance + \" (utt_\" + str(idx) + \")\" + \"\\n\")\n",
    "    sclite_path = sclite_path\n",
    "    with open(os.path.join(sclite_output_dir, 'temp_output.txt'), 'a') as outfile:\n",
    "        subprocess.run([\n",
    "            sclite_path,\n",
    "            '-r', os.path.join(sclite_output_dir, \"references.txt\"),\n",
    "            '-h', os.path.join(sclite_output_dir, \"hypothesis.txt\"),\n",
    "            '-i', 'rm',\n",
    "            '-o', 'all'\n",
    "        ], stdout=outfile)\n",
    "\n",
    "    # placing alignment results into the dataframe\n",
    "    with open(os.path.join(sclite_output_dir, \"hypothesis.txt.pra\")) as f:\n",
    "        lines = f.readlines()\n",
    "    ref_align_list = []\n",
    "    asr_align_list = []\n",
    "    utt_ids_list = []\n",
    "    eval_align_list = []\n",
    "    for line in lines:\n",
    "        clean_line = re.sub(' +', ' ', line).strip()\n",
    "        text_list = clean_line.split(\" \")\n",
    "        # text_list = [item.lower() for item in text_list]\n",
    "        if text_list[0] == \"id:\":\n",
    "            utt_ids_list.append(text_list[1])\n",
    "        elif text_list[0] == \"REF:\":\n",
    "            ref_align_list.append(text_list[1:])\n",
    "        elif text_list[0] == \"HYP:\":\n",
    "            asr_align_list.append(text_list[1:])\n",
    "    df[\"ref_token\"] = ref_align_list\n",
    "    df[\"asr_token\"] = asr_align_list\n",
    "    eval_align_list = []\n",
    "    for row in df.itertuples():\n",
    "        eval_token_list = []\n",
    "        ref_token_list = row.ref_token\n",
    "        asr_token_list = row.asr_token\n",
    "        for idx, ref_token in enumerate(ref_token_list):\n",
    "            if ref_token == asr_token_list[idx]:\n",
    "                eval_token_list.append(\"m\")\n",
    "            elif \"*\" in ref_token:\n",
    "                eval_token_list.append(\"I\")\n",
    "            elif \"*\" in asr_token_list[idx]:\n",
    "                eval_token_list.append(\"D\")\n",
    "            elif ref_token != asr_token_list[idx]:\n",
    "                eval_token_list.append(\"S\")\n",
    "        eval_align_list.append(eval_token_list)\n",
    "    df[\"eval_token\"] = eval_align_list\n",
    "\n",
    "    # cleanup dataframe by removing items with no NER tags at all\n",
    "    idx_to_drop = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Remove no NER tags\"):\n",
    "        if (len(row.ref_ner_tags) == 1) & (row.ref_ner_tags[0][\"entity_group\"] == \"LABEL_0\"):\n",
    "            idx_to_drop.append(idx)\n",
    "    df.drop(idx_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # getting aligned ASR and REF labels (individually, word-wise)\n",
    "    ref_labels = []\n",
    "    asr_labels = []\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Aligning REF & ASR labels\"):\n",
    "        ref_tags = []\n",
    "        for item in row.ref_token:\n",
    "            tokens = item.split(\"_\")\n",
    "            if len(tokens) == 1:\n",
    "                ref_tags.append(\"O\")\n",
    "            elif len(tokens) == 2:\n",
    "                ref_tags.append(tokens[1].upper())\n",
    "            elif len(tokens) == 3:\n",
    "                ref_tags.append(tokens[1].upper() + \"_\" + tokens[2].upper())\n",
    "        ref_labels.append(ref_tags)\n",
    "        asr_tags = []\n",
    "        for item in row.asr_token:\n",
    "            tokens = item.split(\"_\")\n",
    "            if len(tokens) == 1:\n",
    "                asr_tags.append(\"O\")\n",
    "            elif len(tokens) == 2:\n",
    "                asr_tags.append(tokens[1].upper())\n",
    "            elif len(tokens) == 3:\n",
    "                asr_tags.append(tokens[1].upper() + \"_\" + tokens[2].upper())\n",
    "        asr_labels.append(asr_tags)\n",
    "    df[\"ref_labels\"] = ref_labels\n",
    "    df[\"asr_labels\"] = asr_labels\n",
    "\n",
    "    print(\"get alignment - DONE\")\n",
    "\n",
    "    return df, idx_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLP Tagging - ORIG ASR:   0%|          | 0/829 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "NLP Tagging - ORIG ASR: 100%|██████████| 829/829 [01:42<00:00,  8.10it/s]\n",
      "NLP Tagging - MOD ASR: 100%|██████████| 829/829 [01:44<00:00,  7.93it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_df = merged_results.copy(deep=True)\n",
    "ner_df = get_ner_tags(ner_df, nlp_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting NER tags: 100%|██████████| 829/829 [00:00<00:00, 30787.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove no NER tags: 100%|██████████| 829/829 [00:00<00:00, 17585.25it/s]\n",
      "Aligning REF & ASR labels: 100%|██████████| 316/316 [00:00<00:00, 70108.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get alignment - DONE\n",
      "MOD ASR - NER\n",
      "F1 score micro: 0.7222787385554424\n",
      "F1 score macro: 0.5583556296988624\n",
      "WER: 0.19281656309066958\n",
      "ASD: 0.1714380419865155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mod_ner_df, idx_to_drop = get_ner_alignment(ner_df, sclite_output_dir, sclite_path, label_list_ner, mod_asr=True)\n",
    "\n",
    "mod_ner_df[\"f1_score\"] = mod_ner_df[[\"ref_labels\", \"asr_labels\"]].apply(lambda row: f1_score([row.ref_labels], [row.asr_labels], average=\"micro\"), axis=1)\n",
    "\n",
    "print(\"MOD ASR - NER\")\n",
    "y_true = list(mod_ner_df[\"ref_labels\"])\n",
    "y_pred = list(mod_ner_df[\"asr_labels\"])\n",
    "print(\"F1 score micro:\", f1_score(y_true, y_pred, average=\"micro\"))\n",
    "print(\"F1 score macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", mod_ner_df[\"mod_wer\"].mean())\n",
    "print(\"ASD:\", mod_ner_df[\"mod_asd\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting NER tags:   0%|          | 0/829 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting NER tags: 100%|██████████| 829/829 [00:00<00:00, 22975.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove no NER tags: 100%|██████████| 829/829 [00:00<00:00, 18063.59it/s]\n",
      "Aligning REF & ASR labels: 100%|██████████| 316/316 [00:00<00:00, 71009.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get alignment - DONE\n",
      "ORIG ASR - NER\n",
      "F1 score micro: 0.718266253869969\n",
      "F1 score macro: 0.5058669152636408\n",
      "WER: 0.19675650810470247\n",
      "ASD: 0.1766881681958569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_ner_df, ___ = get_ner_alignment(ner_df, sclite_output_dir, sclite_path, label_list_ner, mod_asr=False)\n",
    "\n",
    "orig_ner_df[\"f1_score\"] = orig_ner_df[[\"ref_labels\", \"asr_labels\"]].apply(lambda row: f1_score([row.ref_labels], [row.asr_labels], average=\"micro\"), axis=1)\n",
    "\n",
    "print(\"ORIG ASR - NER\")\n",
    "y_true = list(orig_ner_df[\"ref_labels\"])\n",
    "y_pred = list(orig_ner_df[\"asr_labels\"])\n",
    "print(\"F1 score micro:\", f1_score(y_true, y_pred, average=\"micro\"))\n",
    "print(\"F1 score macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", orig_ner_df[\"orig_wer\"].mean())\n",
    "print(\"ASD:\", orig_ner_df[\"orig_asd\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-polarity Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bert_sa = \"../../nlp_models/sentiment_analysis/finetuned_model\"\n",
    "sa_model = AutoModelForSequenceClassification.from_pretrained(finetuned_bert_sa, num_labels=3)\n",
    "sa_tokenizer = AutoTokenizer.from_pretrained(finetuned_bert_sa, num_labels=3)\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR SENTIMENT ANALYSIS\n",
    "def get_pred_label(x):\n",
    "    if x == 0:\n",
    "        return ['Neutral']\n",
    "    elif x == 1:\n",
    "        return ['Positive']\n",
    "    else:\n",
    "        return ['Negative']\n",
    "\n",
    "def get_sentiment(df, model, tokenizer):\n",
    "    mod_asr_sentiment = []\n",
    "    orig_asr_sentiment = []\n",
    "    ref_sentiment = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Sentiment tagging\"):\n",
    "        tokenized_ref = tokenizer(row.ref_str, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        tokenized_mod_asr = tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        tokenized_orig_asr = tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            model_output_ref = model(**tokenized_ref, output_hidden_states=False)\n",
    "            model_output_mod_asr = model(**tokenized_mod_asr, output_hidden_states=False)\n",
    "            model_output_orig_asr = model(**tokenized_orig_asr, output_hidden_states=False)\n",
    "        pred_ref = model_output_ref['logits'].numpy()\n",
    "        ref_sentiment.append(np.argmax(pred_ref, axis=1))\n",
    "        pred_mod_asr = model_output_mod_asr['logits'].numpy()\n",
    "        pred_orig_asr = model_output_orig_asr['logits'].numpy()\n",
    "        mod_asr_sentiment.append(np.argmax(pred_mod_asr, axis=1))\n",
    "        orig_asr_sentiment.append(np.argmax(pred_orig_asr, axis=1))\n",
    "\n",
    "    df['ref_sentiment'] = ref_sentiment\n",
    "    df['mod_asr_sentiment'] = mod_asr_sentiment\n",
    "    df['orig_asr_sentiment'] = orig_asr_sentiment\n",
    "\n",
    "    df['ref_sentiment'] = df['ref_sentiment'].map(get_pred_label)\n",
    "    df['mod_asr_sentiment'] = df['mod_asr_sentiment'].map(get_pred_label)\n",
    "    df['orig_asr_sentiment'] = df['orig_asr_sentiment'].map(get_pred_label)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment tagging: 100%|██████████| 316/316 [00:29<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis - Orig ASR\n",
      "F1 score: 0.9028613898179115\n",
      "WER: 0.19675650810470247\n",
      "ASD: 0.1766881681958569\n",
      "Sentiment Analysis - MOD ASR\n",
      "F1 score: 0.9225205174572263\n",
      "WER: 0.19281656309066958\n",
      "ASD: 0.1714380419865155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Neutral seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Positive seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Negative seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Neutral seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Positive seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Negative seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Neutral seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Positive seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/janinelr/micromamba/envs/wav2vec_v2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Negative seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "sa_df = merged_results.copy(deep=True)\n",
    "sa_df = sa_df.drop(idx_to_drop)\n",
    "sa_df = sa_df.reset_index(drop=True)\n",
    "\n",
    "sa_df = get_sentiment(sa_df, sa_model, sa_tokenizer)\n",
    "\n",
    "sa_df[\"f1_score_mod\"] = sa_df[[\"ref_sentiment\", \"mod_asr_sentiment\"]].apply(lambda row: f1_score([row.ref_sentiment], [row.mod_asr_sentiment], average=\"macro\"), axis=1)\n",
    "sa_df[\"f1_score_orig\"] = sa_df[[\"ref_sentiment\", \"orig_asr_sentiment\"]].apply(lambda row: f1_score([row.ref_sentiment], [row.orig_asr_sentiment], average=\"macro\"), axis=1)\n",
    "\n",
    "print(\"Sentiment Analysis - Orig ASR\")\n",
    "y_true = sa_df[\"ref_sentiment\"]\n",
    "y_pred = sa_df[\"orig_asr_sentiment\"]\n",
    "# print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", sa_df[\"orig_wer\"].mean())\n",
    "print(\"ASD:\", sa_df[\"orig_asd\"].mean())\n",
    "\n",
    "print(\"Sentiment Analysis - MOD ASR\")\n",
    "y_true = sa_df[\"ref_sentiment\"]\n",
    "y_pred = sa_df[\"mod_asr_sentiment\"]\n",
    "# print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"F1 score:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"WER:\", sa_df[\"mod_wer\"].mean())\n",
    "print(\"ASD:\", sa_df[\"mod_asd\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  word  lemma\n",
      "0                -abel  -abel\n",
      "1               -abels  -abel\n",
      "2               -abelt  -abel\n",
      "3              -abelts  -abel\n",
      "4                -able  -abel\n",
      "...                ...    ...\n",
      "744437   jordskokkpuré      -\n",
      "744438  rotgrønnsakene      -\n",
      "744439      albóndigas      -\n",
      "744440  gelatinplatene      -\n",
      "744441   kyllinglårene      -\n",
      "\n",
      "[744442 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# LEXICON\n",
    "file_path = \"/talebase/data/lex/Sprakbanken/NLB/nlb_nob_20181129.lex\"\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines:\n",
    "        lex_list = line.split(\"\\t\")\n",
    "        lemma = lex_list[13].split(\":\")[1].split(\"|\")[0]\n",
    "        word_list.append(lex_list[0])\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "lex_dict = {\"word\":word_list, \"lemma\":lemma_list}\n",
    "lex_df = pd.DataFrame(lex_dict)\n",
    "print(lex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lemma(lex_df, error_dict):\n",
    "    lemma_found = []\n",
    "    for item in error_dict:\n",
    "        if item[\"type\"] == \"substitute\":\n",
    "            for ref_word, hyp_word in zip(item[\"ref\"], item[\"hyp\"]):\n",
    "                for row in lex_df.itertuples():\n",
    "                    if row.word == ref_word:\n",
    "                        lemma = {\n",
    "                            \"error\": item[\"type\"],\n",
    "                            \"ref_word\": ref_word,\n",
    "                            \"hyp_word\": hyp_word,\n",
    "                            \"lemma\": row.lemma\n",
    "                        }\n",
    "                        lemma_found.append(lemma)\n",
    "    return lemma_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:29<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_lemma = df_pos.copy(deep=True)\n",
    "df_lemma[\"lemma_found\"] = df_lemma[[\"mod_error_dict\"]].progress_apply(lambda row: find_lemma(lex_df, row.mod_error_dict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>orig_cer</th>\n",
       "      <th>orig_wer</th>\n",
       "      <th>orig_asd</th>\n",
       "      <th>mod_cer</th>\n",
       "      <th>mod_wer</th>\n",
       "      <th>mod_asd</th>\n",
       "      <th>orig_ref</th>\n",
       "      <th>orig_asr</th>\n",
       "      <th>orig_error_dict</th>\n",
       "      <th>orig_pos_err_count</th>\n",
       "      <th>orig_sub</th>\n",
       "      <th>orig_ins</th>\n",
       "      <th>orig_del</th>\n",
       "      <th>orig_pos_tags</th>\n",
       "      <th>mod_ref</th>\n",
       "      <th>mod_asr</th>\n",
       "      <th>mod_error_dict</th>\n",
       "      <th>mod_pos_err_count</th>\n",
       "      <th>mod_sub</th>\n",
       "      <th>mod_ins</th>\n",
       "      <th>mod_del</th>\n",
       "      <th>mod_pos_tags</th>\n",
       "      <th>lemma_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_100</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.138008</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.129134</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['an'], 'ref_po...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PROPN, PROPN, PROPN]</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>valget attende mai der de store partiene fra f...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['an', 'cathrin...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PROPN, PROPN, PROPN, PROPN, PROPN]</td>\n",
       "      <td>[{'error': 'substitute', 'ref_word': 'an', 'hy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_102</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.212464</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.084633</td>\n",
       "      <td>kjent at viruset kan overleve flere timer uten...</td>\n",
       "      <td>at viruset kan overleve flere timer utenfor me...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['kjent'], 'ref_pos...</td>\n",
       "      <td>[3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, DET, NOUN, NOUN, NOUN]</td>\n",
       "      <td>kjent at viruset kan overleve flere timer uten...</td>\n",
       "      <td>at viruset kan overleve flere timer utenfor me...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['kjent'], 'ref_pos...</td>\n",
       "      <td>[2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, PROPN, NOUN, NOUN]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_103</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.091840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, PROPN, PROPN]</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>ny lovgivning fra eu bare danskene er bedre vi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_106</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.196988</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076057</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['märtha', 'lou...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, PROPN, X, X, PROPN, X, VERB]</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>til kongefamiliens ferske medlem da har jeg de...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['märtha'], 're...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N2_030505_NRK_D12_NO_cut_108</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.168720</td>\n",
       "      <td>gang slottets informasjonssjef wenche rask for...</td>\n",
       "      <td>gang slottets informasjonssjef venke rask forr...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['wenche'], 're...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, VERB, PRON]</td>\n",
       "      <td>gang slottets informasjonssjef wenche rask for...</td>\n",
       "      <td>gang slottets informasjonssjef wenke rask for ...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['wenche'], 're...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN, PROPN, DET, ADP, PRON]</td>\n",
       "      <td>[{'error': 'substitute', 'ref_word': 'forrige'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.139344</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.268045</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.249815</td>\n",
       "      <td>ikke liker det de står for vil de altså tvinge...</td>\n",
       "      <td>ikke liker det de står for så vil også tvinge ...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[ADV, PRON, ADV, ADV, ADP, AUX, PRON, VERB]</td>\n",
       "      <td>ikke liker det de står for vil de altså tvinge...</td>\n",
       "      <td>liker det de står for så vil det også tvinge f...</td>\n",
       "      <td>[{'type': 'delete', 'ref': ['ikke'], 'ref_pos'...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[PART, ADV, PRON, ADV, PRON, ADV, ADP, VERB]</td>\n",
       "      <td>[{'error': 'substitute', 'ref_word': 'de', 'hy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.127637</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.112867</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelser a...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelse av...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['masseoppsigel...</td>\n",
       "      <td>[8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOU...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelser a...</td>\n",
       "      <td>flertall så ville det blitt masseoppsigelse av...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['masseoppsigel...</td>\n",
       "      <td>[6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, ADP]</td>\n",
       "      <td>[{'error': 'substitute', 'ref_word': 'masseopp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.253129</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.151328</td>\n",
       "      <td>fremskrittspartiet nå vil fjerne hele formuess...</td>\n",
       "      <td>partiene vil fjerne hele formunsskatten som jo...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['fremskrittspa...</td>\n",
       "      <td>[4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, ADV, NOUN, NOUN, PRON]</td>\n",
       "      <td>fremskrittspartiet nå vil fjerne hele formuess...</td>\n",
       "      <td>partiene vil fjerne hele formuesskatten som jo...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['fremskrittspa...</td>\n",
       "      <td>[2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN, NOUN, ADV, PRON]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.079557</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.192340</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['å'], 'ref_pos...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[PART, VERB, VERB]</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>milliarder kroner mindre jeg må si det er fris...</td>\n",
       "      <td>[{'type': 'substitute', 'ref': ['å'], 'ref_pos...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[PART, VERB, VERB, VERB, VERB, ADP]</td>\n",
       "      <td>[{'error': 'substitute', 'ref_word': 'å', 'hyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.817789</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.853918</td>\n",
       "      <td>økonomisk men derfor står de også helt aleine ...</td>\n",
       "      <td>økonomisk men derfor så står det også helt ale...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...</td>\n",
       "      <td>økonomisk men derfor står de også helt aleine ...</td>\n",
       "      <td>økonomisk men derfor så står det også helt ale...</td>\n",
       "      <td>[{'type': 'insert', 'ref': [], 'ref_pos': [], ...</td>\n",
       "      <td>[3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...</td>\n",
       "      <td>[{'error': 'substitute', 'ref_word': 'de', 'hy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            segment_id  orig_cer  orig_wer  \\\n",
       "0                         N2_030505_NRK_D12_NO_cut_100  0.031496      0.10   \n",
       "1                         N2_030505_NRK_D12_NO_cut_102  0.084746      0.20   \n",
       "2                         N2_030505_NRK_D12_NO_cut_103  0.015625      0.10   \n",
       "3                         N2_030505_NRK_D12_NO_cut_106  0.062500      0.20   \n",
       "4                         N2_030505_NRK_D12_NO_cut_108  0.048276      0.10   \n",
       "..                                                 ...       ...       ...   \n",
       "377  2021H264-fullStorting0510Stortinget-20210510-1...  0.139344      0.30   \n",
       "378  2021H264-fullStorting0510Stortinget-20210510-1...  0.058394      0.25   \n",
       "379  2021H264-fullStorting0510Stortinget-20210510-1...  0.138686      0.20   \n",
       "380  2021H264-fullStorting0510Stortinget-20210510-1...  0.026786      0.10   \n",
       "381  2021H264-fullStorting0510Stortinget-20210510-1...  0.796748      0.85   \n",
       "\n",
       "     orig_asd   mod_cer  mod_wer   mod_asd  \\\n",
       "0    0.138008  0.023622     0.15  0.129134   \n",
       "1    0.212464  0.076271     0.15  0.084633   \n",
       "2    0.091840  0.000000     0.00  0.000000   \n",
       "3    0.196988  0.008929     0.05  0.076057   \n",
       "4    0.143233  0.068966     0.15  0.168720   \n",
       "..        ...       ...      ...       ...   \n",
       "377  0.268045  0.147541     0.30  0.249815   \n",
       "378  0.127637  0.051095     0.20  0.112867   \n",
       "379  0.253129  0.131387     0.15  0.151328   \n",
       "380  0.079557  0.053571     0.20  0.192340   \n",
       "381  0.817789  0.804878     0.85  0.853918   \n",
       "\n",
       "                                              orig_ref  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    kjent at viruset kan overleve flere timer uten...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenche rask for...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for vil de altså tvinge...   \n",
       "378  flertall så ville det blitt masseoppsigelser a...   \n",
       "379  fremskrittspartiet nå vil fjerne hele formuess...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor står de også helt aleine ...   \n",
       "\n",
       "                                              orig_asr  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    at viruset kan overleve flere timer utenfor me...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef venke rask forr...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for så vil også tvinge ...   \n",
       "378  flertall så ville det blitt masseoppsigelse av...   \n",
       "379  partiene vil fjerne hele formunsskatten som jo...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor så står det også helt ale...   \n",
       "\n",
       "                                       orig_error_dict  \\\n",
       "0    [{'type': 'substitute', 'ref': ['an'], 'ref_po...   \n",
       "1    [{'type': 'delete', 'ref': ['kjent'], 'ref_pos...   \n",
       "2    [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "3    [{'type': 'substitute', 'ref': ['märtha', 'lou...   \n",
       "4    [{'type': 'substitute', 'ref': ['wenche'], 're...   \n",
       "..                                                 ...   \n",
       "377  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "378  [{'type': 'substitute', 'ref': ['masseoppsigel...   \n",
       "379  [{'type': 'substitute', 'ref': ['fremskrittspa...   \n",
       "380  [{'type': 'substitute', 'ref': ['å'], 'ref_pos...   \n",
       "381  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "\n",
       "                                    orig_pos_err_count  orig_sub  orig_ins  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         2         0   \n",
       "1    [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...         1         2   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1         1   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         3         0   \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1         0   \n",
       "..                                                 ...       ...       ...   \n",
       "377  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         2         1   \n",
       "378  [8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         4         0   \n",
       "379  [4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         2         1   \n",
       "380  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...         1         0   \n",
       "381  [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...         2        15   \n",
       "\n",
       "     orig_del                                      orig_pos_tags  \\\n",
       "0           0                         [ADV, PROPN, PROPN, PROPN]   \n",
       "1           1                       [ADJ, DET, NOUN, NOUN, NOUN]   \n",
       "2           0                              [PROPN, PROPN, PROPN]   \n",
       "3           1               [PROPN, PROPN, X, X, PROPN, X, VERB]   \n",
       "4           1                                [PROPN, VERB, PRON]   \n",
       "..        ...                                                ...   \n",
       "377         3        [ADV, PRON, ADV, ADV, ADP, AUX, PRON, VERB]   \n",
       "378         1  [NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, NOU...   \n",
       "379         1                [NOUN, NOUN, ADV, NOUN, NOUN, PRON]   \n",
       "380         1                                 [PART, VERB, VERB]   \n",
       "381         0  [ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...   \n",
       "\n",
       "                                               mod_ref  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    kjent at viruset kan overleve flere timer uten...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenche rask for...   \n",
       "..                                                 ...   \n",
       "377  ikke liker det de står for vil de altså tvinge...   \n",
       "378  flertall så ville det blitt masseoppsigelser a...   \n",
       "379  fremskrittspartiet nå vil fjerne hele formuess...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor står de også helt aleine ...   \n",
       "\n",
       "                                               mod_asr  \\\n",
       "0    valget attende mai der de store partiene fra f...   \n",
       "1    at viruset kan overleve flere timer utenfor me...   \n",
       "2    ny lovgivning fra eu bare danskene er bedre vi...   \n",
       "3    til kongefamiliens ferske medlem da har jeg de...   \n",
       "4    gang slottets informasjonssjef wenke rask for ...   \n",
       "..                                                 ...   \n",
       "377  liker det de står for så vil det også tvinge f...   \n",
       "378  flertall så ville det blitt masseoppsigelse av...   \n",
       "379  partiene vil fjerne hele formuesskatten som jo...   \n",
       "380  milliarder kroner mindre jeg må si det er fris...   \n",
       "381  økonomisk men derfor så står det også helt ale...   \n",
       "\n",
       "                                        mod_error_dict  \\\n",
       "0    [{'type': 'substitute', 'ref': ['an', 'cathrin...   \n",
       "1    [{'type': 'delete', 'ref': ['kjent'], 'ref_pos...   \n",
       "2                                                   []   \n",
       "3    [{'type': 'substitute', 'ref': ['märtha'], 're...   \n",
       "4    [{'type': 'substitute', 'ref': ['wenche'], 're...   \n",
       "..                                                 ...   \n",
       "377  [{'type': 'delete', 'ref': ['ikke'], 'ref_pos'...   \n",
       "378  [{'type': 'substitute', 'ref': ['masseoppsigel...   \n",
       "379  [{'type': 'substitute', 'ref': ['fremskrittspa...   \n",
       "380  [{'type': 'substitute', 'ref': ['å'], 'ref_pos...   \n",
       "381  [{'type': 'insert', 'ref': [], 'ref_pos': [], ...   \n",
       "\n",
       "                                     mod_pos_err_count  mod_sub  mod_ins  \\\n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        3        0   \n",
       "1    [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1        1   \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0        0   \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1        0   \n",
       "4    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...        2        0   \n",
       "..                                                 ...      ...      ...   \n",
       "377  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        2        1   \n",
       "378  [6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        3        0   \n",
       "379  [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1        1   \n",
       "380  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...        2        1   \n",
       "381  [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, ...        2       15   \n",
       "\n",
       "     mod_del                                       mod_pos_tags  \\\n",
       "0          0           [ADV, PROPN, PROPN, PROPN, PROPN, PROPN]   \n",
       "1          1                           [ADJ, PROPN, NOUN, NOUN]   \n",
       "2          0                                                 []   \n",
       "3          0                                     [PROPN, PROPN]   \n",
       "4          1                     [PROPN, PROPN, DET, ADP, PRON]   \n",
       "..       ...                                                ...   \n",
       "377        3       [PART, ADV, PRON, ADV, PRON, ADV, ADP, VERB]   \n",
       "378        1          [NOUN, NOUN, NOUN, NOUN, NOUN, NOUN, ADP]   \n",
       "379        1                            [NOUN, NOUN, ADV, PRON]   \n",
       "380        1                [PART, VERB, VERB, VERB, VERB, ADP]   \n",
       "381        0  [ADV, PRON, PRON, DET, DET, ADV, DET, NOUN, AU...   \n",
       "\n",
       "                                           lemma_found  \n",
       "0    [{'error': 'substitute', 'ref_word': 'an', 'hy...  \n",
       "1                                                   []  \n",
       "2                                                   []  \n",
       "3                                                   []  \n",
       "4    [{'error': 'substitute', 'ref_word': 'forrige'...  \n",
       "..                                                 ...  \n",
       "377  [{'error': 'substitute', 'ref_word': 'de', 'hy...  \n",
       "378  [{'error': 'substitute', 'ref_word': 'masseopp...  \n",
       "379                                                 []  \n",
       "380  [{'error': 'substitute', 'ref_word': 'å', 'hyp...  \n",
       "381  [{'error': 'substitute', 'ref_word': 'de', 'hy...  \n",
       "\n",
       "[382 rows x 24 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'error': 'substitute', 'ref_word': 'an', 'hyp_word': 'ann', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'an', 'hyp_word': 'ann', 'lemma': 'ane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'forrige', 'hyp_word': 'for', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'forrige', 'hyp_word': 'for', 'lemma': 'forrige'}, {'error': 'substitute', 'ref_word': 'forrige', 'hyp_word': 'for', 'lemma': 'forrige'}, {'error': 'substitute', 'ref_word': 'forrige', 'hyp_word': 'for', 'lemma': 'forrige'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kan', 'hyp_word': 'marinekollagen', 'lemma': 'kunne'}, {'error': 'substitute', 'ref_word': 'kan', 'hyp_word': 'marinekollagen', 'lemma': 'kan'}]\n",
      "[{'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'får'}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'får'}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'får'}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'få'}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'nesten', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'nesten', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'derfor', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'aksjene', 'hyp_word': 'og', 'lemma': 'aksje'}]\n",
      "[{'error': 'substitute', 'ref_word': 'enig', 'hyp_word': 'staden', 'lemma': 'enig'}, {'error': 'substitute', 'ref_word': 'gjerne', 'hyp_word': 'dere', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': 'demme'}, {'error': 'substitute', 'ref_word': 'gi', 'hyp_word': 'i', 'lemma': 'gi'}, {'error': 'substitute', 'ref_word': 'villig', 'hyp_word': 'vi', 'lemma': 'villig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'åe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'at', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'ha', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'her', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ett', 'hyp_word': 'etter', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ett', 'hyp_word': 'etter', 'lemma': 'ete'}, {'error': 'substitute', 'ref_word': 'om', 'hyp_word': 'omgangen', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'om', 'hyp_word': 'omgangen', 'lemma': 'om'}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'høystbydende', 'hyp_word': 'og', 'lemma': 'høystbydende'}, {'error': 'substitute', 'ref_word': 'høystbydende', 'hyp_word': 'og', 'lemma': 'høystbydende'}, {'error': 'substitute', 'ref_word': 'høystbydende', 'hyp_word': 'og', 'lemma': 'høystbydende'}, {'error': 'substitute', 'ref_word': 'høystbydende', 'hyp_word': 'og', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'men', 'hyp_word': 'partiets', 'lemma': 'm'}, {'error': 'substitute', 'ref_word': 'men', 'hyp_word': 'partiets', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'men', 'hyp_word': 'partiets', 'lemma': 'men'}, {'error': 'substitute', 'ref_word': 'men', 'hyp_word': 'partiets', 'lemma': 'mene'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vedlikeholde', 'hyp_word': 'og', 'lemma': 'vedlikeholde'}]\n",
      "[{'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'her', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'her', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'politibetjent', 'lemma': 'ved'}, {'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'politibetjent', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'flyet', 'hyp_word': 'fly', 'lemma': 'fly'}]\n",
      "[{'error': 'substitute', 'ref_word': 'havaristedet', 'hyp_word': 'istedet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'stiller', 'hyp_word': 'spiller', 'lemma': 'stille'}, {'error': 'substitute', 'ref_word': 'stiller', 'hyp_word': 'spiller', 'lemma': 'stiller'}, {'error': 'substitute', 'ref_word': 'stiller', 'hyp_word': 'spiller', 'lemma': 'stille'}, {'error': 'substitute', 'ref_word': 'spørsmål', 'hyp_word': 'spørsmålet', 'lemma': 'spørsmål'}, {'error': 'substitute', 'ref_word': 'spørsmål', 'hyp_word': 'spørsmålet', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'da', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nordvest', 'hyp_word': 'i', 'lemma': 'nordvest'}, {'error': 'substitute', 'ref_word': 'nordvest', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nordvest', 'hyp_word': 'i', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'rak', 'hyp_word': 'rakt', 'lemma': 'rak'}, {'error': 'substitute', 'ref_word': 'rak', 'hyp_word': 'rakt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'rak', 'hyp_word': 'rakt', 'lemma': 'rake'}, {'error': 'substitute', 'ref_word': 'styrta', 'hyp_word': 'spurta', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'styrta', 'hyp_word': 'spurta', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'styrta', 'hyp_word': 'spurta', 'lemma': 'styrte'}, {'error': 'substitute', 'ref_word': 'styrta', 'hyp_word': 'spurta', 'lemma': 'styrte'}, {'error': 'substitute', 'ref_word': 'styrta', 'hyp_word': 'spurta', 'lemma': 'styrte'}, {'error': 'substitute', 'ref_word': 'her', 'hyp_word': 'er', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'alt', 'hyp_word': 'altfor', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'alt', 'hyp_word': 'altfor', 'lemma': 'all'}, {'error': 'substitute', 'ref_word': 'alt', 'hyp_word': 'altfor', 'lemma': 'alt'}, {'error': 'substitute', 'ref_word': 'alt', 'hyp_word': 'altfor', 'lemma': 'ale'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'sakte', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'sakte', 'lemma': 'fôr'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'sakte', 'lemma': 'fare'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'sakte', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'sakte', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sakte', 'hyp_word': 'og', 'lemma': 'sakte'}]\n",
      "[{'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'forsagt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'forsagt', 'lemma': 'fôr'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'forsagt', 'lemma': 'fare'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'forsagt', 'lemma': '-'}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'bevilgning', 'hyp_word': 'bevilgningene', 'lemma': 'bevilgning'}]\n",
      "[{'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'aksjene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'deg', 'hyp_word': 'det', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'koblet', 'hyp_word': 'koplet', 'lemma': 'koble'}, {'error': 'substitute', 'ref_word': 'koblet', 'hyp_word': 'koplet', 'lemma': 'kobbel'}]\n",
      "[{'error': 'substitute', 'ref_word': 'eigarskap', 'hyp_word': 'eierskap', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'politikarane', 'hyp_word': 'politikerne', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'eg', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'får'}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'får'}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'får'}, {'error': 'substitute', 'ref_word': 'får', 'hyp_word': 'for', 'lemma': 'få'}]\n",
      "[{'error': 'substitute', 'ref_word': 'nattflyvninger', 'hyp_word': 'nattflygninger', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'vinmonopolet', 'hyp_word': 'vimonopole', 'lemma': 'vinmonopol'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vennene', 'hyp_word': 'vennans', 'lemma': 'venn'}]\n",
      "[{'error': 'substitute', 'ref_word': 'av', 'hyp_word': 'vår', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vår', 'hyp_word': 'internett', 'lemma': 'vår'}, {'error': 'substitute', 'ref_word': 'vår', 'hyp_word': 'internett', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vår', 'hyp_word': 'internett', 'lemma': 'våre'}, {'error': 'substitute', 'ref_word': 'hvor', 'hyp_word': 'for', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'image', 'hyp_word': 'imme', 'lemma': 'image'}, {'error': 'substitute', 'ref_word': 'image', 'hyp_word': 'imme', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'industri', 'hyp_word': 'industribedriftene', 'lemma': 'industri'}]\n",
      "[{'error': 'substitute', 'ref_word': 'legge', 'hyp_word': 'legger', 'lemma': 'legge'}, {'error': 'substitute', 'ref_word': 'ønsket', 'hyp_word': 'ønske', 'lemma': 'ønske'}, {'error': 'substitute', 'ref_word': 'ønsket', 'hyp_word': 'ønske', 'lemma': 'ønske'}]\n",
      "[{'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'regjeringsmakt', 'hyp_word': 'regjeringsmakten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'landsstyremøte', 'hyp_word': 'måtte', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'å', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'å', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'utøver', 'hyp_word': 'utøve', 'lemma': 'utøver'}, {'error': 'substitute', 'ref_word': 'utøver', 'hyp_word': 'utøve', 'lemma': 'utøve'}, {'error': 'substitute', 'ref_word': 'irritert', 'hyp_word': 'iritert', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'irritert', 'hyp_word': 'iritert', 'lemma': 'irritere'}]\n",
      "[{'error': 'substitute', 'ref_word': 'knut', 'hyp_word': 'kisovehold', 'lemma': 'knut'}]\n",
      "[{'error': 'substitute', 'ref_word': 'mykje', 'hyp_word': 'mye', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'cella', 'hyp_word': 'sellasie', 'lemma': 'celle'}, {'error': 'substitute', 'ref_word': 'straffeutmålinga', 'hyp_word': 'straffeutmålinger', 'lemma': 'straffeutmåling'}, {'error': 'substitute', 'ref_word': 'høgsterett', 'hyp_word': 'høysterett', 'lemma': 'høgsterett'}]\n",
      "[{'error': 'substitute', 'ref_word': 'fleire', 'hyp_word': 'flere', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'gongar', 'hyp_word': 'ganger', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'varte'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'bli'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'vare'}, {'error': 'substitute', 'ref_word': 'stillinga', 'hyp_word': 'stillingen', 'lemma': 'stilling'}]\n",
      "[{'error': 'substitute', 'ref_word': 'åpenbart', 'hyp_word': 'bart', 'lemma': 'åpenbar'}, {'error': 'substitute', 'ref_word': 'åpenbart', 'hyp_word': 'bart', 'lemma': 'åpenbare'}, {'error': 'substitute', 'ref_word': 'han', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'en', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'millionar', 'hyp_word': 'millioner', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kronar', 'hyp_word': 'kroner', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'omsorga', 'hyp_word': 'omsorgen', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'deira', 'hyp_word': 'dei', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'endå', 'hyp_word': 'enda', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'strengare', 'hyp_word': 'strengere', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'meningsløse', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'av', 'hyp_word': 'avhengig', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'slåtten', 'hyp_word': 'slottensaken', 'lemma': 'slåtte'}, {'error': 'substitute', 'ref_word': 'slåtten', 'hyp_word': 'slottensaken', 'lemma': 'slått'}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'ifra', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'ifra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'føle', 'hyp_word': 'da', 'lemma': 'føle'}, {'error': 'substitute', 'ref_word': 'då', 'hyp_word': 'tingrett', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'då', 'hyp_word': 'tingrett', 'lemma': 'då'}, {'error': 'substitute', 'ref_word': 'satt', 'hyp_word': 'sa', 'lemma': 'satt'}, {'error': 'substitute', 'ref_word': 'satt', 'hyp_word': 'sa', 'lemma': 'sette'}, {'error': 'substitute', 'ref_word': 'satt', 'hyp_word': 'sa', 'lemma': 'sitte'}]\n",
      "[{'error': 'substitute', 'ref_word': 'retten', 'hyp_word': 'en', 'lemma': 'rett'}, {'error': 'substitute', 'ref_word': 'retten', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'retten', 'hyp_word': 'en', 'lemma': 'rette'}, {'error': 'substitute', 'ref_word': 'knytte', 'hyp_word': 'knytter', 'lemma': 'knytte'}, {'error': 'substitute', 'ref_word': 'knytte', 'hyp_word': 'knytter', 'lemma': 'knytte'}, {'error': 'substitute', 'ref_word': 'knytte', 'hyp_word': 'knytter', 'lemma': 'knyte'}, {'error': 'substitute', 'ref_word': 'knytte', 'hyp_word': 'knytter', 'lemma': 'knyte'}, {'error': 'substitute', 'ref_word': 'bevisa', 'hyp_word': 'bevise', 'lemma': 'bevis'}, {'error': 'substitute', 'ref_word': 'meine', 'hyp_word': 'mener', 'lemma': 'meine'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'påstand', 'hyp_word': 'påstanden', 'lemma': 'påstand'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'varte'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'bli'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'vart', 'hyp_word': 'var', 'lemma': 'vare'}]\n",
      "[{'error': 'substitute', 'ref_word': 'heile', 'hyp_word': 'hele', 'lemma': 'heil'}, {'error': 'substitute', 'ref_word': 'heile', 'hyp_word': 'hele', 'lemma': 'heil'}, {'error': 'substitute', 'ref_word': 'heile', 'hyp_word': 'hele', 'lemma': 'heile'}, {'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sjå', 'hyp_word': 'være', 'lemma': 'sjå'}, {'error': 'substitute', 'ref_word': 'sjå', 'hyp_word': 'være', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sjå', 'hyp_word': 'være', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nærmare', 'hyp_word': 'nærmere', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'då', 'hyp_word': 'da', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'då', 'hyp_word': 'da', 'lemma': 'då'}, {'error': 'substitute', 'ref_word': 'eit', 'hyp_word': 'et', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'tilhøre', 'hyp_word': 'tilhører', 'lemma': 'tilhøre'}, {'error': 'substitute', 'ref_word': 'avgjere', 'hyp_word': 'avgjøre', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'straffeutmålinga', 'hyp_word': 'straffeutmåling', 'lemma': 'straffeutmåling'}]\n",
      "[{'error': 'substitute', 'ref_word': 'slåtten', 'hyp_word': 'slotten', 'lemma': 'slåtte'}, {'error': 'substitute', 'ref_word': 'slåtten', 'hyp_word': 'slotten', 'lemma': 'slått'}]\n",
      "[{'error': 'substitute', 'ref_word': 'domfelte', 'hyp_word': 'domfeltet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'domfelte', 'hyp_word': 'domfeltet', 'lemma': 'domfelle'}, {'error': 'substitute', 'ref_word': 'han', 'hyp_word': 'indre', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'kalle', 'hyp_word': 'kaller', 'lemma': 'kalle'}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'vil', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'vil', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'de', 'lemma': 'jeg'}, {'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'de', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'reporter', 'hyp_word': 'er', 'lemma': 'reporter'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'slå', 'hyp_word': 'slår', 'lemma': 'slå'}, {'error': 'substitute', 'ref_word': 'slå', 'hyp_word': 'slår', 'lemma': 'slå'}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'den', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'den', 'lemma': 'demme'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'mista', 'hyp_word': 'miste', 'lemma': 'miste'}, {'error': 'substitute', 'ref_word': 'mista', 'hyp_word': 'miste', 'lemma': 'miste'}, {'error': 'substitute', 'ref_word': 'mista', 'hyp_word': 'miste', 'lemma': 'mista'}, {'error': 'substitute', 'ref_word': 'saman', 'hyp_word': 'sammen', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'et', 'hyp_word': 'eit', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'et', 'hyp_word': 'eit', 'lemma': 'ete'}, {'error': 'substitute', 'ref_word': 'selskap', 'hyp_word': 'eierselskap', 'lemma': 'selskap'}, {'error': 'substitute', 'ref_word': 'selskap', 'hyp_word': 'eierselskap', 'lemma': 'selskap'}, {'error': 'substitute', 'ref_word': 'selskap', 'hyp_word': 'eierselskap', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'driva', 'hyp_word': 'drive', 'lemma': 'drive'}]\n",
      "[{'error': 'substitute', 'ref_word': 'reindyrka', 'hyp_word': 'reindyrket', 'lemma': 'reindyrke'}, {'error': 'substitute', 'ref_word': 'reindyrka', 'hyp_word': 'reindyrket', 'lemma': 'reindyrke'}, {'error': 'substitute', 'ref_word': 'reindyrka', 'hyp_word': 'reindyrket', 'lemma': 'reindyrke'}, {'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tilbod', 'hyp_word': 'tilbud', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'il', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'etterspørse', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'etterspurt', 'hyp_word': 'ved', 'lemma': 'etterspurt'}, {'error': 'substitute', 'ref_word': 'etterspurt', 'hyp_word': 'ved', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'gjeld', 'hyp_word': 'gjelder', 'lemma': 'gjeld'}, {'error': 'substitute', 'ref_word': 'gjeld', 'hyp_word': 'gjelder', 'lemma': 'gjeld'}, {'error': 'substitute', 'ref_word': 'gjeld', 'hyp_word': 'gjelder', 'lemma': 'gjelde'}, {'error': 'substitute', 'ref_word': 'gjeld', 'hyp_word': 'gjelder', 'lemma': 'gjelde'}, {'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'seier', 'hyp_word': 'sier', 'lemma': 'sei'}, {'error': 'substitute', 'ref_word': 'seier', 'hyp_word': 'sier', 'lemma': 'seier'}, {'error': 'substitute', 'ref_word': 'seier', 'hyp_word': 'sier', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'konsesjonen', 'hyp_word': 'konsesjonene', 'lemma': 'konsesjon'}, {'error': 'substitute', 'ref_word': 'konsesjonen', 'hyp_word': 'konsesjonene', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'husstander', 'hyp_word': 'hustander', 'lemma': 'husstand'}, {'error': 'substitute', 'ref_word': 'husstander', 'hyp_word': 'hustander', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'homofile', 'hyp_word': 'homofiler', 'lemma': 'homofil'}, {'error': 'substitute', 'ref_word': 'homofile', 'hyp_word': 'homofiler', 'lemma': 'homofil'}]\n",
      "[{'error': 'substitute', 'ref_word': 'gren', 'hyp_word': 'green', 'lemma': 'gren'}, {'error': 'substitute', 'ref_word': 'gren', 'hyp_word': 'green', 'lemma': 'gren'}, {'error': 'substitute', 'ref_word': 'gren', 'hyp_word': 'green', 'lemma': 'gren'}, {'error': 'substitute', 'ref_word': 'gren', 'hyp_word': 'green', 'lemma': 'grene'}]\n",
      "[{'error': 'substitute', 'ref_word': 'enda', 'hyp_word': 'ennå', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'de', 'lemma': 'd'}]\n",
      "[{'error': 'substitute', 'ref_word': 'beslutningen', 'hyp_word': 'beslutninger', 'lemma': 'beslutning'}]\n",
      "[{'error': 'substitute', 'ref_word': 'flystasjonen', 'hyp_word': 'og', 'lemma': 'flystasjon'}, {'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'så', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'tøysete', 'hyp_word': 'tøyste', 'lemma': 'tøyset'}, {'error': 'substitute', 'ref_word': 'tøysete', 'hyp_word': 'tøyste', 'lemma': 'tøysete'}, {'error': 'substitute', 'ref_word': 'tøysete', 'hyp_word': 'tøyste', 'lemma': 'tøyset'}, {'error': 'substitute', 'ref_word': 'tøysete', 'hyp_word': 'tøyste', 'lemma': 'tøysete'}, {'error': 'substitute', 'ref_word': 'tøysete', 'hyp_word': 'tøyste', 'lemma': 'tøysete'}, {'error': 'substitute', 'ref_word': 'tøysete', 'hyp_word': 'tøyste', 'lemma': 'tøyse'}]\n",
      "[{'error': 'substitute', 'ref_word': 'flystripen', 'hyp_word': 'typen', 'lemma': 'flystripe'}, {'error': 'substitute', 'ref_word': 'være', 'hyp_word': 'bare', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'være', 'hyp_word': 'bare', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'freser', 'hyp_word': 'fraser', 'lemma': 'fres'}, {'error': 'substitute', 'ref_word': 'freser', 'hyp_word': 'fraser', 'lemma': 'freser'}, {'error': 'substitute', 'ref_word': 'freser', 'hyp_word': 'fraser', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'freser', 'hyp_word': 'fraser', 'lemma': 'frese'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'jeg', 'lemma': 'vie'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kolossalt', 'hyp_word': 'kollosalt', 'lemma': 'kolossal'}]\n",
      "[{'error': 'substitute', 'ref_word': 'markedshøyskole', 'hyp_word': 'høyskole', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ni', 'hyp_word': 'ny', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'de', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': 'fôr'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': 'fare'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'syns', 'hyp_word': 'synes', 'lemma': 'synse'}, {'error': 'substitute', 'ref_word': 'syns', 'hyp_word': 'synes', 'lemma': 'syn'}, {'error': 'substitute', 'ref_word': 'syns', 'hyp_word': 'synes', 'lemma': 'syn'}, {'error': 'substitute', 'ref_word': 'syns', 'hyp_word': 'synes', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'nok', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'han', 'hyp_word': 'kan', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ham', 'hyp_word': 'han', 'lemma': 'ham'}, {'error': 'substitute', 'ref_word': 'ham', 'hyp_word': 'han', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ham', 'hyp_word': 'han', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'han', 'hyp_word': 'man', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'vinmonpoleregler', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'til', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'til', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'leder', 'hyp_word': 'ledig', 'lemma': 'leder'}, {'error': 'substitute', 'ref_word': 'leder', 'hyp_word': 'ledig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'leder', 'hyp_word': 'ledig', 'lemma': 'lede'}, {'error': 'substitute', 'ref_word': 'leder', 'hyp_word': 'ledig', 'lemma': 'led'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kapital', 'hyp_word': 'kapitalet', 'lemma': 'kapital'}, {'error': 'substitute', 'ref_word': 'kapital', 'hyp_word': 'kapitalet', 'lemma': 'kapital'}]\n",
      "[{'error': 'substitute', 'ref_word': 'en', 'hyp_word': 'den', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'polet', 'hyp_word': 'pole', 'lemma': 'pole'}, {'error': 'substitute', 'ref_word': 'polet', 'hyp_word': 'pole', 'lemma': 'pol'}]\n",
      "[{'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': 'fôr'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': 'fare'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'fordi', 'lemma': '-'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ost', 'hyp_word': 'tre', 'lemma': 'ost'}, {'error': 'substitute', 'ref_word': 'ost', 'hyp_word': 'tre', 'lemma': 'oste'}]\n",
      "[{'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'dumt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'dumt', 'lemma': 'fôr'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'dumt', 'lemma': 'fare'}, {'error': 'substitute', 'ref_word': 'for', 'hyp_word': 'dumt', 'lemma': '-'}]\n",
      "[{'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'altså', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'oppgitt', 'hyp_word': 'oppgitte', 'lemma': 'oppgitt'}, {'error': 'substitute', 'ref_word': 'oppgitt', 'hyp_word': 'oppgitte', 'lemma': 'oppgi'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'til', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'til', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'hederlig', 'hyp_word': 'hedelig', 'lemma': 'hederlig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'dumma', 'hyp_word': 'dummet', 'lemma': 'dumme'}, {'error': 'substitute', 'ref_word': 'dumma', 'hyp_word': 'dummet', 'lemma': 'dumme'}, {'error': 'substitute', 'ref_word': 'dumma', 'hyp_word': 'dummet', 'lemma': 'dumme'}, {'error': 'substitute', 'ref_word': 'dumma', 'hyp_word': 'dummet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vinmonopolet', 'hyp_word': 'vinopole', 'lemma': 'vinmonopol'}]\n",
      "[{'error': 'substitute', 'ref_word': 'flokker', 'hyp_word': 'flakker', 'lemma': 'flokk'}, {'error': 'substitute', 'ref_word': 'flokker', 'hyp_word': 'flakker', 'lemma': 'flokke'}, {'error': 'substitute', 'ref_word': 'sammen', 'hyp_word': 'fram', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'burde', 'hyp_word': 'rutene', 'lemma': 'burde'}, {'error': 'substitute', 'ref_word': 'han', 'hyp_word': 'til', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'virkelig', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'virkelig', 'hyp_word': 'ikke', 'lemma': 'virkelig'}, {'error': 'substitute', 'ref_word': 'firmaet', 'hyp_word': 'firma', 'lemma': 'firma'}]\n",
      "[{'error': 'substitute', 'ref_word': 'få', 'hyp_word': 'får', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'få', 'hyp_word': 'får', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'år', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'folkevalgte', 'hyp_word': 'folkevalgt', 'lemma': 'folkevalgt'}, {'error': 'substitute', 'ref_word': 'folkevalgte', 'hyp_word': 'folkevalgt', 'lemma': 'folkevalgt'}]\n",
      "[{'error': 'substitute', 'ref_word': 'påstår', 'hyp_word': 'påse', 'lemma': 'påstå'}, {'error': 'substitute', 'ref_word': 'tilfellet', 'hyp_word': 'tilfelle', 'lemma': 'tilfelle'}]\n",
      "[{'error': 'substitute', 'ref_word': 'må', 'hyp_word': 'har', 'lemma': 'måtte'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'oppi', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'oppi', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'løst', 'hyp_word': 'løs', 'lemma': 'løs'}, {'error': 'substitute', 'ref_word': 'løst', 'hyp_word': 'løs', 'lemma': 'løs'}, {'error': 'substitute', 'ref_word': 'løst', 'hyp_word': 'løs', 'lemma': 'løs'}, {'error': 'substitute', 'ref_word': 'løst', 'hyp_word': 'løs', 'lemma': 'løse'}, {'error': 'substitute', 'ref_word': 'dere', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'et', 'hyp_word': 'spørsmål', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'et', 'hyp_word': 'spørsmål', 'lemma': 'ete'}, {'error': 'substitute', 'ref_word': 'godt', 'hyp_word': 'det', 'lemma': 'god'}, {'error': 'substitute', 'ref_word': 'godt', 'hyp_word': 'det', 'lemma': 'godt'}, {'error': 'substitute', 'ref_word': 'godt', 'hyp_word': 'det', 'lemma': 'godte'}, {'error': 'substitute', 'ref_word': 'spørsmål', 'hyp_word': 'er', 'lemma': 'spørsmål'}, {'error': 'substitute', 'ref_word': 'spørsmål', 'hyp_word': 'er', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'ting', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'reiser', 'hyp_word': 'reise', 'lemma': 'reise'}, {'error': 'substitute', 'ref_word': 'reiser', 'hyp_word': 'reise', 'lemma': 'reise'}, {'error': 'substitute', 'ref_word': 'reiser', 'hyp_word': 'reise', 'lemma': 'reis'}]\n",
      "[{'error': 'substitute', 'ref_word': 'lurer', 'hyp_word': 'lura', 'lemma': 'lur'}, {'error': 'substitute', 'ref_word': 'lurer', 'hyp_word': 'lura', 'lemma': 'lure'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'steinfelt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'steinfelt', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'steinfelt', 'lemma': 'åe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'spres', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'liksom', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'komme', 'hyp_word': 'kan', 'lemma': 'komme'}, {'error': 'substitute', 'ref_word': 'komme', 'hyp_word': 'kan', 'lemma': 'komme'}]\n",
      "[{'error': 'substitute', 'ref_word': 'forbli', 'hyp_word': 'bli', 'lemma': 'forbli'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ikke', 'hyp_word': 'er', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikke', 'hyp_word': 'er', 'lemma': 'ikke'}, {'error': 'substitute', 'ref_word': 'lavinntekt', 'hyp_word': 'inntekt', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'viktige', 'hyp_word': 'viktig', 'lemma': 'viktig'}, {'error': 'substitute', 'ref_word': 'viktige', 'hyp_word': 'viktig', 'lemma': 'viktig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'hedvik', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'hedvik', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'formodentlig', 'hyp_word': 'muntlig', 'lemma': 'formodentlig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'var', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'var', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'timen', 'hyp_word': 'time', 'lemma': 'time'}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'hans', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'stede', 'hyp_word': 'stedet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'stede', 'hyp_word': 'stedet', 'lemma': 'stede'}]\n",
      "[{'error': 'substitute', 'ref_word': 'attentatforsøk', 'hyp_word': 'atentatforsøk', 'lemma': 'attentatforsøk'}, {'error': 'substitute', 'ref_word': 'attentatforsøk', 'hyp_word': 'atentatforsøk', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'vitnet', 'hyp_word': 'vitne', 'lemma': 'vitne'}, {'error': 'substitute', 'ref_word': 'vitnet', 'hyp_word': 'vitne', 'lemma': 'vitne'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ja', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ja', 'hyp_word': 'en', 'lemma': 'ja'}]\n",
      "[{'error': 'substitute', 'ref_word': 'nord', 'hyp_word': 'nordpå', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nord', 'hyp_word': 'nordpå', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nord', 'hyp_word': 'nordpå', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'c', 'hyp_word': 'se', 'lemma': 'c'}]\n",
      "[{'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'spekulert', 'hyp_word': 'ert', 'lemma': 'spekulere'}, {'error': 'substitute', 'ref_word': 'forhandla', 'hyp_word': 'forhandlet', 'lemma': 'forhandle'}, {'error': 'substitute', 'ref_word': 'forhandla', 'hyp_word': 'forhandlet', 'lemma': 'forhandle'}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'en', 'lemma': 'eie'}, {'error': 'substitute', 'ref_word': 'ho', 'hyp_word': 'hun', 'lemma': 'ho'}, {'error': 'substitute', 'ref_word': 'ho', 'hyp_word': 'hun', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'seinare', 'hyp_word': 'seinere', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'da', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'diskuterast', 'hyp_word': 'diskuteres', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ta', 'hyp_word': 'tas', 'lemma': 'ta'}, {'error': 'substitute', 'ref_word': 'si', 'hyp_word': 'i', 'lemma': 'si'}, {'error': 'substitute', 'ref_word': 'si', 'hyp_word': 'i', 'lemma': 'si'}, {'error': 'substitute', 'ref_word': 'no', 'hyp_word': 'noe', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'no', 'hyp_word': 'noe', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'no', 'hyp_word': 'noe', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'lunsj', 'hyp_word': 'lønns', 'lemma': 'lunsj'}, {'error': 'substitute', 'ref_word': 'lunsj', 'hyp_word': 'lønns', 'lemma': 'lunsje'}, {'error': 'substitute', 'ref_word': 'då', 'hyp_word': 'da', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'då', 'hyp_word': 'da', 'lemma': 'då'}]\n",
      "[{'error': 'substitute', 'ref_word': 'informasjonen', 'hyp_word': 'informasjoner', 'lemma': 'informasjon'}, {'error': 'substitute', 'ref_word': 'kontrollkomitéen', 'hyp_word': 'kontrollkomiteen', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'overvurderer', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'overvurderer', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'overvurderer', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'vurderer', 'hyp_word': 'et', 'lemma': 'vurdere'}, {'error': 'substitute', 'ref_word': 'frå', 'hyp_word': 'fra', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'seier', 'hyp_word': 'sier', 'lemma': 'sei'}, {'error': 'substitute', 'ref_word': 'seier', 'hyp_word': 'sier', 'lemma': 'seier'}, {'error': 'substitute', 'ref_word': 'seier', 'hyp_word': 'sier', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'følgje', 'hyp_word': 'følge', 'lemma': 'følgje'}, {'error': 'substitute', 'ref_word': 'følgje', 'hyp_word': 'følge', 'lemma': 'følgje'}, {'error': 'substitute', 'ref_word': 'følgje', 'hyp_word': 'følge', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'følgje', 'hyp_word': 'følge', 'lemma': 'følgje'}]\n",
      "[{'error': 'substitute', 'ref_word': 'påstandane', 'hyp_word': 'påstandene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'påstandane', 'hyp_word': 'påstandene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'blei', 'hyp_word': 'ble', 'lemma': 'blei'}, {'error': 'substitute', 'ref_word': 'blei', 'hyp_word': 'ble', 'lemma': 'bli'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'saka', 'hyp_word': 'saken', 'lemma': 'sake'}, {'error': 'substitute', 'ref_word': 'saka', 'hyp_word': 'saken', 'lemma': 'sake'}, {'error': 'substitute', 'ref_word': 'saka', 'hyp_word': 'saken', 'lemma': 'sake'}, {'error': 'substitute', 'ref_word': 'saka', 'hyp_word': 'saken', 'lemma': 'sak'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kva', 'hyp_word': 'hva', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kva', 'hyp_word': 'hva', 'lemma': '-'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'påstandar', 'hyp_word': 'påstander', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'no', 'hyp_word': 'nå', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'no', 'hyp_word': 'nå', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'no', 'hyp_word': 'nå', 'lemma': '-'}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'møter', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'møter', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'skjebne', 'hyp_word': 'sheba', 'lemma': 'skjebne'}]\n",
      "[{'error': 'substitute', 'ref_word': 'tåler', 'hyp_word': 'tore', 'lemma': 'tåle'}, {'error': 'substitute', 'ref_word': 'kulde', 'hyp_word': 'kulla', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'di', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'gjør', 'hyp_word': 'jr', 'lemma': 'gjø'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kirkens', 'hyp_word': 'kikenes', 'lemma': 'kirke'}]\n",
      "[{'error': 'substitute', 'ref_word': 'menneskerettighetsorganisasjoner', 'hyp_word': 'mennesker', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'her', 'hyp_word': 'tilsagnsnr', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'bakstad', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'bakstad', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'gir', 'hyp_word': 'gjerme', 'lemma': 'gir'}, {'error': 'substitute', 'ref_word': 'gir', 'hyp_word': 'gjerme', 'lemma': 'gir'}, {'error': 'substitute', 'ref_word': 'gir', 'hyp_word': 'gjerme', 'lemma': 'gire'}, {'error': 'substitute', 'ref_word': 'gir', 'hyp_word': 'gjerme', 'lemma': 'gi'}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'di', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'blåste', 'hyp_word': 'bloste', 'lemma': 'blåse'}, {'error': 'substitute', 'ref_word': 'blåste', 'hyp_word': 'bloste', 'lemma': 'blåse'}]\n",
      "[{'error': 'substitute', 'ref_word': 'utfor', 'hyp_word': 'ut', 'lemma': 'utfor'}, {'error': 'substitute', 'ref_word': 'utfor', 'hyp_word': 'ut', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'autovernet', 'hyp_word': 'vernet', 'lemma': 'autovern'}, {'error': 'substitute', 'ref_word': 'hva', 'hyp_word': 'kvasoskjedd', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'utfor', 'hyp_word': 'utføre', 'lemma': 'utfor'}, {'error': 'substitute', 'ref_word': 'utfor', 'hyp_word': 'utføre', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'uka', 'hyp_word': 'å', 'lemma': 'uke'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'prøve', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nu', 'hyp_word': 'nå', 'lemma': 'nu'}]\n",
      "[{'error': 'substitute', 'ref_word': 'hvor', 'hyp_word': 'kor', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'oppå', 'hyp_word': 'på', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'inne', 'hyp_word': 'inn', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}, {'error': 'substitute', 'ref_word': 'mo', 'hyp_word': 'moiranaver', 'lemma': 'mo'}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'rana', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'rana', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'typiske', 'hyp_word': 'typisk', 'lemma': 'typisk'}, {'error': 'substitute', 'ref_word': 'typiske', 'hyp_word': 'typisk', 'lemma': 'typisk'}, {'error': 'substitute', 'ref_word': 'knekkebrød', 'hyp_word': 'brød', 'lemma': 'knekkebrød'}, {'error': 'substitute', 'ref_word': 'knekkebrød', 'hyp_word': 'brød', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kjøttboller', 'hyp_word': 'kjøttbullar', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'resten', 'hyp_word': 'restene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'resten', 'hyp_word': 'restene', 'lemma': 'rest'}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'har', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'har', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'skryter', 'hyp_word': 'skryte', 'lemma': 'skryter'}, {'error': 'substitute', 'ref_word': 'skryter', 'hyp_word': 'skryte', 'lemma': 'skryte'}, {'error': 'substitute', 'ref_word': 'fått', 'hyp_word': 'få', 'lemma': 'få'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'en', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'firetida', 'hyp_word': 'tida', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'om', 'hyp_word': 'men', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'om', 'hyp_word': 'men', 'lemma': 'om'}, {'error': 'substitute', 'ref_word': 'natta', 'hyp_word': 'åtte', 'lemma': 'natte'}, {'error': 'substitute', 'ref_word': 'natta', 'hyp_word': 'åtte', 'lemma': 'natte'}, {'error': 'substitute', 'ref_word': 'natta', 'hyp_word': 'åtte', 'lemma': 'natte'}, {'error': 'substitute', 'ref_word': 'natta', 'hyp_word': 'åtte', 'lemma': 'natt'}, {'error': 'substitute', 'ref_word': 'stod', 'hyp_word': 'sto', 'lemma': 'stå'}]\n",
      "[{'error': 'substitute', 'ref_word': 'hver', 'hyp_word': 'vera', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': 'demme'}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'der', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'e', 'lemma': 'jeg'}, {'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'e', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kjerringa', 'hyp_word': 'kjærringa', 'lemma': 'kjerring'}, {'error': 'substitute', 'ref_word': 'oppe', 'hyp_word': 'opp', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'oppe', 'hyp_word': 'opp', 'lemma': 'oppe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': 'demme'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kortet', 'hyp_word': 'korkje', 'lemma': 'korte'}, {'error': 'substitute', 'ref_word': 'kortet', 'hyp_word': 'korkje', 'lemma': 'kort'}, {'error': 'substitute', 'ref_word': 'problemer', 'hyp_word': 'problem', 'lemma': 'problem'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'eili', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'eili', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ifrå', 'hyp_word': 'ifra', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'brygger', 'hyp_word': 'bruer', 'lemma': 'brygge'}, {'error': 'substitute', 'ref_word': 'brygger', 'hyp_word': 'bruer', 'lemma': 'brygge'}, {'error': 'substitute', 'ref_word': 'brygger', 'hyp_word': 'bruer', 'lemma': 'brygger'}, {'error': 'substitute', 'ref_word': 'brygger', 'hyp_word': 'bruer', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'brygger', 'hyp_word': 'bruer', 'lemma': 'brygge'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vært', 'hyp_word': 'verte', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ordet', 'hyp_word': 'året', 'lemma': 'ord'}, {'error': 'substitute', 'ref_word': 'ordet', 'hyp_word': 'året', 'lemma': 'orde'}]\n",
      "[{'error': 'substitute', 'ref_word': 'nitten', 'hyp_word': 'ten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'geografiske', 'hyp_word': 'geografisk', 'lemma': 'geografisk'}, {'error': 'substitute', 'ref_word': 'geografiske', 'hyp_word': 'geografisk', 'lemma': 'geografisk'}, {'error': 'substitute', 'ref_word': 'gis', 'hyp_word': 'gi', 'lemma': 'gi'}]\n",
      "[{'error': 'substitute', 'ref_word': 'som', 'hyp_word': 'svar', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'mer', 'hyp_word': 'med', 'lemma': 'm'}, {'error': 'substitute', 'ref_word': 'mer', 'hyp_word': 'med', 'lemma': 'mye'}, {'error': 'substitute', 'ref_word': 'da', 'hyp_word': 'av', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'canadiske', 'hyp_word': 'kanadiske', 'lemma': 'canadisk'}, {'error': 'substitute', 'ref_word': 'canadiske', 'hyp_word': 'kanadiske', 'lemma': 'canadisk'}, {'error': 'substitute', 'ref_word': 'geografiske', 'hyp_word': 'geografisk', 'lemma': 'geografisk'}, {'error': 'substitute', 'ref_word': 'geografiske', 'hyp_word': 'geografisk', 'lemma': 'geografisk'}, {'error': 'substitute', 'ref_word': 'da', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'særlig', 'hyp_word': 'salig', 'lemma': 'særlig'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'og', 'lemma': 'åe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ett', 'hyp_word': 'et', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ett', 'hyp_word': 'et', 'lemma': 'ete'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'om', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'om', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'om', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'fra', 'hyp_word': 'konfliktstudenfra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'studie', 'hyp_word': 'studier', 'lemma': 'studie'}, {'error': 'substitute', 'ref_word': 'studiet', 'hyp_word': 'studier', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'informasjonssystemer', 'hyp_word': 'informasjonssystemet', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'av', 'hyp_word': 'at', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'foreslå', 'hyp_word': 'foreslår', 'lemma': 'foreslå'}]\n",
      "[{'error': 'substitute', 'ref_word': 'nå', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nå', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nå', 'hyp_word': 'og', 'lemma': 'nå'}, {'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'verken', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'nok', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'nok', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'bosteds', 'hyp_word': 'bosted', 'lemma': 'bosted'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'fritidseiendom', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'eiendomsskatten', 'hyp_word': 'eiendomskatten', 'lemma': 'eiendomsskatt'}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'men', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'av', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'går', 'hyp_word': 'gå', 'lemma': 'gåre'}, {'error': 'substitute', 'ref_word': 'at', 'hyp_word': 'president', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'nemlig', 'lemma': 'ved'}, {'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'nemlig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'bolig', 'hyp_word': 'boliger', 'lemma': 'bolig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'som', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'du', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'verkeverfall', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'verkeverfall', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'enige', 'hyp_word': 'enig', 'lemma': 'enig'}, {'error': 'substitute', 'ref_word': 'enige', 'hyp_word': 'enig', 'lemma': 'enig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'du', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'du', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'at', 'hyp_word': 'du', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'noen', 'hyp_word': 'noe', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'at', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'at', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'at', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'skatte', 'hyp_word': 'skatter', 'lemma': 'skatte'}]\n",
      "[{'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'få', 'hyp_word': 'på', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'få', 'hyp_word': 'på', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'seg', 'hyp_word': 'det', 'lemma': 'sige'}, {'error': 'substitute', 'ref_word': 'seg', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vil', 'hyp_word': 'jo', 'lemma': 'ville'}, {'error': 'substitute', 'ref_word': 'at', 'hyp_word': 'president', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'har', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'har', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'du', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'som', 'hyp_word': 'ispansminister', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'og', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'og', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'minstepensjonister', 'hyp_word': 'minimal', 'lemma': 'minstepensjonist'}, {'error': 'substitute', 'ref_word': 'minstepensjonister', 'hyp_word': 'minimal', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'minimalt', 'hyp_word': 'inntekt', 'lemma': 'minimal'}, {'error': 'substitute', 'ref_word': 'med', 'hyp_word': 'de', 'lemma': 'med'}, {'error': 'substitute', 'ref_word': 'med', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'inntekt', 'hyp_word': 'har', 'lemma': 'inntekt'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'de', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'de', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'sjøl', 'hyp_word': 'selv', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'glasshus', 'hyp_word': 'glasshuset', 'lemma': 'glasshus'}, {'error': 'substitute', 'ref_word': 'glasshus', 'hyp_word': 'glasshuset', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'først', 'hyp_word': 'første', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sier', 'hyp_word': 'egne', 'lemma': 'si'}]\n",
      "[{'error': 'substitute', 'ref_word': 'sosialistene', 'hyp_word': 'sosialisten', 'lemma': 'sosialist'}, {'error': 'substitute', 'ref_word': 'fjernet', 'hyp_word': 'fjerna', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fjernet', 'hyp_word': 'fjerna', 'lemma': 'fjerne'}]\n",
      "[{'error': 'substitute', 'ref_word': 'kom', 'hyp_word': 'president', 'lemma': 'komme'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'det', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'regjering', 'hyp_word': 'kan', 'lemma': 'regjering'}, {'error': 'substitute', 'ref_word': 'sosialdemokraten', 'hyp_word': 'sosialdemokratene', 'lemma': 'sosialdemokrat'}, {'error': 'substitute', 'ref_word': 'sørget', 'hyp_word': 'sørgeforat', 'lemma': 'sørge'}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'sverige', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'sverige', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'venstresiden', 'hyp_word': 'venstre', 'lemma': 'venstreside'}, {'error': 'substitute', 'ref_word': 'venstresiden', 'hyp_word': 'venstre', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'seg', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'seg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'med', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'med', 'hyp_word': 'lang', 'lemma': 'med'}, {'error': 'substitute', 'ref_word': 'med', 'hyp_word': 'lang', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'senterpartiet', 'hyp_word': 'nei', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sine', 'hyp_word': 'sitt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sine', 'hyp_word': 'sitt', 'lemma': 'sine'}, {'error': 'substitute', 'ref_word': 'fjernet', 'hyp_word': 'den', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fjernet', 'hyp_word': 'den', 'lemma': 'fjerne'}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'fjerna', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'fjerna', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'i', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'trenger', 'hyp_word': 'tenker', 'lemma': 'trenge'}, {'error': 'substitute', 'ref_word': 'trenger', 'hyp_word': 'tenker', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'du', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'du', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'jeg', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'eiendomsskatt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'innan', 'hyp_word': 'innen', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'en', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'blitt', 'lemma': 'ha'}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'jo', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'jo', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'lenge', 'hyp_word': 'lenger', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kom', 'hyp_word': 'kommer', 'lemma': 'komme'}]\n",
      "[{'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'det', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'med', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'med', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'var', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'null', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'null', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'null', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'null', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'senterpartiet', 'hyp_word': 'senterpartiregjering', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'null', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'null', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'så', 'hyp_word': 'også', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'så', 'hyp_word': 'også', 'lemma': 'så'}, {'error': 'substitute', 'ref_word': 'så', 'hyp_word': 'også', 'lemma': 'se'}]\n",
      "[{'error': 'substitute', 'ref_word': 'bompenger', 'hyp_word': 'bompengere', 'lemma': 'bompenger'}, {'error': 'substitute', 'ref_word': 'ned', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'regjeringen', 'hyp_word': 'regjerigen', 'lemma': 'regjering'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'så', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'så', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'innover', 'hyp_word': 'over', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'innover', 'hyp_word': 'over', 'lemma': 'innovere'}]\n",
      "[{'error': 'substitute', 'ref_word': 'skrivast', 'hyp_word': 'skrives', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'eiendomsskatt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'sine', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kommunane', 'hyp_word': 'kommuner', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'så', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'så', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'tjueen', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'kommuner', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'skriv', 'hyp_word': 'skriver', 'lemma': 'skriv'}, {'error': 'substitute', 'ref_word': 'skriv', 'hyp_word': 'skriver', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'skriv', 'hyp_word': 'skriver', 'lemma': 'skrive'}, {'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'eiendomsskatt', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'som', 'hyp_word': 'at', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'skulle', 'hyp_word': 'skal', 'lemma': 'skulle'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'samordna', 'hyp_word': 'samordnet', 'lemma': 'samordne'}, {'error': 'substitute', 'ref_word': 'samordna', 'hyp_word': 'samordnet', 'lemma': 'samordne'}, {'error': 'substitute', 'ref_word': 'samordna', 'hyp_word': 'samordnet', 'lemma': 'samordne'}, {'error': 'substitute', 'ref_word': 'samordna', 'hyp_word': 'samordnet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'samordna', 'hyp_word': 'samordnet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'inntektssystemet', 'hyp_word': 'systemet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kommunar', 'hyp_word': 'kommuner', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'vesentlig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'vesentlig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'vesentlig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'vesentlig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ei', 'hyp_word': 'vesentlig', 'lemma': 'eie'}, {'error': 'substitute', 'ref_word': 'vesentleg', 'hyp_word': 'inntektskjeldei', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'takk', 'lemma': 'jeg'}, {'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'takk', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'må', 'hyp_word': 'president', 'lemma': 'måtte'}, {'error': 'substitute', 'ref_word': 'om', 'hyp_word': 'med', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'om', 'hyp_word': 'med', 'lemma': 'om'}]\n",
      "[{'error': 'substitute', 'ref_word': 'synes', 'hyp_word': 'syns', 'lemma': 'syne'}, {'error': 'substitute', 'ref_word': 'synes', 'hyp_word': 'syns', 'lemma': 'synes'}, {'error': 'substitute', 'ref_word': 'så', 'hyp_word': 'syn', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'så', 'hyp_word': 'syn', 'lemma': 'så'}, {'error': 'substitute', 'ref_word': 'så', 'hyp_word': 'syn', 'lemma': 'se'}]\n",
      "[{'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'du', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'med', 'lemma': 'ved'}, {'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'med', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'det', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'det', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'skattene', 'hyp_word': 'skatteavgifter', 'lemma': 'skatt'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'men', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'i', 'lemma': 'vie'}]\n",
      "[{'error': 'substitute', 'ref_word': 'sammen', 'hyp_word': 'dersom', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'med', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'med', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sånn', 'hyp_word': 'som', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'at', 'hyp_word': 'de', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'synes', 'hyp_word': 'syns', 'lemma': 'syne'}, {'error': 'substitute', 'ref_word': 'synes', 'hyp_word': 'syns', 'lemma': 'synes'}, {'error': 'substitute', 'ref_word': 'synd', 'hyp_word': 'syn', 'lemma': 'synd'}, {'error': 'substitute', 'ref_word': 'synd', 'hyp_word': 'syn', 'lemma': 'synde'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'arbeider', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'arbeider', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'eiendomsskatt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'manglar', 'hyp_word': 'mangler', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'manglar', 'hyp_word': 'mangler', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vore', 'hyp_word': 'våre', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'hovudsak', 'hyp_word': 'hovedsak', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'gi', 'hyp_word': 'gir', 'lemma': 'gi'}, {'error': 'substitute', 'ref_word': 'ville', 'hyp_word': 'klarer', 'lemma': 'vill'}, {'error': 'substitute', 'ref_word': 'ville', 'hyp_word': 'klarer', 'lemma': 'vill'}, {'error': 'substitute', 'ref_word': 'ville', 'hyp_word': 'klarer', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ville', 'hyp_word': 'klarer', 'lemma': 'ville'}, {'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'og', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'taler', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'er', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'er', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'er', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'er', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'som', 'hyp_word': 'kommerstaten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fikk', 'hyp_word': 'helgheim', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'meg', 'hyp_word': 'komme', 'lemma': 'mige'}, {'error': 'substitute', 'ref_word': 'meg', 'hyp_word': 'komme', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nå', 'hyp_word': 'da', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nå', 'hyp_word': 'da', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nå', 'hyp_word': 'da', 'lemma': 'nå'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ti', 'hyp_word': 'tiårige', 'lemma': 'ti'}, {'error': 'substitute', 'ref_word': 'ti', 'hyp_word': 'tiårige', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ti', 'hyp_word': 'tiårige', 'lemma': 'tie'}, {'error': 'substitute', 'ref_word': 'år', 'hyp_word': 'bystyre', 'lemma': 'år'}, {'error': 'substitute', 'ref_word': 'år', 'hyp_word': 'bystyre', 'lemma': 'år'}, {'error': 'substitute', 'ref_word': 'år', 'hyp_word': 'bystyre', 'lemma': 'år'}, {'error': 'substitute', 'ref_word': 'år', 'hyp_word': 'bystyre', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fjernet', 'hyp_word': 'fjerner', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fjernet', 'hyp_word': 'fjerner', 'lemma': 'fjerne'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'ved', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'ved', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'jeg', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'fram', 'hyp_word': 'frem', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'og', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'gikk', 'hyp_word': 'også', 'lemma': 'gå'}, {'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'gått', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'så', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'vann', 'hyp_word': 'avvannet', 'lemma': 'vann'}, {'error': 'substitute', 'ref_word': 'vann', 'hyp_word': 'avvannet', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vann', 'hyp_word': 'avvannet', 'lemma': 'vanne'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'avløp', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'eiendomsskatt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'på', 'hyp_word': 'for', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'eiendomsskatt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'frå', 'hyp_word': 'fra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'svarbrevet', 'hyp_word': 'brevet', 'lemma': 'svarbrev'}]\n",
      "[{'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'andre', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'andre', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'hordeogså', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'hordeogså', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'hordeogså', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'grunneiere', 'hyp_word': 'grunneier', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'grunneiere', 'hyp_word': 'grunneier', 'lemma': 'grunneier'}, {'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'som', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'har', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'har', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'har', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'være', 'hyp_word': 'vært', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'være', 'hyp_word': 'vært', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'helhetlige', 'hyp_word': 'helheten', 'lemma': 'helhetlig'}, {'error': 'substitute', 'ref_word': 'helhetlige', 'hyp_word': 'helheten', 'lemma': 'helhetlig'}, {'error': 'substitute', 'ref_word': 'helhetlige', 'hyp_word': 'helheten', 'lemma': 'helhetlig'}, {'error': 'substitute', 'ref_word': 'helhetlige', 'hyp_word': 'helheten', 'lemma': 'helhetlig'}]\n",
      "[{'error': 'substitute', 'ref_word': 'mente', 'hyp_word': 'det', 'lemma': 'mene'}, {'error': 'substitute', 'ref_word': 'mente', 'hyp_word': 'det', 'lemma': 'mene'}, {'error': 'substitute', 'ref_word': 'mente', 'hyp_word': 'det', 'lemma': 'mente'}, {'error': 'substitute', 'ref_word': 'at', 'hyp_word': 'var', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vil', 'hyp_word': 'så', 'lemma': 'ville'}, {'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'bare', 'lemma': 'jeg'}, {'error': 'substitute', 'ref_word': 'jeg', 'hyp_word': 'bare', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'min', 'hyp_word': 'men', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'min', 'hyp_word': 'men', 'lemma': 'mine'}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'president', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'synes', 'hyp_word': 'syns', 'lemma': 'syne'}, {'error': 'substitute', 'ref_word': 'synes', 'hyp_word': 'syns', 'lemma': 'synes'}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sjøl', 'hyp_word': 'sjøla', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'tror', 'hyp_word': 'trur', 'lemma': 'tro'}, {'error': 'substitute', 'ref_word': 'konsekvent', 'hyp_word': 'konsekvente', 'lemma': 'konsekvent'}]\n",
      "[{'error': 'substitute', 'ref_word': 'norgesmestre', 'hyp_word': 'norgesmestere', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'norgesmestre', 'hyp_word': 'norgesmester', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'deres', 'hyp_word': 'sis', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ikke', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikke', 'hyp_word': 'det', 'lemma': 'ikke'}, {'error': 'substitute', 'ref_word': 'virkelighetsfjernt', 'hyp_word': 'virkelighetsvert', 'lemma': 'virkelighetsfjern'}, {'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'dem', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ut', 'hyp_word': 'eiendomsskatt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'eigedomsskatt', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'fra', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'fra', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'tatt', 'hyp_word': 'de', 'lemma': 'ta'}]\n",
      "[{'error': 'substitute', 'ref_word': 'litt', 'hyp_word': 'grann', 'lemma': 'litt'}, {'error': 'substitute', 'ref_word': 'litt', 'hyp_word': 'grann', 'lemma': 'lite'}]\n",
      "[{'error': 'substitute', 'ref_word': 'bustad', 'hyp_word': 'busta', 'lemma': 'bustad'}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'fritidseiendom', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'frå', 'hyp_word': 'fra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'frå', 'hyp_word': 'fra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'tjuefjorten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kommunane', 'hyp_word': 'kommunene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nytta', 'hyp_word': 'nytte', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nytta', 'hyp_word': 'nytte', 'lemma': 'nytte'}, {'error': 'substitute', 'ref_word': 'nytta', 'hyp_word': 'nytte', 'lemma': 'nytte'}]\n",
      "[{'error': 'substitute', 'ref_word': 'inn', 'hyp_word': 'ja', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'skal', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'skal', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'skal', 'lemma': 'åe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'innbyggerne', 'hyp_word': 'innbyggernes', 'lemma': 'innbygger'}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ifra', 'hyp_word': 'fra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'at', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ha', 'hyp_word': 'besta', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'rett', 'hyp_word': 'retten', 'lemma': 'rett'}, {'error': 'substitute', 'ref_word': 'rett', 'hyp_word': 'retten', 'lemma': 'rett'}, {'error': 'substitute', 'ref_word': 'rett', 'hyp_word': 'retten', 'lemma': 'rette'}]\n",
      "[{'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'om', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'om', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'da', 'hyp_word': 'når', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'an', 'hyp_word': 'ann', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'an', 'hyp_word': 'ann', 'lemma': 'ane'}, {'error': 'substitute', 'ref_word': 'sitte', 'hyp_word': 'på', 'lemma': 'sitte'}]\n",
      "[{'error': 'substitute', 'ref_word': 'da', 'hyp_word': 'det', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'denne', 'hyp_word': 'den', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'heter', 'hyp_word': 'etter', 'lemma': 'hete'}, {'error': 'substitute', 'ref_word': 'heter', 'hyp_word': 'etter', 'lemma': 'hete'}, {'error': 'substitute', 'ref_word': 'heter', 'hyp_word': 'etter', 'lemma': 'hete'}, {'error': 'substitute', 'ref_word': 'lunsj', 'hyp_word': 'lønns', 'lemma': 'lunsj'}, {'error': 'substitute', 'ref_word': 'lunsj', 'hyp_word': 'lønns', 'lemma': 'lunsje'}, {'error': 'substitute', 'ref_word': 'syvende', 'hyp_word': 'sven', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'yte', 'hyp_word': 'skal', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'yte', 'hyp_word': 'skal', 'lemma': 'yte'}, {'error': 'substitute', 'ref_word': 'yte', 'hyp_word': 'skal', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'syvende', 'hyp_word': 'synes', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'vist', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': 'd'}]\n",
      "[{'error': 'substitute', 'ref_word': 'avsløre', 'hyp_word': 'avklare', 'lemma': 'avsløre'}, {'error': 'substitute', 'ref_word': 'én', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'å', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'taler', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'tingene', 'hyp_word': 'og', 'lemma': 'ting'}, {'error': 'substitute', 'ref_word': 'tingene', 'hyp_word': 'og', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'kommunane', 'hyp_word': 'kommunene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'oppgåvene', 'hyp_word': 'oppgavene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dei', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kommunar', 'hyp_word': 'kommuner', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'teke', 'hyp_word': 'innført', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'eiendomsskatt', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'eiendomsskatt', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'vi', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'de', 'lemma': 'demme'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'så', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'så', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'så', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'tingene', 'hyp_word': 'tingen', 'lemma': 'ting'}, {'error': 'substitute', 'ref_word': 'tingene', 'hyp_word': 'tingen', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nei', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nei', 'hyp_word': 'jeg', 'lemma': 'nei'}, {'error': 'substitute', 'ref_word': 'nei', 'hyp_word': 'jeg', 'lemma': 'nei'}, {'error': 'substitute', 'ref_word': 'nei', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nei', 'hyp_word': 'jeg', 'lemma': 'neie'}]\n",
      "[{'error': 'substitute', 'ref_word': 'gård', 'hyp_word': 'går', 'lemma': 'gård'}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'som', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'der', 'hyp_word': 'som', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'vi', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'for', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'for', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'for', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'for', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'altså', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'altså', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'altså', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'kan', 'hyp_word': 'bruke', 'lemma': 'kunne'}, {'error': 'substitute', 'ref_word': 'kan', 'hyp_word': 'bruke', 'lemma': 'kan'}, {'error': 'substitute', 'ref_word': 'bruke', 'hyp_word': 'den', 'lemma': 'bruke'}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'altså', 'hyp_word': 'også', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'eller', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'eller', 'hyp_word': 'og', 'lemma': 'elle'}]\n",
      "[{'error': 'substitute', 'ref_word': 'inn', 'hyp_word': 'penger', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'penger', 'hyp_word': 'på', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'penger', 'hyp_word': 'på', 'lemma': 'penge'}, {'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sånn', 'hyp_word': 'som', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'siden', 'hyp_word': 'sida', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'siden', 'hyp_word': 'sida', 'lemma': 'side'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ned', 'hyp_word': 'jeg', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'i', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'gangen', 'hyp_word': 'to', 'lemma': 'gang'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'tusen', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'tusen', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'da', 'hyp_word': 'år', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ifrå', 'hyp_word': 'ifra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kommunane', 'hyp_word': 'kommunene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dårleg', 'hyp_word': 'dårlig', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'kjem', 'hyp_word': 'beror', 'lemma': 'kjemme'}, {'error': 'substitute', 'ref_word': 'kjem', 'hyp_word': 'beror', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'sinsyk', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'sinsyk', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'netto', 'hyp_word': 'nettopp', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'netto', 'hyp_word': 'nettopp', 'lemma': 'netto'}]\n",
      "[{'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'så', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'drive', 'hyp_word': 'driver', 'lemma': 'drive'}, {'error': 'substitute', 'ref_word': 'drive', 'hyp_word': 'driver', 'lemma': 'drive'}]\n",
      "[{'error': 'substitute', 'ref_word': 'og', 'hyp_word': 'karakterisere', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'kommunane', 'hyp_word': 'kommunene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'løysa', 'hyp_word': 'løse', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'løysa', 'hyp_word': 'løse', 'lemma': 'løyse'}, {'error': 'substitute', 'ref_word': 'ein', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tilfredsstillande', 'hyp_word': 'tilfredsstillende', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikkje', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'man', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'man', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'kuttet', 'hyp_word': 'kutte', 'lemma': 'kutte'}, {'error': 'substitute', 'ref_word': 'kuttet', 'hyp_word': 'kutte', 'lemma': 'kutt'}]\n",
      "[{'error': 'substitute', 'ref_word': 'to', 'hyp_word': 'i', 'lemma': 'to'}, {'error': 'substitute', 'ref_word': 'to', 'hyp_word': 'i', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'to', 'hyp_word': 'i', 'lemma': 'toe'}, {'error': 'substitute', 'ref_word': 'tusen', 'hyp_word': 'totusenogfemten', 'lemma': 'tusen'}, {'error': 'substitute', 'ref_word': 'tusen', 'hyp_word': 'totusenogfemten', 'lemma': 'tusen'}, {'error': 'substitute', 'ref_word': 'tusen', 'hyp_word': 'totusenogfemten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tusen', 'hyp_word': 'totusenogfemten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': 'd'}]\n",
      "[{'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'dette', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'dem', 'hyp_word': 'dette', 'lemma': 'demme'}, {'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'det', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'kollegaene', 'hyp_word': 'kollegene', 'lemma': 'kollega'}, {'error': 'substitute', 'ref_word': 'kollegaene', 'hyp_word': 'kollegene', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'overbrakte', 'hyp_word': 'overbringe', 'lemma': 'overbringe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'tilrette', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'tjuenitten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'man', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'vi', 'hyp_word': 'man', 'lemma': 'vie'}, {'error': 'substitute', 'ref_word': 'eiendomsskatt', 'hyp_word': 'eiendomsskatter', 'lemma': 'eiendomsskatt'}]\n",
      "[{'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'til', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'nitten', 'hyp_word': 'tjuenitten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'det', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'året', 'hyp_word': 'årene', 'lemma': 'året'}, {'error': 'substitute', 'ref_word': 'året', 'hyp_word': 'årene', 'lemma': 'året'}, {'error': 'substitute', 'ref_word': 'året', 'hyp_word': 'årene', 'lemma': 'året'}, {'error': 'substitute', 'ref_word': 'året', 'hyp_word': 'årene', 'lemma': 'år'}]\n",
      "[{'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'viktigste', 'lemma': 'e'}, {'error': 'substitute', 'ref_word': 'er', 'hyp_word': 'viktigste', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'hele', 'hyp_word': 'heile', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'hele', 'hyp_word': 'heile', 'lemma': 'hel'}, {'error': 'substitute', 'ref_word': 'hele', 'hyp_word': 'heile', 'lemma': 'hel'}, {'error': 'substitute', 'ref_word': 'hele', 'hyp_word': 'heile', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'hele', 'hyp_word': 'heile', 'lemma': 'hele'}]\n",
      "[{'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'skattlegging', 'hyp_word': 'skartlegging', 'lemma': 'skattlegge'}]\n",
      "[{'error': 'substitute', 'ref_word': 'åtte', 'hyp_word': 'åtteforslag', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'åtte', 'hyp_word': 'åtteforslag', 'lemma': 'eie'}, {'error': 'substitute', 'ref_word': 'forslag', 'hyp_word': 'og', 'lemma': 'forslag'}, {'error': 'substitute', 'ref_word': 'forslag', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'omgang', 'hyp_word': 'president', 'lemma': 'omgang'}, {'error': 'substitute', 'ref_word': 'omgang', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'venstresida', 'hyp_word': 'venstre', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'venstresida', 'hyp_word': 'venstre', 'lemma': 'venstreside'}]\n",
      "[{'error': 'substitute', 'ref_word': 'siden', 'hyp_word': 'sia', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'siden', 'hyp_word': 'sia', 'lemma': 'side'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ut', 'hyp_word': 'utover', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ordførere', 'hyp_word': 'ordfører', 'lemma': 'ordfører'}, {'error': 'substitute', 'ref_word': 'ordførere', 'hyp_word': 'ordfører', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'høyere', 'hyp_word': 'høgere', 'lemma': 'høy'}]\n",
      "[{'error': 'substitute', 'ref_word': 'til', 'hyp_word': 'av', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'skuffende', 'hyp_word': 'pene', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'skuffende', 'hyp_word': 'pene', 'lemma': 'skuffe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'derigjennom', 'hyp_word': 'kanskje', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fått', 'hyp_word': 'ta', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'sheriffen', 'hyp_word': 'opp', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sheriffen', 'hyp_word': 'opp', 'lemma': 'sheriff'}, {'error': 'substitute', 'ref_word': 'av', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sitt', 'hyp_word': 'vær', 'lemma': 'sitte'}, {'error': 'substitute', 'ref_word': 'sitt', 'hyp_word': 'vær', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'store', 'hyp_word': 'så', 'lemma': 'stor'}, {'error': 'substitute', 'ref_word': 'store', 'hyp_word': 'så', 'lemma': 'stor'}, {'error': 'substitute', 'ref_word': 'forbilde', 'hyp_word': 'god', 'lemma': 'forbilde'}, {'error': 'substitute', 'ref_word': 'fascinerende', 'hyp_word': 'fasinerende', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'fascinerende', 'hyp_word': 'fasinerende', 'lemma': 'fascinere'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'fremskrittsparti', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'fremskrittsparti', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'fremskrittsparti', 'lemma': 'åe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'skatt', 'hyp_word': 'skattete', 'lemma': 'skatt'}, {'error': 'substitute', 'ref_word': 'skatt', 'hyp_word': 'skattete', 'lemma': 'skatte'}, {'error': 'substitute', 'ref_word': 'sittet', 'hyp_word': 'suttet', 'lemma': 'sitte'}, {'error': 'substitute', 'ref_word': 'vært', 'hyp_word': 'våre', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'høyere', 'hyp_word': 'høgere', 'lemma': 'høy'}]\n",
      "[{'error': 'substitute', 'ref_word': 'vært', 'hyp_word': 'være', 'lemma': 'være'}, {'error': 'substitute', 'ref_word': 'høyere', 'hyp_word': 'høgere', 'lemma': 'høy'}, {'error': 'substitute', 'ref_word': 'høyere', 'hyp_word': 'høgere', 'lemma': 'høy'}]\n",
      "[{'error': 'substitute', 'ref_word': 'fra', 'hyp_word': 'ifra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'president', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'president', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'fra', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'tretten', 'hyp_word': 'tjuetretten', 'lemma': 'trette'}, {'error': 'substitute', 'ref_word': 'tretten', 'hyp_word': 'tjuetretten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'der', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'der', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'der', 'lemma': 'var'}, {'error': 'substitute', 'ref_word': 'var', 'hyp_word': 'der', 'lemma': 'være'}]\n",
      "[{'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'tjuenitten', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'altså', 'hyp_word': 'også', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'ikke', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'ikke', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'ikke', 'lemma': 'mane'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'den', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'den', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'den', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'tjue', 'hyp_word': 'tjuetjue', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'og', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'selvsagt', 'hyp_word': 'sjølsagt', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'selvsagt', 'hyp_word': 'sjølsagt', 'lemma': 'selvsagt'}, {'error': 'substitute', 'ref_word': 'selvsagt', 'hyp_word': 'sjølsagt', 'lemma': 'selvsagt'}, {'error': 'substitute', 'ref_word': 'selvsagt', 'hyp_word': 'sjølsagt', 'lemma': 'selvsagt'}, {'error': 'substitute', 'ref_word': 'fra', 'hyp_word': 'ifra', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'fremskrittspartiets', 'hyp_word': 'si', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'merket', 'hyp_word': 'merka', 'lemma': 'merke'}, {'error': 'substitute', 'ref_word': 'merket', 'hyp_word': 'merka', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'merket', 'hyp_word': 'merka', 'lemma': 'merke'}, {'error': 'substitute', 'ref_word': 'også', 'hyp_word': 'og', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'han', 'hyp_word': 'som', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'påpekte', 'hyp_word': 'påpeker', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'påpekte', 'hyp_word': 'påpeker', 'lemma': 'påpeke'}, {'error': 'substitute', 'ref_word': 'påpekte', 'hyp_word': 'påpeker', 'lemma': 'påpeke'}, {'error': 'substitute', 'ref_word': 'viktige', 'hyp_word': 'viktig', 'lemma': 'viktig'}, {'error': 'substitute', 'ref_word': 'viktige', 'hyp_word': 'viktig', 'lemma': 'viktig'}, {'error': 'substitute', 'ref_word': 'inntektskilder', 'hyp_word': 'inntektskilde', 'lemma': 'inntektskilde'}, {'error': 'substitute', 'ref_word': 'inntektskilder', 'hyp_word': 'inntektskilde', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'se', 'hyp_word': 'sjå', 'lemma': 'se'}]\n",
      "[{'error': 'substitute', 'ref_word': 'konsekvensene', 'hyp_word': 'konsekvenser', 'lemma': 'konsekvens'}, {'error': 'substitute', 'ref_word': 'endringene', 'hyp_word': 'har', 'lemma': 'endring'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'vært', 'lemma': 'i'}, {'error': 'substitute', 'ref_word': 'i', 'hyp_word': 'vært', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'verk', 'hyp_word': 'verker', 'lemma': 'verk'}, {'error': 'substitute', 'ref_word': 'verk', 'hyp_word': 'verker', 'lemma': 'verk'}, {'error': 'substitute', 'ref_word': 'verk', 'hyp_word': 'verker', 'lemma': 'verke'}, {'error': 'substitute', 'ref_word': 'fått', 'hyp_word': 'medført', 'lemma': 'få'}, {'error': 'substitute', 'ref_word': 'som', 'hyp_word': 'der', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': 'man'}, {'error': 'substitute', 'ref_word': 'man', 'hyp_word': 'en', 'lemma': 'mane'}]\n",
      "[{'error': 'substitute', 'ref_word': 'ryggraden', 'hyp_word': 'ryggrad', 'lemma': 'ryggrad'}, {'error': 'substitute', 'ref_word': 'ikke', 'hyp_word': 'kje', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'ikke', 'hyp_word': 'kje', 'lemma': 'ikke'}, {'error': 'substitute', 'ref_word': 'bestående', 'hyp_word': 'bestå', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'bestående', 'hyp_word': 'bestå', 'lemma': 'bestå'}, {'error': 'substitute', 'ref_word': 'av', 'hyp_word': 'stående', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'regjeringspartiene', 'hyp_word': 'regjeringspartier', 'lemma': 'regjeringsparti'}]\n",
      "[{'error': 'substitute', 'ref_word': 'siden', 'hyp_word': 'sida', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'siden', 'hyp_word': 'sida', 'lemma': 'side'}, {'error': 'substitute', 'ref_word': 'holdt', 'hyp_word': 'holdte', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'holdt', 'hyp_word': 'holdte', 'lemma': 'holde'}, {'error': 'substitute', 'ref_word': 'representanten', 'hyp_word': 'representant', 'lemma': 'representant'}]\n",
      "[{'error': 'substitute', 'ref_word': 'selvstyret', 'hyp_word': 'sjølstyret', 'lemma': 'selvstyre'}, {'error': 'substitute', 'ref_word': 'selvstyret', 'hyp_word': 'sjølstyret', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'selv', 'hyp_word': 'sjøl', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'selv', 'hyp_word': 'sjøl', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'selv', 'hyp_word': 'sjøl', 'lemma': '-'}, {'error': 'substitute', 'ref_word': 'bekymringen', 'hyp_word': 'bekymringa', 'lemma': 'bekymring'}, {'error': 'substitute', 'ref_word': 'dypt', 'hyp_word': 'president', 'lemma': 'dyp'}]\n",
      "[{'error': 'substitute', 'ref_word': 'infrastruktur', 'hyp_word': 'truktur', 'lemma': 'infrastruktur'}, {'error': 'substitute', 'ref_word': 'di', 'hyp_word': 'de', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'di', 'hyp_word': 'de', 'lemma': 'die'}, {'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'her', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'har', 'hyp_word': 'her', 'lemma': 'ha'}, {'error': 'substitute', 'ref_word': 'dersom', 'hyp_word': 'der', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'noen', 'hyp_word': 'noe', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'med', 'lemma': 'ved'}, {'error': 'substitute', 'ref_word': 'ved', 'hyp_word': 'med', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'den', 'hyp_word': 'en', 'lemma': 'd'}, {'error': 'substitute', 'ref_word': 'målingen', 'hyp_word': 'av', 'lemma': 'måling'}]\n",
      "[{'error': 'substitute', 'ref_word': 'eiendomsskatt', 'hyp_word': 'iendomsskattsån', 'lemma': 'eiendomsskatt'}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'altså', 'hyp_word': 'også', 'lemma': ''}]\n",
      "[{'error': 'substitute', 'ref_word': 'masseoppsigelser', 'hyp_word': 'masseoppsigelse', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'sjukepleiere', 'hyp_word': 'sykepleiere', 'lemma': 'sjukepleier'}, {'error': 'substitute', 'ref_word': 'skolene', 'hyp_word': 'skolen', 'lemma': 'skole'}]\n",
      "[{'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'går', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'går', 'lemma': 'å'}, {'error': 'substitute', 'ref_word': 'å', 'hyp_word': 'går', 'lemma': 'åe'}, {'error': 'substitute', 'ref_word': 'tappe', 'hyp_word': 'med', 'lemma': 'tappe'}]\n",
      "[{'error': 'substitute', 'ref_word': 'de', 'hyp_word': 'det', 'lemma': ''}, {'error': 'substitute', 'ref_word': 'denne', 'hyp_word': 'her', 'lemma': ''}]\n"
     ]
    }
   ],
   "source": [
    "for row in df_lemma.itertuples():\n",
    "    if len(row.lemma_found) != 0:\n",
    "        print(row.lemma_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Errors - NB Tale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_errors_df = df_pos.copy(deep=True)\n",
    "\n",
    "idx_included = []\n",
    "for idx, row in nb_errors_df.iterrows():\n",
    "    if \"free\" in row.segment_id:\n",
    "        idx_included.append(idx)\n",
    "\n",
    "nb_errors_df = nb_errors_df.filter(items=idx_included, axis=0)\n",
    "len(nb_errors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower asd: 25\n",
      "higher asd: 13\n"
     ]
    }
   ],
   "source": [
    "lower_mod_asd_nb = nb_errors_df[nb_errors_df[\"mod_asd\"] < nb_errors_df[\"orig_asd\"]]\n",
    "higher_mod_asd_nb = nb_errors_df[nb_errors_df[\"orig_asd\"] < nb_errors_df[\"mod_asd\"]]\n",
    "\n",
    "print(\"lower asd:\", len(lower_mod_asd_nb))\n",
    "print(\"higher asd:\", len(higher_mod_asd_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1945642/2778154589.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lower_mod_asd_nb[\"asd_score_diff\"] = lower_mod_asd_nb[\"orig_asd\"] - lower_mod_asd_nb[\"mod_asd\"]\n",
      "/tmp/ipykernel_1945642/2778154589.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  higher_mod_asd_nb[\"asd_score_diff\"] = higher_mod_asd_nb[\"mod_asd\"] - higher_mod_asd_nb[\"orig_asd\"]\n"
     ]
    }
   ],
   "source": [
    "lower_mod_asd_nb[\"asd_score_diff\"] = lower_mod_asd_nb[\"orig_asd\"] - lower_mod_asd_nb[\"mod_asd\"]\n",
    "lower_mod_asd_nb = lower_mod_asd_nb.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "higher_mod_asd_nb[\"asd_score_diff\"] = higher_mod_asd_nb[\"mod_asd\"] - higher_mod_asd_nb[\"orig_asd\"]\n",
    "higher_mod_asd_nb = higher_mod_asd_nb.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['utføre', 'ut'], 'hyp_pos': ['VERB', 'ADP']}\n",
      "{'type': 'substitute', 'ref': ['utfor', 'autovernet'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['o', 'vernet'], 'hyp_pos': ['NUM', 'NOUN']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.40\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utfor </td><td>utfor</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utføre</td><td>ut   </td><td>o    </td><td>vernet  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.022</td><td>0.024 </td><td>0.015  </td><td>0.016</td><td>0.054 </td><td>0.028</td><td>0.057</td><td>0.13</td><td>0.637 </td><td>0.664</td><td>0.706</td><td>0.532   </td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['utfor'], 'ref_pos': ['ADP'], 'hyp': ['utføre'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utfor </td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utføre</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.005</td><td>0.008 </td><td>0.004  </td><td>0.005</td><td>0.013 </td><td>0.015</td><td>0.024</td><td>0.113</td><td>0.541 </td><td>0.054</td><td>0.052   </td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['åtte'], 'ref_pos': ['NUM'], 'hyp': ['åtteogsytti'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['og', 'sytti'], 'ref_pos': ['CCONJ', 'NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>og   </td><td>sy  </td><td>##tti</td><td>##tti</td><td>##tti</td><td>##tti</td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>i     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>åtte </td><td>åtte</td><td>åtte </td><td>##ogs</td><td>##ytt</td><td>##i  </td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>##aria</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.083</td><td>0.012</td><td>0.006</td><td>0.004</td><td>0.004</td><td>0.008</td><td>0.009 </td><td>0.024</td><td>0.097</td><td>0.722</td><td>0.53</td><td>0.455</td><td>0.687</td><td>0.598</td><td>0.584</td><td>0.019    </td><td>0.006</td><td>0.004</td><td>0.006</td><td>0.006</td><td>0.006 </td><td>0.01 </td><td>0.009</td><td>0.011</td><td>0.026 </td><td>0.667 </td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0   </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0     </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0    </td><td>0        </td><td>0 </td><td>0 </td><td>0  </td><td>0  </td><td>0     </td><td>0    </td><td>0   </td><td>0  </td><td>0     </td><td>0</td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['fordi', 'at'], 'ref_pos': ['SCONJ', 'SCONJ'], 'hyp': ['fred', 'vil'], 'hyp_pos': ['NOUN', 'AUX']}\n",
      "{'type': 'delete', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['variabler', 'som'], 'ref_pos': ['NOUN', 'PRON'], 'hyp': ['variable', 'så'], 'hyp_pos': ['ADJ', 'ADV']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>som     </td><td>var     </td><td>var  </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fred    </td><td>vil  </td><td>vil  </td><td>vil  </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variable </td><td>variable</td><td>variable</td><td>så   </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.048</td><td>0.047 </td><td>0.027</td><td>0.011</td><td>0.015  </td><td>0.016</td><td>0.025</td><td>0.019  </td><td>0.026</td><td>0.103   </td><td>0.428   </td><td>0.553</td><td>0.581</td><td>0.579</td><td>0.123</td><td>0.143</td><td>0.066</td><td>0.029</td><td>0.041</td><td>0.049</td><td>0.354    </td><td>0.631   </td><td>0.676   </td><td>0.707</td><td>0.166</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['at'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['som'], 'ref_pos': ['PRON'], 'hyp': ['svar'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['var', 'mye'], 'ref_pos': ['AUX', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>variabler</td><td>som  </td><td>var  </td><td>mye </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>fordi</td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>svar     </td><td>svar </td><td>svar </td><td>svar</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.078</td><td>0.009 </td><td>0.003</td><td>0.003</td><td>0.004  </td><td>0.004</td><td>0.005</td><td>0.005  </td><td>0.005</td><td>0.009   </td><td>0.041</td><td>0.451</td><td>0.019</td><td>0.019</td><td>0.024</td><td>0.019</td><td>0.015</td><td>0.021</td><td>0.038</td><td>0.148    </td><td>0.638    </td><td>0.656</td><td>0.697</td><td>0.68</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['konflikter'], 'ref_pos': ['NOUN'], 'hyp': ['konflikten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['borgerkrig'], 'ref_pos': ['NOUN'], 'hyp': ['borgekrig'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>borgerkrig</td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikten</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borg      </td><td>##ek      </td><td>##rig     </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.006   </td><td>0.005</td><td>0.003</td><td>0.003</td><td>0.003</td><td>0.004   </td><td>0.025</td><td>0.158     </td><td>0.039     </td><td>0.021</td><td>0.037</td><td>0.007</td><td>0.022 </td><td>0.78    </td><td>0.1</td><td>0.025</td><td>0.018</td><td>0.014     </td><td>0.015</td><td>0.028</td><td>0.015</td><td>0.052</td><td>0.549     </td><td>0.743     </td><td>0.546     </td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.05\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.002</td><td>0.001   </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.001   </td><td>0.001</td><td>0.003     </td><td>0.003     </td><td>0.003</td><td>0.03 </td><td>0.005</td><td>0.02  </td><td>0.781   </td><td>0.097</td><td>0.022</td><td>0.016</td><td>0.01      </td><td>0.011</td><td>0.008</td><td>0.008</td><td>0.008</td><td>0.01      </td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['skryter'], 'ref_pos': ['VERB'], 'hyp': ['skryte'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['dem'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['nå'], 'ref_pos': ['ADV'], 'hyp': ['du'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'delete', 'ref': ['jeg'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['to'], 'ref_pos': ['NUM'], 'hyp': ['ho'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['salam'], 'hyp_pos': ['X']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>sal  </td><td>##aman</td><td>##ca </td><td>skryter</td><td>dem   </td><td>av   </td><td>nå   </td><td>hadde</td><td>jeg  </td><td>i    </td><td>vår  </td><td>prøvd</td><td>å    </td><td>fått </td><td>to   </td><td>sy   </td><td>##tten</td><td>##åringen</td><td>da   </td><td>på   </td><td>spansk</td><td>skole</td><td>i    </td><td>i    </td><td>i   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>i    </td><td>sal  </td><td>##aman</td><td>##ca </td><td>skryte </td><td>skryte</td><td>av   </td><td>du   </td><td>hadde</td><td>hadde</td><td>i    </td><td>vår  </td><td>prøvd</td><td>å    </td><td>fått </td><td>ho   </td><td>sy   </td><td>##tten</td><td>##åringen</td><td>da   </td><td>på   </td><td>spansk</td><td>skole</td><td>i    </td><td>sal  </td><td>##am</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.093</td><td>0.073</td><td>0.081</td><td>0.036 </td><td>0.071</td><td>0.319  </td><td>0.778 </td><td>0.186</td><td>0.677</td><td>0.135</td><td>0.689</td><td>0.061</td><td>0.039</td><td>0.039</td><td>0.033</td><td>0.042</td><td>0.627</td><td>0.062</td><td>0.039 </td><td>0.058    </td><td>0.047</td><td>0.026</td><td>0.033 </td><td>0.027</td><td>0.185</td><td>0.695</td><td>0.71</td><td>0.018</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['skryter'], 'ref_pos': ['VERB'], 'hyp': ['skryte'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['dem'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['nå'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['fått'], 'ref_pos': ['VERB'], 'hyp': ['få'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['salam'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.22\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>sal </td><td>##aman</td><td>##ca </td><td>skryter</td><td>dem   </td><td>av   </td><td>nå   </td><td>hadde</td><td>jeg  </td><td>i    </td><td>vår  </td><td>prøvd</td><td>å    </td><td>fått </td><td>to  </td><td>sy   </td><td>##tten</td><td>##åringen</td><td>da   </td><td>på   </td><td>spansk</td><td>skole</td><td>i    </td><td>i    </td><td>i    </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>i    </td><td>sal </td><td>##aman</td><td>##ca </td><td>skryte </td><td>skryte</td><td>av   </td><td>av   </td><td>hadde</td><td>jeg  </td><td>i    </td><td>vår  </td><td>prøvd</td><td>å    </td><td>få   </td><td>to  </td><td>sy   </td><td>##tten</td><td>##åringen</td><td>da   </td><td>på   </td><td>spansk</td><td>skole</td><td>i    </td><td>sal  </td><td>##am </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.043</td><td>0.07</td><td>0.029 </td><td>0.058</td><td>0.307  </td><td>0.756 </td><td>0.137</td><td>0.757</td><td>0.065</td><td>0.021</td><td>0.027</td><td>0.019</td><td>0.024</td><td>0.062</td><td>0.157</td><td>0.02</td><td>0.019</td><td>0.021 </td><td>0.021    </td><td>0.022</td><td>0.023</td><td>0.028 </td><td>0.023</td><td>0.174</td><td>0.685</td><td>0.694</td><td>0.016</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 5\n",
    "\n",
    "for row in lower_mod_asd_nb.itertuples():\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an example with errors to demonstrate errors that we want to analyze: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower ASD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Notes for paper:</u>\n",
    "- difference in the results will highly depend on the token vocabulary, look at the vocab of the tokenizer\n",
    "- systematic analysis - looking at the overall POS counts, word stems & lemmas (we expect ASD not to be too high if mistaken)\n",
    "- look if fixed phrases, multi-word expressions, or compound words were correctly transcribed\n",
    "- distribution \n",
    "- entropy of the logits produced by the model with original loss & with modified loss\n",
    "- example-based \n",
    "- changing the metric does not necessarily reduce the word error rate\n",
    "- it's okay to introduce the proposed approach and the difficulties faced during implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>ref_str</th>\n",
       "      <th>orig_asr</th>\n",
       "      <th>orig_cer</th>\n",
       "      <th>orig_wer</th>\n",
       "      <th>orig_asd</th>\n",
       "      <th>mod_asr</th>\n",
       "      <th>mod_cer</th>\n",
       "      <th>mod_wer</th>\n",
       "      <th>mod_asd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2021H264-fullStorting0510Stortinget-20210510-1...</td>\n",
       "      <td>sine egne inntekter  den eneste skatten kommun...</td>\n",
       "      <td>sine egne inntekter den ene skatten kommunesty...</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.077353</td>\n",
       "      <td>sine egne inntekter den ene skatten kommunesty...</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.077353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            segment_id  \\\n",
       "812  2021H264-fullStorting0510Stortinget-20210510-1...   \n",
       "\n",
       "                                               ref_str  \\\n",
       "812  sine egne inntekter  den eneste skatten kommun...   \n",
       "\n",
       "                                              orig_asr  orig_cer  orig_wer  \\\n",
       "812  sine egne inntekter den ene skatten kommunesty...  0.057377       0.1   \n",
       "\n",
       "     orig_asd                                            mod_asr   mod_cer  \\\n",
       "812  0.077353  sine egne inntekter den ene skatten kommunesty...  0.057377   \n",
       "\n",
       "     mod_wer   mod_asd  \n",
       "812      0.1  0.077353  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results[merged_results[\"segment_id\"] == \"2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_84\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher asd: 180\n",
      "lower asd: 202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1945642/4056735429.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lower_asd_df[\"asd_score_diff\"] = lower_asd_df[\"orig_asd\"] - lower_asd_df[\"mod_asd\"]\n",
      "/tmp/ipykernel_1945642/4056735429.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  higher_asd_df[\"asd_score_diff\"] = higher_asd_df[\"mod_asd\"] - higher_asd_df[\"orig_asd\"]\n"
     ]
    }
   ],
   "source": [
    "higher_asd_df = df_pos[df_pos[\"orig_asd\"] < df_pos[\"mod_asd\"]]\n",
    "lower_asd_df = df_pos[df_pos[\"mod_asd\"] < df_pos[\"orig_asd\"]]\n",
    "\n",
    "print(\"higher asd:\", len(higher_asd_df))\n",
    "print(\"lower asd:\", len(lower_asd_df))\n",
    "\n",
    "lower_asd_df[\"asd_score_diff\"] = lower_asd_df[\"orig_asd\"] - lower_asd_df[\"mod_asd\"]\n",
    "lower_asd_df = lower_asd_df.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "higher_asd_df[\"asd_score_diff\"] = higher_asd_df[\"mod_asd\"] - higher_asd_df[\"orig_asd\"]\n",
    "higher_asd_df = higher_asd_df.sort_values(by=[\"asd_score_diff\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when nouns are incorrect, we expect the asd score to be higher\n",
    "# fine-tuning with ASD should reduce these types of errors\n",
    "\n",
    "noun_errors_df = lower_asd_df.copy(deep=True)\n",
    "\n",
    "orig_noun_errors = []\n",
    "mod_noun_errors = []\n",
    "for row in noun_errors_df.itertuples():\n",
    "    orig_errors = []\n",
    "    mod_errors = []\n",
    "    for error_dict in row.orig_error_dict:\n",
    "        if \"NOUN\" in error_dict[\"ref_pos\"]:\n",
    "            orig_errors.append((error_dict[\"ref\"], error_dict[\"hyp\"]))\n",
    "    for error_dict in row.mod_error_dict:\n",
    "        if \"NOUN\" in error_dict[\"ref_pos\"]:\n",
    "            mod_errors.append((error_dict[\"ref\"], error_dict[\"hyp\"]))\n",
    "    orig_noun_errors.append(orig_errors)\n",
    "    mod_noun_errors.append(mod_errors)\n",
    "\n",
    "noun_errors_df[\"orig_noun_errors\"] = orig_noun_errors\n",
    "noun_errors_df[\"mod_noun_errors\"] = mod_noun_errors\n",
    "\n",
    "# noun_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment id: p1_g02_m2_5_x-free_cut_15\n",
      "ref: og klarte akkurat å stoppe før han for utfor autovernet\n",
      "-----\n",
      "orig hyp: og klarte akkurat å stoppe før han for utføre ut o vernet\n",
      "orig_wer: 0.4 orig_asd 0.40001090747217\n",
      "-----\n",
      "mod hyp: og klarte akkurat å stoppe før han for utføre autovernet\n",
      "mod_wer: 0.1 mod_asd: 0.1293611331282719\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['utføre', 'ut'], 'hyp_pos': ['VERB', 'ADP']}\n",
      "{'type': 'substitute', 'ref': ['utfor', 'autovernet'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['o', 'vernet'], 'hyp_pos': ['NUM', 'NOUN']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.40\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utfor </td><td>utfor</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for </td><td>utføre</td><td>ut   </td><td>o    </td><td>vernet  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.071</td><td>0.022</td><td>0.024 </td><td>0.015  </td><td>0.016</td><td>0.054 </td><td>0.028</td><td>0.057</td><td>0.13</td><td>0.637 </td><td>0.664</td><td>0.706</td><td>0.532   </td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['utfor'], 'ref_pos': ['ADP'], 'hyp': ['utføre'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 13, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utfor </td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>klarte</td><td>akkurat</td><td>å    </td><td>stoppe</td><td>før  </td><td>han  </td><td>for  </td><td>utføre</td><td>auto </td><td>##vernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.012</td><td>0.005</td><td>0.008 </td><td>0.004  </td><td>0.005</td><td>0.013 </td><td>0.015</td><td>0.024</td><td>0.113</td><td>0.541 </td><td>0.054</td><td>0.052   </td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_79\n",
      "ref: mot at interessegrupper får betale reiser og konferanser han vil ha regler mot dette det må ikke herske tvil om\n",
      "-----\n",
      "orig hyp: mot at interessegrupper får betale reiser og konferanser han vil ha regler mot dette det vil\n",
      "orig_wer: 0.25 orig_asd 0.239950370724734\n",
      "-----\n",
      "mod hyp: mot at interessegrupper får betale reiser og konferanser han vil ha regler mot dette det må ikke herske tvil om\n",
      "mod_wer: 0.0 mod_asd: 0.0\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['må'], 'ref_pos': ['AUX'], 'hyp': ['vil'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['ikke', 'herske', 'tvil', 'om'], 'ref_pos': ['PART', 'VERB', 'NOUN', 'ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mot  </td><td>at   </td><td>interesse</td><td>##grupper</td><td>får  </td><td>betale</td><td>reiser</td><td>og   </td><td>konferanser</td><td>han  </td><td>vil  </td><td>ha  </td><td>regler</td><td>mot  </td><td>dette</td><td>det  </td><td>må   </td><td>ikke </td><td>her  </td><td>##ske</td><td>tvil </td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mot  </td><td>at   </td><td>interesse</td><td>##grupper</td><td>får  </td><td>betale</td><td>reiser</td><td>og   </td><td>konferanser</td><td>han  </td><td>vil  </td><td>ha  </td><td>regler</td><td>mot  </td><td>dette</td><td>det  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.053</td><td>0.007</td><td>0.004</td><td>0.002    </td><td>0.002    </td><td>0.003</td><td>0.003 </td><td>0.002 </td><td>0.002</td><td>0.004      </td><td>0.009</td><td>0.021</td><td>0.01</td><td>0.006 </td><td>0.014</td><td>0.027</td><td>0.101</td><td>0.598</td><td>0.791</td><td>0.878</td><td>0.836</td><td>0.843</td><td>0.688</td><td>0.018</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mot</td><td>at</td><td>interesse</td><td>##grupper</td><td>får</td><td>betale</td><td>reiser</td><td>og</td><td>konferanser</td><td>han</td><td>vil</td><td>ha</td><td>regler</td><td>mot</td><td>dette</td><td>det</td><td>må</td><td>ikke</td><td>her</td><td>##ske</td><td>tvil</td><td>om</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mot</td><td>at</td><td>interesse</td><td>##grupper</td><td>får</td><td>betale</td><td>reiser</td><td>og</td><td>konferanser</td><td>han</td><td>vil</td><td>ha</td><td>regler</td><td>mot</td><td>dette</td><td>det</td><td>må</td><td>ikke</td><td>her</td><td>##ske</td><td>tvil</td><td>om</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0 </td><td>0        </td><td>0        </td><td>0  </td><td>0     </td><td>0     </td><td>0 </td><td>0          </td><td>0  </td><td>0  </td><td>0 </td><td>0     </td><td>0  </td><td>0    </td><td>0  </td><td>0 </td><td>0   </td><td>0  </td><td>0    </td><td>0   </td><td>0 </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_45\n",
      "ref: elsker skatter som er helt uavhengig av folks evne til å betale skatt venstresida vil ha skyhøye engangsavgifter og dyrest\n",
      "-----\n",
      "orig hyp: elsker skatter som er helt uavhenget av folks evne til å betale skatt venstre side vil ha skyhøge eengangsavgifter og dyrest\n",
      "orig_wer: 0.25 orig_asd 0.3397096462899228\n",
      "-----\n",
      "mod hyp: elsker skatter som er helt uavhengig av folks evne til å betale skatt venstresida vil ha skyhøye eengangsavgifter og dyrest\n",
      "mod_wer: 0.05 mod_asd: 0.1115618886097413\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['uavhengig'], 'ref_pos': ['ADJ'], 'hyp': ['uavhenget'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['venstre'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['venstresida'], 'ref_pos': ['NOUN'], 'hyp': ['side'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['skyhøye', 'engangsavgifter'], 'ref_pos': ['ADJ', 'NOUN'], 'hyp': ['skyhøge', 'eengangsavgifter'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>uavhengig</td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>##høye</td><td>engangs</td><td>engangs</td><td>##avgifter</td><td>##avgifter</td><td>og  </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uav      </td><td>##heng   </td><td>##et     </td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstre </td><td>side </td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høg </td><td>##e   </td><td>een    </td><td>##gang </td><td>##savgift </td><td>##er      </td><td>og  </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.016</td><td>0.012 </td><td>0.015  </td><td>0.014</td><td>0.021</td><td>0.035</td><td>0.345    </td><td>0.48     </td><td>0.557    </td><td>0.075</td><td>0.026</td><td>0.028</td><td>0.019</td><td>0.022</td><td>0.032 </td><td>0.051</td><td>0.361   </td><td>0.472</td><td>0.056</td><td>0.041</td><td>0.081</td><td>0.404 </td><td>0.449 </td><td>0.64   </td><td>0.611  </td><td>0.254     </td><td>0.457     </td><td>0.04</td><td>0.037</td><td>0.043</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['engangsavgifter'], 'ref_pos': ['NOUN'], 'hyp': ['eengangsavgifter'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.11\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>engangs</td><td>engangs</td><td>##avgifter</td><td>##avgifter</td><td>og   </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>elsker</td><td>skatter</td><td>som  </td><td>er   </td><td>helt </td><td>uavhengig</td><td>av   </td><td>folks</td><td>evne </td><td>til  </td><td>å    </td><td>betale</td><td>skatt</td><td>venstres</td><td>##ida</td><td>vil  </td><td>ha   </td><td>sky  </td><td>##høye</td><td>een    </td><td>##gang </td><td>##savgift </td><td>##er      </td><td>og   </td><td>dyre </td><td>##st </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.002 </td><td>0.002  </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001    </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002 </td><td>0.002</td><td>0.002   </td><td>0.001</td><td>0.002</td><td>0.003</td><td>0.009</td><td>0.015 </td><td>0.592  </td><td>0.588  </td><td>0.243     </td><td>0.433     </td><td>0.016</td><td>0.017</td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: p1_g02_m2_5_x-free_cut_6\n",
      "ref: mars da fikk vi ni hundre og åtte og sytti kilometer på ei uka tre runder rundt gran canaria i\n",
      "-----\n",
      "orig hyp: mars da fikk vi ni hundre og åtteogsytti kilometer på ei uka tre runder rundt gran canaria\n",
      "orig_wer: 0.2 orig_asd 0.2041407132729937\n",
      "-----\n",
      "mod hyp: mars da fikk vi ni hundre og åtte og sytti kilometer på ei uka tre runder rundt gran canaria i\n",
      "mod_wer: 0.0 mod_asd: 0.0\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['åtte'], 'ref_pos': ['NUM'], 'hyp': ['åtteogsytti'], 'hyp_pos': ['NUM']}\n",
      "{'type': 'delete', 'ref': ['og', 'sytti'], 'ref_pos': ['CCONJ', 'NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>og   </td><td>sy  </td><td>##tti</td><td>##tti</td><td>##tti</td><td>##tti</td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>i     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars </td><td>da   </td><td>fikk </td><td>vi   </td><td>ni   </td><td>hundre</td><td>og   </td><td>åtte </td><td>åtte </td><td>åtte</td><td>åtte </td><td>##ogs</td><td>##ytt</td><td>##i  </td><td>kilometer</td><td>på   </td><td>ei   </td><td>uka  </td><td>tre  </td><td>runder</td><td>rundt</td><td>gran </td><td>can  </td><td>##aria</td><td>##aria</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.083</td><td>0.012</td><td>0.006</td><td>0.004</td><td>0.004</td><td>0.008</td><td>0.009 </td><td>0.024</td><td>0.097</td><td>0.722</td><td>0.53</td><td>0.455</td><td>0.687</td><td>0.598</td><td>0.584</td><td>0.019    </td><td>0.006</td><td>0.004</td><td>0.006</td><td>0.006</td><td>0.006 </td><td>0.01 </td><td>0.009</td><td>0.011</td><td>0.026 </td><td>0.667 </td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 24, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>mars</td><td>da</td><td>fikk</td><td>vi</td><td>ni</td><td>hundre</td><td>og</td><td>åtte</td><td>og</td><td>sy</td><td>##tti</td><td>kilometer</td><td>på</td><td>ei</td><td>uka</td><td>tre</td><td>runder</td><td>rundt</td><td>gran</td><td>can</td><td>##aria</td><td>i</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0   </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0     </td><td>0 </td><td>0   </td><td>0 </td><td>0 </td><td>0    </td><td>0        </td><td>0 </td><td>0 </td><td>0  </td><td>0  </td><td>0     </td><td>0    </td><td>0   </td><td>0  </td><td>0     </td><td>0</td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_144\n",
      "ref: måte det har vært gjennomført en ren henrettelse av langsomhet ormestad fra vår side er vi vel av den oppfatning\n",
      "-----\n",
      "orig hyp: måte det har vært gjennomført en ren henrettelse avhengig som heter da fra vår side er vi i har denne oppfatningen\n",
      "orig_wer: 0.4 orig_asd 0.4612358430979374\n",
      "-----\n",
      "mod hyp: måte det har vært gjennomført en ren henrettelse avhengig somhet sta fra vår side er vi av den oppfatning\n",
      "mod_wer: 0.2 mod_asd: 0.2693644592168565\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['avhengig'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'substitute', 'ref': ['av', 'langsomhet', 'ormestad'], 'ref_pos': ['ADP', 'NOUN', 'PROPN'], 'hyp': ['som', 'heter', 'da'], 'hyp_pos': ['PRON', 'VERB', 'ADV']}\n",
      "{'type': 'substitute', 'ref': ['vel', 'av', 'den', 'oppfatning'], 'ref_pos': ['ADV', 'ADP', 'DET', 'NOUN'], 'hyp': ['i', 'har', 'denne', 'oppfatningen'], 'hyp_pos': ['ADP', 'VERB', 'DET', 'NOUN']}\n",
      "ASD-NorBERT ref len: 28, ASD=0.46\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e </td><td>av   </td><td>av      </td><td>av   </td><td>langsom</td><td>##het</td><td>or  </td><td>##mest</td><td>##ad </td><td>##ad </td><td>##ad</td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vel  </td><td>av   </td><td>av   </td><td>den  </td><td>oppfatning  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e </td><td>##e  </td><td>avhengig</td><td>som  </td><td>som    </td><td>som  </td><td>som </td><td>som   </td><td>som  </td><td>heter</td><td>da  </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vi   </td><td>i    </td><td>har  </td><td>denne</td><td>oppfatningen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.06 </td><td>0.045</td><td>0.025</td><td>0.027</td><td>0.019</td><td>0.021      </td><td>0.03</td><td>0.023</td><td>0.041</td><td>0.029  </td><td>0.046</td><td>0.07</td><td>0.573</td><td>0.696   </td><td>0.633</td><td>0.812  </td><td>0.707</td><td>0.77</td><td>0.807 </td><td>0.744</td><td>0.804</td><td>0.8 </td><td>0.086</td><td>0.046</td><td>0.043</td><td>0.139</td><td>0.083</td><td>0.729</td><td>0.634</td><td>0.599</td><td>0.446</td><td>0.229       </td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['av', 'langsomhet', 'ormestad'], 'ref_pos': ['ADP', 'NOUN', 'PROPN'], 'hyp': ['avhengig', 'somhet', 'sta'], 'hyp_pos': ['ADJ', 'NOUN', 'VERB']}\n",
      "{'type': 'delete', 'ref': ['vel'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 28, ASD=0.27\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e  </td><td>av  </td><td>av      </td><td>av    </td><td>langsom</td><td>##het </td><td>or    </td><td>##mest</td><td>##ad </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vel  </td><td>av  </td><td>den </td><td>oppfatning</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>måte </td><td>det  </td><td>har  </td><td>vært </td><td>gjennomført</td><td>en  </td><td>ren  </td><td>hen  </td><td>##rette</td><td>##ls </td><td>##e  </td><td>##e </td><td>avhengig</td><td>somhet</td><td>somhet </td><td>somhet</td><td>somhet</td><td>somhet</td><td>sta  </td><td>fra  </td><td>vår  </td><td>side </td><td>er   </td><td>vi   </td><td>vi   </td><td>av  </td><td>den </td><td>oppfatning</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.017</td><td>0.015</td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.012      </td><td>0.02</td><td>0.019</td><td>0.048</td><td>0.031  </td><td>0.048</td><td>0.072</td><td>0.56</td><td>0.63    </td><td>0.556 </td><td>0.632  </td><td>0.571 </td><td>0.718 </td><td>0.727 </td><td>0.603</td><td>0.056</td><td>0.033</td><td>0.029</td><td>0.034</td><td>0.038</td><td>0.706</td><td>0.05</td><td>0.04</td><td>0.031     </td><td>0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_051206_NRK_D12_NO_OOLremoved_cut_35\n",
      "ref: at man må stille alle disse personene må stille sine plasser til disposisjon inklusiv monika kristensen solås bråket startet da\n",
      "-----\n",
      "orig hyp: at man må stille alle disse personene må stille sine plasser til disposisjon inklusiv monica christine sone bråket startet\n",
      "orig_wer: 0.2 orig_asd 0.2816538722729323\n",
      "-----\n",
      "mod hyp: at man må stille alle disse personene må stille sine plasser til disposisjon inklusiv monica kristensen solus bråket startet da\n",
      "mod_wer: 0.1 mod_asd: 0.0923559605542246\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['monika', 'kristensen', 'solås'], 'ref_pos': ['PROPN', 'PROPN', 'PROPN'], 'hyp': ['monica', 'christine', 'sone'], 'hyp_pos': ['PROPN', 'X', 'X']}\n",
      "{'type': 'delete', 'ref': ['da'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.28\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at  </td><td>man  </td><td>må   </td><td>stille</td><td>alle </td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ika</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##ås </td><td>##ås</td><td>bråket</td><td>startet</td><td>da     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at  </td><td>man  </td><td>må   </td><td>stille</td><td>alle </td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ica</td><td>christ </td><td>##ine</td><td>##ine</td><td>##ine</td><td>sone</td><td>bråket</td><td>startet</td><td>startet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.11 </td><td>0.01</td><td>0.006</td><td>0.006</td><td>0.005 </td><td>0.003</td><td>0.006</td><td>0.01     </td><td>0.005</td><td>0.005 </td><td>0.004</td><td>0.01   </td><td>0.004</td><td>0.006      </td><td>0.019   </td><td>0.094</td><td>0.383</td><td>0.497  </td><td>0.537</td><td>0.626</td><td>0.631</td><td>0.66</td><td>0.288 </td><td>0.265  </td><td>0.663  </td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['monika'], 'ref_pos': ['PROPN'], 'hyp': ['monica'], 'hyp_pos': ['PROPN']}\n",
      "{'type': 'substitute', 'ref': ['solås'], 'ref_pos': ['PROPN'], 'hyp': ['solus'], 'hyp_pos': ['X']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>man  </td><td>må   </td><td>stille</td><td>alle</td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ika</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##ås </td><td>bråket</td><td>startet</td><td>da   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>man  </td><td>må   </td><td>stille</td><td>alle</td><td>disse</td><td>personene</td><td>må   </td><td>stille</td><td>sine </td><td>plasser</td><td>til  </td><td>disposisjon</td><td>inklusiv</td><td>mon  </td><td>##ica</td><td>kristen</td><td>##sen</td><td>sol  </td><td>##us </td><td>bråket</td><td>startet</td><td>da   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001 </td><td>0.0 </td><td>0.001</td><td>0.002    </td><td>0.001</td><td>0.001 </td><td>0.001</td><td>0.002  </td><td>0.001</td><td>0.001      </td><td>0.009   </td><td>0.073</td><td>0.388</td><td>0.025  </td><td>0.035</td><td>0.088</td><td>0.499</td><td>0.015 </td><td>0.004  </td><td>0.003</td><td>0.0  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_154\n",
      "ref: ikkje på førtito åringen når han seiar han ikkje hugsar noko den domfelte tok omtenkingstid på spørsmålet om han vil\n",
      "-----\n",
      "orig hyp: ikkje på førtito åringen når hans eier han ikkje hugsar noko den domfelte tok om tenkningstid på spørsmålet om han ville\n",
      "orig_wer: 0.25 orig_asd 0.2694798834810394\n",
      "-----\n",
      "mod hyp: ikkje på førtito åringen når han seier han ikke hugsar noko den domfelte tok omtenkningstid på spørsmålet om han vil\n",
      "mod_wer: 0.15 mod_asd: 0.0867934216375821\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['han', 'seiar'], 'ref_pos': ['PRON', 'VERB'], 'hyp': ['hans', 'eier'], 'hyp_pos': ['PRON', 'VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['om'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['tenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['vil'], 'ref_pos': ['VERB'], 'hyp': ['ville'], 'hyp_pos': ['AUX']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.27\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når </td><td>han  </td><td>han  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk   </td><td>##ingstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når </td><td>hans </td><td>eier </td><td>han  </td><td>han  </td><td>han  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>om   </td><td>tenkning</td><td>##stid   </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>ville</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.017</td><td>0.007</td><td>0.006</td><td>0.005</td><td>0.009  </td><td>0.06</td><td>0.335</td><td>0.765</td><td>0.118</td><td>0.761</td><td>0.673</td><td>0.085</td><td>0.039</td><td>0.028</td><td>0.027</td><td>0.051</td><td>0.025</td><td>0.024   </td><td>0.102</td><td>0.603</td><td>0.51    </td><td>0.307    </td><td>0.021</td><td>0.019     </td><td>0.021</td><td>0.017</td><td>0.192</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['seiar'], 'ref_pos': ['VERB'], 'hyp': ['seier'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['ikkje'], 'ref_pos': ['PART'], 'hyp': ['ikke'], 'hyp_pos': ['PART']}\n",
      "{'type': 'substitute', 'ref': ['omtenkingstid'], 'ref_pos': ['NOUN'], 'hyp': ['omtenkningstid'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 27, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seia </td><td>##r  </td><td>han  </td><td>ikkje</td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ingstid </td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ikkje</td><td>på   </td><td>førti</td><td>##to </td><td>åringen</td><td>når  </td><td>han  </td><td>seier</td><td>seier</td><td>han  </td><td>ikke </td><td>hug  </td><td>##sar</td><td>noko </td><td>den  </td><td>domfelte</td><td>tok  </td><td>omt  </td><td>##enk</td><td>##ningstid</td><td>på   </td><td>spørsmålet</td><td>om   </td><td>han  </td><td>vil </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.005</td><td>0.012</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.004  </td><td>0.012</td><td>0.011</td><td>0.201</td><td>0.443</td><td>0.021</td><td>0.14 </td><td>0.019</td><td>0.019</td><td>0.031</td><td>0.012</td><td>0.014   </td><td>0.028</td><td>0.048</td><td>0.065</td><td>0.245     </td><td>0.018</td><td>0.011     </td><td>0.009</td><td>0.007</td><td>0.01</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_203\n",
      "ref: saken kom opp for lagmannsretten i januar nedla aktoratet påstand om seks års fengsel for åke gren men i dommen\n",
      "-----\n",
      "orig hyp: saken kom opp for lagmannsretten i januar nedelaaktoratet påstand om seks års fengsel for å åke gren men i dommen\n",
      "orig_wer: 0.15 orig_asd 0.1776004109983102\n",
      "-----\n",
      "mod hyp: saken kom opp for lagmannsretten i januar nedla aktoratet påstand om seks års fengsel for åke gren men i dommen\n",
      "mod_wer: 0.0 mod_asd: 0.0\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['nedla'], 'ref_pos': ['VERB'], 'hyp': ['nedelaaktoratet'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['aktoratet'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['å'], 'hyp_pos': ['PART']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>saken</td><td>kom  </td><td>opp  </td><td>for  </td><td>lagmannsretten</td><td>i    </td><td>januar</td><td>ned  </td><td>##la </td><td>aktor</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om   </td><td>seks </td><td>års </td><td>fengsel</td><td>for  </td><td>å    </td><td>å   </td><td>##ke </td><td>gren </td><td>men  </td><td>i    </td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>saken</td><td>kom  </td><td>opp  </td><td>for  </td><td>lagmannsretten</td><td>i    </td><td>januar</td><td>nede </td><td>##la </td><td>##akt</td><td>##ora</td><td>##tet </td><td>påstand</td><td>om   </td><td>seks </td><td>års </td><td>fengsel</td><td>for  </td><td>å    </td><td>å   </td><td>##ke </td><td>gren </td><td>men  </td><td>i    </td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.004</td><td>0.003</td><td>0.004</td><td>0.003</td><td>0.004         </td><td>0.004</td><td>0.008 </td><td>0.313</td><td>0.081</td><td>0.643</td><td>0.644</td><td>0.422 </td><td>0.021  </td><td>0.012</td><td>0.012</td><td>0.01</td><td>0.01   </td><td>0.025</td><td>0.148</td><td>0.13</td><td>0.037</td><td>0.021</td><td>0.017</td><td>0.014</td><td>0.011 </td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 25, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>saken</td><td>kom</td><td>opp</td><td>for</td><td>lagmannsretten</td><td>i</td><td>januar</td><td>ned</td><td>##la</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om</td><td>seks</td><td>års</td><td>fengsel</td><td>for</td><td>å</td><td>##ke</td><td>gren</td><td>men</td><td>i</td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>saken</td><td>kom</td><td>opp</td><td>for</td><td>lagmannsretten</td><td>i</td><td>januar</td><td>ned</td><td>##la</td><td>aktor</td><td>##atet</td><td>påstand</td><td>om</td><td>seks</td><td>års</td><td>fengsel</td><td>for</td><td>å</td><td>##ke</td><td>gren</td><td>men</td><td>i</td><td>dommen</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0             </td><td>0</td><td>0     </td><td>0  </td><td>0   </td><td>0    </td><td>0     </td><td>0      </td><td>0 </td><td>0   </td><td>0  </td><td>0      </td><td>0  </td><td>0</td><td>0   </td><td>0   </td><td>0  </td><td>0</td><td>0     </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_113\n",
      "ref: angelica skal se over alle gavene lover gjeruldsen er selvfølgelig den skjønneste babyen i hele verden helt objektivt sett ja\n",
      "-----\n",
      "orig hyp: angelica skal se over alle gavene lover gjeroldsenb heleverden objektivt\n",
      "orig_wer: 0.6 orig_asd 0.4656876424140228\n",
      "-----\n",
      "mod hyp: angelica skal se over alle gavene lover gjeruldsen nesten hele verden helt objektivt sett\n",
      "mod_wer: 0.35 mod_asd: 0.2964287726006986\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['gjeruldsen', 'er'], 'ref_pos': ['PROPN', 'AUX'], 'hyp': ['gjeroldsenb', 'heleverden'], 'hyp_pos': ['PROPN', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['selvfølgelig', 'den', 'skjønneste', 'babyen', 'i', 'hele', 'verden', 'helt'], 'ref_pos': ['ADJ', 'DET', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['sett', 'ja'], 'ref_pos': ['VERB', 'INTJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.47\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal </td><td>se  </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld </td><td>##sen</td><td>er   </td><td>selvfølgelig</td><td>den  </td><td>skjønne</td><td>##ste</td><td>babyen</td><td>i    </td><td>hele </td><td>verden  </td><td>helt    </td><td>objektivt</td><td>sett     </td><td>ja       </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal </td><td>se  </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##olds</td><td>##en </td><td>##en </td><td>##en        </td><td>##en </td><td>##en   </td><td>##en </td><td>##b   </td><td>##b  </td><td>hele </td><td>##verden</td><td>##verden</td><td>objektivt</td><td>objektivt</td><td>objektivt</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.097</td><td>0.015</td><td>0.015</td><td>0.026</td><td>0.02</td><td>0.014</td><td>0.014</td><td>0.019</td><td>0.023</td><td>0.089</td><td>0.067</td><td>0.532 </td><td>0.357</td><td>0.613</td><td>0.732       </td><td>0.621</td><td>0.861  </td><td>0.775</td><td>0.749 </td><td>0.703</td><td>0.255</td><td>0.37    </td><td>0.818   </td><td>0.385    </td><td>0.732    </td><td>0.676    </td><td>0.022</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['nesten'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'delete', 'ref': ['selvfølgelig', 'den', 'skjønneste', 'babyen', 'i'], 'ref_pos': ['ADJ', 'DET', 'ADJ', 'NOUN', 'ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['ja'], 'ref_pos': ['INTJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal</td><td>se   </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld</td><td>##sen</td><td>er   </td><td>selvfølgelig</td><td>den   </td><td>skjønne</td><td>##ste </td><td>babyen</td><td>i     </td><td>hele </td><td>verden</td><td>helt </td><td>objektivt</td><td>sett </td><td>ja   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>angel</td><td>##ica</td><td>skal</td><td>se   </td><td>over </td><td>alle </td><td>gaven</td><td>##e  </td><td>lover</td><td>gjer </td><td>##uld</td><td>##sen</td><td>##sen</td><td>nesten      </td><td>nesten</td><td>nesten </td><td>nesten</td><td>nesten</td><td>nesten</td><td>hele </td><td>verden</td><td>helt </td><td>objektivt</td><td>sett </td><td>sett </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.046</td><td>0.011</td><td>0.012</td><td>0.02</td><td>0.017</td><td>0.012</td><td>0.012</td><td>0.013</td><td>0.015</td><td>0.087</td><td>0.025</td><td>0.021</td><td>0.068</td><td>0.662</td><td>0.579       </td><td>0.742 </td><td>0.782  </td><td>0.787 </td><td>0.835 </td><td>0.678 </td><td>0.197</td><td>0.115 </td><td>0.059</td><td>0.055    </td><td>0.081</td><td>0.647</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_48\n",
      "ref: notodden politistasjon så skal flyet ha styrtet altså i slettefjell som du nevnte det er cirka tolv hundre meters høyde\n",
      "-----\n",
      "orig hyp: notodden politivstersjon så skal flyene styrtet altså i slettefjell som du nevnte det er cirka tolv hundre meters høyde\n",
      "orig_wer: 0.15 orig_asd 0.2449409632231249\n",
      "-----\n",
      "mod hyp: notodden politistasjon så skal fly styrtet altså i slettefjell som du nevnte det er cirka tolv hundre meters høyde\n",
      "mod_wer: 0.1 mod_asd: 0.0759300285597235\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['politistasjon'], 'ref_pos': ['NOUN'], 'hyp': ['politivstersjon'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['flyet'], 'ref_pos': ['NOUN'], 'hyp': ['flyene'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ha'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>politistasjon</td><td>politistasjon</td><td>politistasjon</td><td>så   </td><td>skal </td><td>flyet </td><td>ha    </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som  </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politi       </td><td>##vs         </td><td>##ters       </td><td>##jon        </td><td>så   </td><td>skal </td><td>flyene</td><td>flyene</td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som  </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.034</td><td>0.164</td><td>0.195  </td><td>0.52         </td><td>0.732        </td><td>0.694        </td><td>0.587        </td><td>0.056</td><td>0.088</td><td>0.179 </td><td>0.655 </td><td>0.082  </td><td>0.04 </td><td>0.019</td><td>0.034 </td><td>0.034  </td><td>0.016</td><td>0.018</td><td>0.02  </td><td>0.028</td><td>0.014</td><td>0.017</td><td>0.016</td><td>0.016 </td><td>0.013 </td><td>0.014</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['flyet'], 'ref_pos': ['NOUN'], 'hyp': ['fly'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ha'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>så  </td><td>skal </td><td>flyet</td><td>ha   </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>not  </td><td>##odden</td><td>politistasjon</td><td>så  </td><td>skal </td><td>fly  </td><td>fly  </td><td>styrtet</td><td>altså</td><td>i    </td><td>slette</td><td>##fjell</td><td>som </td><td>du   </td><td>nevnte</td><td>det  </td><td>er   </td><td>cirka</td><td>tolv </td><td>hundre</td><td>meters</td><td>høyde</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.007</td><td>0.014</td><td>0.015  </td><td>0.013        </td><td>0.02</td><td>0.107</td><td>0.187</td><td>0.572</td><td>0.074  </td><td>0.031</td><td>0.015</td><td>0.015 </td><td>0.016  </td><td>0.01</td><td>0.012</td><td>0.012 </td><td>0.018</td><td>0.009</td><td>0.012</td><td>0.012</td><td>0.011 </td><td>0.008 </td><td>0.009</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_286\n",
      "ref: og karakteriserer landets kommunepolitikere som griske og nærmest late som om at de ikke har kreativitet nok til å finne\n",
      "-----\n",
      "orig hyp: karakterisere landets kommunepolitikere som griske og og nærmest later som at de ikke har kreativitetsnok til å finne\n",
      "orig_wer: 0.35 orig_asd 0.2881092645221707\n",
      "-----\n",
      "mod hyp: karakterisere landets kommunepolitikere som griske og nærmest late som at de ikke har kreativitet nok til å finne\n",
      "mod_wer: 0.15 mod_asd: 0.1196073826734738\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['karakterisere'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['karakteriserer'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'substitute', 'ref': ['late'], 'ref_pos': ['ADJ'], 'hyp': ['later'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['om'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['kreativitet'], 'ref_pos': ['NOUN'], 'hyp': ['kreativitetsnok'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['nok'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>karakteriserer</td><td>karakteriserer</td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>og   </td><td>nærmest</td><td>late </td><td>som </td><td>om   </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>kreativitet</td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>karakteris    </td><td>##ere         </td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>og   </td><td>nærmest</td><td>later</td><td>som </td><td>som  </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>##sn       </td><td>##ok       </td><td>##ok </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.038</td><td>0.632</td><td>0.288         </td><td>0.544         </td><td>0.016  </td><td>0.004  </td><td>0.012       </td><td>0.027</td><td>0.011</td><td>0.014</td><td>0.129</td><td>0.187</td><td>0.06   </td><td>0.348</td><td>0.16</td><td>0.568</td><td>0.067</td><td>0.015</td><td>0.036</td><td>0.033</td><td>0.148      </td><td>0.623      </td><td>0.615      </td><td>0.712</td><td>0.079</td><td>0.029</td><td>0.014</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['karakterisere'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['karakteriserer'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['om'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>karakteriserer</td><td>karakteriserer</td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>nærmest</td><td>late </td><td>som  </td><td>om   </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>karakteris    </td><td>##ere         </td><td>landets</td><td>kommune</td><td>##politikere</td><td>som  </td><td>gris </td><td>##ke </td><td>og   </td><td>nærmest</td><td>late </td><td>som  </td><td>som  </td><td>at   </td><td>de   </td><td>ikke </td><td>har  </td><td>kreativitet</td><td>nok  </td><td>til  </td><td>å    </td><td>finne</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.037</td><td>0.637</td><td>0.279         </td><td>0.533         </td><td>0.014  </td><td>0.002  </td><td>0.007       </td><td>0.017</td><td>0.004</td><td>0.005</td><td>0.011</td><td>0.005  </td><td>0.023</td><td>0.081</td><td>0.544</td><td>0.042</td><td>0.008</td><td>0.007</td><td>0.006</td><td>0.014      </td><td>0.007</td><td>0.006</td><td>0.007</td><td>0.007</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_43\n",
      "ref: er blitt et diskusjonene omkring hans troverdighet hadde det nok vært lurt for ham å gå selv om han ikke\n",
      "-----\n",
      "orig hyp: omkring hans troverdighet hadde den nok vært rushan å gå om han ikke har\n",
      "orig_wer: 0.5 orig_asd 0.5468914002033878\n",
      "-----\n",
      "mod hyp: omkring hans troverdighet så hadde det nok vært lurt for han å gå om man ikke\n",
      "mod_wer: 0.4 mod_asd: 0.3805770876613845\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['er', 'blitt', 'et', 'diskusjonene'], 'ref_pos': ['AUX', 'VERB', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['den'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['lurt'], 'ref_pos': ['ADJ'], 'hyp': ['rushan'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['for', 'ham'], 'ref_pos': ['ADP', 'PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['selv'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['har'], 'hyp_pos': ['AUX']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.55\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er     </td><td>blitt  </td><td>et     </td><td>diskusjonen</td><td>##e    </td><td>omkring</td><td>hans </td><td>troverdighet</td><td>hadde</td><td>det </td><td>nok  </td><td>vært </td><td>lurt </td><td>lurt </td><td>lurt </td><td>for  </td><td>ham  </td><td>å    </td><td>gå   </td><td>selv </td><td>om   </td><td>han  </td><td>ikke </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>omkring</td><td>omkring</td><td>omkring</td><td>omkring    </td><td>omkring</td><td>omkring</td><td>hans </td><td>troverdighet</td><td>hadde</td><td>den </td><td>nok  </td><td>vært </td><td>vært </td><td>rush </td><td>##an </td><td>##an </td><td>##an </td><td>å    </td><td>gå   </td><td>gå   </td><td>om   </td><td>han  </td><td>ikke </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.059</td><td>0.518  </td><td>0.761  </td><td>0.738  </td><td>0.678      </td><td>0.675  </td><td>0.312  </td><td>0.063</td><td>0.072       </td><td>0.063</td><td>0.51</td><td>0.064</td><td>0.146</td><td>0.646</td><td>0.676</td><td>0.787</td><td>0.809</td><td>0.787</td><td>0.174</td><td>0.112</td><td>0.826</td><td>0.202</td><td>0.077</td><td>0.136</td><td>0.513</td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['er', 'blitt', 'et', 'diskusjonene'], 'ref_pos': ['AUX', 'VERB', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['ham'], 'ref_pos': ['PRON'], 'hyp': ['han'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'delete', 'ref': ['selv'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['han'], 'ref_pos': ['PRON'], 'hyp': ['man'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.38\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er     </td><td>blitt  </td><td>et     </td><td>diskusjonen</td><td>##e    </td><td>omkring</td><td>hans </td><td>troverdighet</td><td>troverdighet</td><td>hadde</td><td>det  </td><td>nok  </td><td>vært </td><td>lurt </td><td>for  </td><td>ham  </td><td>å    </td><td>gå   </td><td>selv </td><td>om  </td><td>han  </td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>omkring</td><td>omkring</td><td>omkring</td><td>omkring    </td><td>omkring</td><td>omkring</td><td>hans </td><td>troverdighet</td><td>så          </td><td>hadde</td><td>det  </td><td>nok  </td><td>vært </td><td>lurt </td><td>for  </td><td>han  </td><td>å    </td><td>gå   </td><td>gå   </td><td>om  </td><td>man  </td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.521  </td><td>0.762  </td><td>0.754  </td><td>0.669      </td><td>0.678  </td><td>0.28   </td><td>0.056</td><td>0.066       </td><td>0.752       </td><td>0.074</td><td>0.048</td><td>0.039</td><td>0.041</td><td>0.043</td><td>0.063</td><td>0.193</td><td>0.056</td><td>0.048</td><td>0.809</td><td>0.18</td><td>0.481</td><td>0.08</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_234\n",
      "ref: den har litt bedre friksjon enn enn den isen vi kjenner hjemme takk skal du ha john gulldahl ansvarlig for\n",
      "-----\n",
      "orig hyp: den har litt bedre fleksjon en enn den isen jeg kjenner hjemme takk skal du ha jon guldal ansvarlig for\n",
      "orig_wer: 0.25 orig_asd 0.2647869176215524\n",
      "-----\n",
      "mod hyp: den har litt bedre friksjon enn enn den isen jeg kjenner hjemme takk skal du ha jon guldal ansvarlig for\n",
      "mod_wer: 0.15 mod_asd: 0.1242855305587567\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['friksjon', 'enn'], 'ref_pos': ['NOUN', 'ADP'], 'hyp': ['fleksjon', 'en'], 'hyp_pos': ['NOUN', 'DET']}\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['john', 'gulldahl'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['jon', 'guldal'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.26\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>friksjon</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>vi   </td><td>kjenner</td><td>hjemme</td><td>takk</td><td>skal </td><td>du  </td><td>ha   </td><td>john </td><td>gull </td><td>##dahl</td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>fle     </td><td>##ksjon </td><td>en      </td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>jeg  </td><td>kjenner</td><td>hjemme</td><td>takk</td><td>skal </td><td>du  </td><td>ha   </td><td>jon  </td><td>gul  </td><td>##dal </td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.028</td><td>0.029</td><td>0.025</td><td>0.032</td><td>0.565   </td><td>0.548   </td><td>0.765   </td><td>0.105</td><td>0.049</td><td>0.022</td><td>0.035</td><td>0.287</td><td>0.032  </td><td>0.027 </td><td>0.04</td><td>0.021</td><td>0.03</td><td>0.024</td><td>0.206</td><td>0.436</td><td>0.36  </td><td>0.03     </td><td>0.018</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'substitute', 'ref': ['john', 'gulldahl'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['jon', 'guldal'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.12\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>vi   </td><td>kjenner</td><td>hjemme</td><td>takk </td><td>skal </td><td>du   </td><td>ha   </td><td>john </td><td>gull </td><td>##dahl</td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>den  </td><td>har  </td><td>litt </td><td>bedre</td><td>friksjon</td><td>enn  </td><td>enn  </td><td>den  </td><td>isen </td><td>jeg  </td><td>kjenner</td><td>hjemme</td><td>takk </td><td>skal </td><td>du   </td><td>ha   </td><td>jon  </td><td>gul  </td><td>##dal </td><td>ansvarlig</td><td>for  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.007</td><td>0.004</td><td>0.004</td><td>0.002</td><td>0.004   </td><td>0.003</td><td>0.003</td><td>0.005</td><td>0.006</td><td>0.271</td><td>0.017  </td><td>0.009 </td><td>0.022</td><td>0.012</td><td>0.017</td><td>0.013</td><td>0.222</td><td>0.429</td><td>0.348 </td><td>0.02     </td><td>0.009</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: p1_g08_m2_5_x-free_cut_10\n",
      "ref: nyttig for statsvitere som forsker på konflikt fordi at vi da kunne få til å måle variabler som var mye\n",
      "-----\n",
      "orig hyp: nyttig for statsvitere som forsker på konflikt fred vil da kunne få til å måle variable så mye\n",
      "orig_wer: 0.3 orig_asd 0.3040213021501807\n",
      "-----\n",
      "mod hyp: nyttig for statsvitere som forsker på konflikt fordi vi da kunne få til å måle variabler svar\n",
      "mod_wer: 0.2 mod_asd: 0.1672065716643471\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['fordi', 'at'], 'ref_pos': ['SCONJ', 'SCONJ'], 'hyp': ['fred', 'vil'], 'hyp_pos': ['NOUN', 'AUX']}\n",
      "{'type': 'delete', 'ref': ['vi'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['variabler', 'som'], 'ref_pos': ['NOUN', 'PRON'], 'hyp': ['variable', 'så'], 'hyp_pos': ['ADJ', 'ADV']}\n",
      "{'type': 'delete', 'ref': ['var'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>som     </td><td>var     </td><td>var  </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fred    </td><td>vil  </td><td>vil  </td><td>vil  </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variable </td><td>variable</td><td>variable</td><td>så   </td><td>mye  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.048</td><td>0.047 </td><td>0.027</td><td>0.011</td><td>0.015  </td><td>0.016</td><td>0.025</td><td>0.019  </td><td>0.026</td><td>0.103   </td><td>0.428   </td><td>0.553</td><td>0.581</td><td>0.579</td><td>0.123</td><td>0.143</td><td>0.066</td><td>0.029</td><td>0.041</td><td>0.049</td><td>0.354    </td><td>0.631   </td><td>0.676   </td><td>0.707</td><td>0.166</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['at'], 'ref_pos': ['SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['som'], 'ref_pos': ['PRON'], 'hyp': ['svar'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['var', 'mye'], 'ref_pos': ['AUX', 'ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>at   </td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>variabler</td><td>som  </td><td>var  </td><td>mye </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>nyttig</td><td>for  </td><td>stats</td><td>##viter</td><td>##e  </td><td>som  </td><td>forsker</td><td>på   </td><td>konflikt</td><td>fordi</td><td>fordi</td><td>vi   </td><td>da   </td><td>kunne</td><td>få   </td><td>til  </td><td>å    </td><td>måle </td><td>variabler</td><td>svar     </td><td>svar </td><td>svar </td><td>svar</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.078</td><td>0.009 </td><td>0.003</td><td>0.003</td><td>0.004  </td><td>0.004</td><td>0.005</td><td>0.005  </td><td>0.005</td><td>0.009   </td><td>0.041</td><td>0.451</td><td>0.019</td><td>0.019</td><td>0.024</td><td>0.019</td><td>0.015</td><td>0.021</td><td>0.038</td><td>0.148    </td><td>0.638    </td><td>0.656</td><td>0.697</td><td>0.68</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_2\n",
      "ref: hva hun får er hemmelig her er nrk dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo\n",
      "-----\n",
      "orig hyp: hva hun får er hemmelig her er n r k dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo\n",
      "orig_wer: 0.15 orig_asd 0.1367109439342619\n",
      "-----\n",
      "mod hyp: hva hun får er hemmelig her er nrk dagsnytt klokka er tolv tretti først til striden rundt kraftselskapet hafslund oslo\n",
      "mod_wer: 0.0 mod_asd: 0.0\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['n', 'r'], 'hyp_pos': ['ADV', 'ADV']}\n",
      "{'type': 'substitute', 'ref': ['nrk'], 'ref_pos': ['PROPN'], 'hyp': ['k'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.14\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hva  </td><td>hun  </td><td>får  </td><td>er   </td><td>hemmelig</td><td>her  </td><td>er   </td><td>er   </td><td>er   </td><td>nrk  </td><td>dags </td><td>##nytt</td><td>klokka</td><td>er  </td><td>tolv </td><td>tretti</td><td>først</td><td>til  </td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha   </td><td>##fs </td><td>##lund</td><td>oslo </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hva  </td><td>hun  </td><td>får  </td><td>er   </td><td>hemmelig</td><td>her  </td><td>er   </td><td>n    </td><td>r    </td><td>k    </td><td>dags </td><td>##nytt</td><td>klokka</td><td>er  </td><td>tolv </td><td>tretti</td><td>først</td><td>til  </td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha   </td><td>##fs </td><td>##lund</td><td>oslo </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.006</td><td>0.002</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.003   </td><td>0.013</td><td>0.046</td><td>0.722</td><td>0.643</td><td>0.659</td><td>0.054</td><td>0.042 </td><td>0.023 </td><td>0.02</td><td>0.023</td><td>0.024 </td><td>0.014</td><td>0.012</td><td>0.017  </td><td>0.016</td><td>0.015</td><td>0.015      </td><td>0.019</td><td>0.022</td><td>0.02  </td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "ASD-NorBERT ref len: 26, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>hva</td><td>hun</td><td>får</td><td>er</td><td>hemmelig</td><td>her</td><td>er</td><td>nrk</td><td>dags</td><td>##nytt</td><td>klokka</td><td>er</td><td>tolv</td><td>tretti</td><td>først</td><td>til</td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha</td><td>##fs</td><td>##lund</td><td>oslo</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>hva</td><td>hun</td><td>får</td><td>er</td><td>hemmelig</td><td>her</td><td>er</td><td>nrk</td><td>dags</td><td>##nytt</td><td>klokka</td><td>er</td><td>tolv</td><td>tretti</td><td>først</td><td>til</td><td>striden</td><td>rundt</td><td>kraft</td><td>##selskapet</td><td>ha</td><td>##fs</td><td>##lund</td><td>oslo</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0  </td><td>0  </td><td>0 </td><td>0       </td><td>0  </td><td>0 </td><td>0  </td><td>0   </td><td>0     </td><td>0     </td><td>0 </td><td>0   </td><td>0     </td><td>0    </td><td>0  </td><td>0      </td><td>0    </td><td>0    </td><td>0          </td><td>0 </td><td>0   </td><td>0     </td><td>0   </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_47\n",
      "ref: etterforskningsledelsen også er hva mer vet du om ulykken ja at etter det politiet sier ved politioverbetjent arne skogen ved\n",
      "-----\n",
      "orig hyp: etterforskningsledelsen også er hva mer vet du om ulykken etter det politiet sier politijobben arnesen ved\n",
      "orig_wer: 0.3 orig_asd 0.3015432221371427\n",
      "-----\n",
      "mod hyp: etterforskningsledelsen også er hva mer vet du om ulykken etter det politiet sier politibetjent arne skogen ved\n",
      "mod_wer: 0.2 mod_asd: 0.1682110311432818\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved', 'politioverbetjent'], 'ref_pos': ['ADP', 'NOUN'], 'hyp': ['politijobben', 'arnesen'], 'hyp_pos': ['NOUN', 'PROPN']}\n",
      "{'type': 'delete', 'ref': ['arne', 'skogen'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>##betjent</td><td>ar   </td><td>##ne   </td><td>skogen </td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du   </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>politi   </td><td>##jobben </td><td>ar   </td><td>##nesen</td><td>##nesen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.007         </td><td>0.007      </td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.006</td><td>0.008</td><td>0.012</td><td>0.018</td><td>0.021  </td><td>0.816  </td><td>0.712  </td><td>0.173</td><td>0.084</td><td>0.04    </td><td>0.089</td><td>0.761</td><td>0.12  </td><td>0.679 </td><td>0.531    </td><td>0.619    </td><td>0.238</td><td>0.628  </td><td>0.563  </td><td>0.085</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['ja', 'at'], 'ref_pos': ['INTJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['ved'], 'ref_pos': ['ADP'], 'hyp': ['politibetjent'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['politioverbetjent'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 26, ASD=0.17\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du  </td><td>om   </td><td>ulykken</td><td>ja     </td><td>at     </td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>ved  </td><td>politi</td><td>##over</td><td>##betjent</td><td>ar   </td><td>##ne </td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>etterforskning</td><td>##sledelsen</td><td>også </td><td>er   </td><td>hva  </td><td>mer  </td><td>vet  </td><td>du  </td><td>om   </td><td>ulykken</td><td>ulykken</td><td>ulykken</td><td>etter</td><td>det  </td><td>politiet</td><td>sier </td><td>sier </td><td>politi</td><td>politi</td><td>##betjent</td><td>ar   </td><td>##ne </td><td>skogen</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.004         </td><td>0.004      </td><td>0.005</td><td>0.006</td><td>0.006</td><td>0.004</td><td>0.007</td><td>0.01</td><td>0.016</td><td>0.018  </td><td>0.822  </td><td>0.71   </td><td>0.169</td><td>0.086</td><td>0.047   </td><td>0.096</td><td>0.696</td><td>0.053 </td><td>0.575 </td><td>0.091    </td><td>0.037</td><td>0.041</td><td>0.038 </td><td>0.032</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_187\n",
      "ref: et felleseiga selskap skal driva kanalane dersom det blir noko av samarbeidet som meklarselskapet first securities tok initiativet til målet\n",
      "-----\n",
      "orig hyp: et felles eierselskap skal drive kanalene dersom det blir noko av samarbeidet som mekler selskapet først i kuritis tok initiativet til målet\n",
      "orig_wer: 0.45 orig_asd 0.4641739890679388\n",
      "-----\n",
      "mod hyp: eit felles eierselskap skal drive kanalene dersom det blir noko av samarbeidet som mekler selskapet først securities tok initiativet til målet\n",
      "mod_wer: 0.4 mod_asd: 0.331713081212074\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['felleseiga', 'selskap'], 'ref_pos': ['ADJ', 'NOUN'], 'hyp': ['felles', 'eierselskap'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['driva', 'kanalane'], 'ref_pos': ['VERB', 'NOUN'], 'hyp': ['drive', 'kanalene'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['mekler', 'selskapet'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['meklarselskapet', 'first', 'securities'], 'ref_pos': ['NOUN', 'ADJ', 'X'], 'hyp': ['først', 'i', 'kuritis'], 'hyp_pos': ['ADV', 'ADP', 'NOUN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.46\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>##eig</td><td>##a  </td><td>selskap </td><td>skal </td><td>driv </td><td>##a  </td><td>kanal   </td><td>##ane   </td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##lar</td><td>##selskapet</td><td>first</td><td>first</td><td>first</td><td>sec  </td><td>sec  </td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>eiers</td><td>eiers</td><td>##elskap</td><td>skal </td><td>drive</td><td>drive</td><td>kanalene</td><td>kanalene</td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##ler</td><td>selskapet  </td><td>først</td><td>i    </td><td>kur  </td><td>kur  </td><td>##iti</td><td>##s  </td><td>##s      </td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.043</td><td>0.089</td><td>0.081 </td><td>0.523</td><td>0.821</td><td>0.251   </td><td>0.042</td><td>0.169</td><td>0.601</td><td>0.336   </td><td>0.596   </td><td>0.036 </td><td>0.032</td><td>0.037</td><td>0.136</td><td>0.057</td><td>0.04       </td><td>0.133</td><td>0.097</td><td>0.316</td><td>0.276      </td><td>0.606</td><td>0.713</td><td>0.718</td><td>0.748</td><td>0.75 </td><td>0.737</td><td>0.741    </td><td>0.076</td><td>0.033      </td><td>0.051</td><td>0.072</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['et', 'felleseiga', 'selskap'], 'ref_pos': ['DET', 'ADJ', 'NOUN'], 'hyp': ['eit', 'felles', 'eierselskap'], 'hyp_pos': ['DET', 'ADJ', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['driva', 'kanalane'], 'ref_pos': ['VERB', 'NOUN'], 'hyp': ['drive', 'kanalene'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['mekler'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['meklarselskapet', 'first'], 'ref_pos': ['NOUN', 'ADJ'], 'hyp': ['selskapet', 'først'], 'hyp_pos': ['NOUN', 'ADV']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.33\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>et   </td><td>felles</td><td>##eig</td><td>##a  </td><td>selskap </td><td>skal </td><td>driv </td><td>##a  </td><td>kanal   </td><td>##ane   </td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##lar</td><td>##selskapet</td><td>first</td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>eit  </td><td>felles</td><td>eiers</td><td>eiers</td><td>##elskap</td><td>skal </td><td>drive</td><td>drive</td><td>kanalene</td><td>kanalene</td><td>dersom</td><td>det  </td><td>blir </td><td>noko </td><td>av   </td><td>samarbeidet</td><td>som  </td><td>mek  </td><td>##ler</td><td>selskapet  </td><td>først</td><td>sec  </td><td>##urities</td><td>tok  </td><td>initiativet</td><td>til  </td><td>målet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.044</td><td>0.252</td><td>0.082 </td><td>0.529</td><td>0.826</td><td>0.236   </td><td>0.036</td><td>0.183</td><td>0.615</td><td>0.422   </td><td>0.603   </td><td>0.033 </td><td>0.027</td><td>0.029</td><td>0.049</td><td>0.037</td><td>0.036      </td><td>0.126</td><td>0.092</td><td>0.307</td><td>0.255      </td><td>0.566</td><td>0.163</td><td>0.085    </td><td>0.048</td><td>0.039      </td><td>0.055</td><td>0.071</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: p1_g08_m2_5_x-free_cut_7\n",
      "ref: analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne et eller per definisjon så skjer jo en borgerkrig\n",
      "-----\n",
      "orig hyp: analysenivå som var mer relevant for konflikten borgerkriger skjer jo gjerne innenfor et eller per definisjon så skjer jo en borgekrig\n",
      "orig_wer: 0.15 orig_asd 0.1836098550871423\n",
      "-----\n",
      "mod hyp: analysenivå som var mer relevant for konflikter borgerkriger skjer jo gjerne innenfor et eller per definisjon så skjer jo en borgerkrig\n",
      "mod_wer: 0.05 mod_asd: 0.0548193429109065\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['konflikter'], 'ref_pos': ['NOUN'], 'hyp': ['konflikten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['borgerkrig'], 'ref_pos': ['NOUN'], 'hyp': ['borgekrig'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>borgerkrig</td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer  </td><td>relevant</td><td>for  </td><td>konflikten</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borg      </td><td>##ek      </td><td>##rig     </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.022</td><td>0.006   </td><td>0.005</td><td>0.003</td><td>0.003</td><td>0.003</td><td>0.004   </td><td>0.025</td><td>0.158     </td><td>0.039     </td><td>0.021</td><td>0.037</td><td>0.007</td><td>0.022 </td><td>0.78    </td><td>0.1</td><td>0.025</td><td>0.018</td><td>0.014     </td><td>0.015</td><td>0.028</td><td>0.015</td><td>0.052</td><td>0.549     </td><td>0.743     </td><td>0.546     </td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['innenfor'], 'hyp_pos': ['ADP']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.05\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>gjerne  </td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>analysen</td><td>##ivå</td><td>som  </td><td>var  </td><td>mer</td><td>relevant</td><td>for  </td><td>konflikter</td><td>borgerkrig</td><td>##er </td><td>skjer</td><td>jo   </td><td>gjerne</td><td>innenfor</td><td>et   </td><td>eller</td><td>per  </td><td>definisjon</td><td>så   </td><td>skjer</td><td>jo   </td><td>en   </td><td>borgerkrig</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.002</td><td>0.001   </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.001   </td><td>0.001</td><td>0.003     </td><td>0.003     </td><td>0.003</td><td>0.03 </td><td>0.005</td><td>0.02  </td><td>0.781   </td><td>0.097</td><td>0.022</td><td>0.016</td><td>0.01      </td><td>0.011</td><td>0.008</td><td>0.008</td><td>0.008</td><td>0.01      </td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_102\n",
      "ref: kjent at viruset kan overleve flere timer utenfor menneskekroppen norge er nest best i eøsområdet til å sette i verk\n",
      "-----\n",
      "orig hyp: at viruset kan overleve flere timer utenfor menneskekroppen norge er nest best i e ø sområdet til å sette i verk\n",
      "orig_wer: 0.2 orig_asd 0.2124638949439912\n",
      "-----\n",
      "mod hyp: at viruset kan overleve flere timer utenfor menneskekroppen norge er nest best i eøs området til å sette i verk\n",
      "mod_wer: 0.15 mod_asd: 0.0846332315559219\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['kjent'], 'ref_pos': ['ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['e', 'ø'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['eøsområdet'], 'ref_pos': ['NOUN'], 'hyp': ['sområdet'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.21\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kjent</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e   </td><td>##øs </td><td>##øs </td><td>##området</td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e   </td><td>ø    </td><td>som  </td><td>##rådet  </td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.036</td><td>0.709</td><td>0.174</td><td>0.018  </td><td>0.019</td><td>0.012   </td><td>0.016</td><td>0.013</td><td>0.013  </td><td>0.015   </td><td>0.023    </td><td>0.049</td><td>0.029</td><td>0.028</td><td>0.033</td><td>0.081</td><td>0.14</td><td>0.579</td><td>0.725</td><td>0.56     </td><td>0.047</td><td>0.027</td><td>0.019</td><td>0.011</td><td>0.013</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['kjent'], 'ref_pos': ['ADJ'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['eøs'], 'hyp_pos': ['PROPN']}\n",
      "{'type': 'substitute', 'ref': ['eøsområdet'], 'ref_pos': ['NOUN'], 'hyp': ['området'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kjent</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e    </td><td>##øs </td><td>##området</td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>at   </td><td>viruset</td><td>kan  </td><td>overleve</td><td>flere</td><td>timer</td><td>utenfor</td><td>menneske</td><td>##kroppen</td><td>norge</td><td>er   </td><td>nest </td><td>best </td><td>i    </td><td>e    </td><td>##øs </td><td>området  </td><td>til  </td><td>å    </td><td>sette</td><td>i    </td><td>verk </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.034</td><td>0.703</td><td>0.173</td><td>0.014  </td><td>0.015</td><td>0.009   </td><td>0.012</td><td>0.01 </td><td>0.01   </td><td>0.012   </td><td>0.015    </td><td>0.019</td><td>0.013</td><td>0.012</td><td>0.009</td><td>0.017</td><td>0.032</td><td>0.097</td><td>0.175    </td><td>0.009</td><td>0.008</td><td>0.007</td><td>0.008</td><td>0.008</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_106\n",
      "ref: til kongefamiliens ferske medlem da har jeg den gleden av å kunngjøre at prinsesse märtha louise og ari behn har\n",
      "-----\n",
      "orig hyp: til kongefamiliens ferske medlem da har jeg den gleden av å kunngjøre at prinsesse martha louis og ari ben\n",
      "orig_wer: 0.2 orig_asd 0.1969883593608145\n",
      "-----\n",
      "mod hyp: til kongefamiliens ferske medlem da har jeg den gleden av å kunngjøre at prinsesse martha louise og ari behn har\n",
      "mod_wer: 0.05 mod_asd: 0.076056870728658\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['märtha', 'louise'], 'ref_pos': ['PROPN', 'PROPN'], 'hyp': ['martha', 'louis'], 'hyp_pos': ['X', 'X']}\n",
      "{'type': 'substitute', 'ref': ['behn'], 'ref_pos': ['PROPN'], 'hyp': ['ben'], 'hyp_pos': ['X']}\n",
      "{'type': 'delete', 'ref': ['har'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 29, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og  </td><td>ar   </td><td>##i  </td><td>beh </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s  </td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den  </td><td>gleden</td><td>av   </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##uis</td><td>og  </td><td>ar   </td><td>##i  </td><td>##i </td><td>##i  </td><td>ben  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.041</td><td>0.008</td><td>0.004</td><td>0.003     </td><td>0.002</td><td>0.003 </td><td>0.002 </td><td>0.004</td><td>0.005</td><td>0.002</td><td>0.002</td><td>0.003 </td><td>0.002</td><td>0.003</td><td>0.006</td><td>0.004  </td><td>0.031</td><td>0.037    </td><td>0.608</td><td>0.563</td><td>0.081</td><td>0.121</td><td>0.579</td><td>0.03</td><td>0.035</td><td>0.066</td><td>0.72</td><td>0.504</td><td>0.578</td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['märtha'], 'ref_pos': ['PROPN'], 'hyp': ['martha'], 'hyp_pos': ['PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>[UNK]</td><td>[UNK]</td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>konge</td><td>##familien</td><td>##s</td><td>ferske</td><td>medlem</td><td>da   </td><td>har  </td><td>jeg  </td><td>den</td><td>gleden</td><td>av </td><td>å    </td><td>kunn </td><td>##gjøre</td><td>at   </td><td>prinsesse</td><td>mart </td><td>##ha </td><td>lo   </td><td>##uis</td><td>##e  </td><td>og   </td><td>ar   </td><td>##i  </td><td>beh  </td><td>##n  </td><td>har  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.001     </td><td>0.0</td><td>0.001 </td><td>0.001 </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.0   </td><td>0.0</td><td>0.001</td><td>0.002</td><td>0.001  </td><td>0.002</td><td>0.025    </td><td>0.605</td><td>0.556</td><td>0.048</td><td>0.028</td><td>0.026</td><td>0.009</td><td>0.014</td><td>0.018</td><td>0.012</td><td>0.015</td><td>0.005</td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 20\n",
    "\n",
    "for row in lower_asd_df.itertuples():\n",
    "\n",
    "    print(\"segment id:\", row.segment_id)\n",
    "    print(\"ref:\", row.orig_ref)\n",
    "    print(\"-----\")\n",
    "    print(\"orig hyp:\", row.orig_asr)\n",
    "    print(\"orig_wer:\", row.orig_wer, \"orig_asd\", row.orig_asd)\n",
    "    print(\"-----\")\n",
    "    print(\"mod hyp:\", row.mod_asr)\n",
    "    print(\"mod_wer:\", row.mod_wer, \"mod_asd:\", row.mod_asd)\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment id: p1_g03_m2_4_x-free_cut_1\n",
      "ref: til hjemstedet i hvert fall to ganger i året sist vi var på hjemstedet i mo i rana var sist\n",
      "-----\n",
      "orig hyp: til hjemstedet i hvert fall to ganger i året sist vi var på hjemstedet i mo i rana var sist\n",
      "orig_wer: 0.0 orig_asd 0.0\n",
      "-----\n",
      "mod hyp: til hjemstedet i hvert fall to ganger i året sist vi var på hjemstedet i moiranaver sist\n",
      "mod_wer: 0.2 mod_asd: 0.2230919036242782\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 25, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til</td><td>hjem</td><td>##stedet</td><td>i</td><td>hvert</td><td>fall</td><td>to</td><td>ganger</td><td>i</td><td>året</td><td>sist</td><td>vi</td><td>var</td><td>på</td><td>hjem</td><td>##stedet</td><td>i</td><td>mo</td><td>i</td><td>ran</td><td>##a</td><td>var</td><td>sist</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til</td><td>hjem</td><td>##stedet</td><td>i</td><td>hvert</td><td>fall</td><td>to</td><td>ganger</td><td>i</td><td>året</td><td>sist</td><td>vi</td><td>var</td><td>på</td><td>hjem</td><td>##stedet</td><td>i</td><td>mo</td><td>i</td><td>ran</td><td>##a</td><td>var</td><td>sist</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0  </td><td>0   </td><td>0       </td><td>0</td><td>0    </td><td>0   </td><td>0 </td><td>0     </td><td>0</td><td>0   </td><td>0   </td><td>0 </td><td>0  </td><td>0 </td><td>0   </td><td>0       </td><td>0</td><td>0 </td><td>0</td><td>0  </td><td>0  </td><td>0  </td><td>0   </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['mo'], 'ref_pos': ['PROPN'], 'hyp': ['moiranaver'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['i', 'rana', 'var'], 'ref_pos': ['ADP', 'PROPN', 'AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.22\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>til  </td><td>hjem </td><td>##stedet</td><td>i    </td><td>hvert</td><td>fall </td><td>to   </td><td>ganger</td><td>i    </td><td>året </td><td>sist </td><td>vi   </td><td>var  </td><td>på   </td><td>hjem </td><td>##stedet</td><td>i    </td><td>mo   </td><td>i    </td><td>ran  </td><td>##a  </td><td>##a  </td><td>##a  </td><td>var  </td><td>sist </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>til  </td><td>hjem </td><td>##stedet</td><td>i    </td><td>hvert</td><td>fall </td><td>to   </td><td>ganger</td><td>i    </td><td>året </td><td>sist </td><td>vi   </td><td>var  </td><td>på   </td><td>hjem </td><td>##stedet</td><td>i    </td><td>mo   </td><td>mo   </td><td>mo   </td><td>##ira</td><td>##nav</td><td>##er </td><td>##er </td><td>sist </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.025</td><td>0.005</td><td>0.008</td><td>0.011   </td><td>0.006</td><td>0.003</td><td>0.003</td><td>0.003</td><td>0.003 </td><td>0.003</td><td>0.005</td><td>0.017</td><td>0.008</td><td>0.014</td><td>0.009</td><td>0.013</td><td>0.021   </td><td>0.069</td><td>0.276</td><td>0.677</td><td>0.648</td><td>0.589</td><td>0.707</td><td>0.528</td><td>0.627</td><td>0.109</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_126\n",
      "ref: har det noe man utøver sjefen i vinmonopolet blir sittende etter smøresakene de ansatte er irritert lurer på hvorfor noen\n",
      "-----\n",
      "orig hyp: har det noe man utøver sjefen i vinmonopolet blir sittende etter smøresakene de ansatte er irritert\n",
      "orig_wer: 0.2 orig_asd 0.1311465097663045\n",
      "-----\n",
      "mod hyp: har det noe å utøve sjefen i vinmonopolet blir sittende etter smøresakene de ansatte er iritert\n",
      "mod_wer: 0.35 mod_asd: 0.338422189739016\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['lurer', 'på', 'hvorfor', 'noen'], 'ref_pos': ['VERB', 'SCONJ', 'ADV', 'PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>har  </td><td>det  </td><td>noe  </td><td>man  </td><td>utøver</td><td>sjefen</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>blir</td><td>sittende</td><td>etter</td><td>smøre</td><td>##sakene</td><td>de  </td><td>ansatte</td><td>er   </td><td>irritert</td><td>lurer   </td><td>på      </td><td>hvorfor </td><td>noen    </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>har  </td><td>det  </td><td>noe  </td><td>man  </td><td>utøver</td><td>sjefen</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>blir</td><td>sittende</td><td>etter</td><td>smøre</td><td>##sakene</td><td>de  </td><td>ansatte</td><td>er   </td><td>irritert</td><td>irritert</td><td>irritert</td><td>irritert</td><td>irritert</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.084</td><td>0.013</td><td>0.005</td><td>0.007</td><td>0.005</td><td>0.005 </td><td>0.004 </td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.002   </td><td>0.01</td><td>0.004   </td><td>0.009</td><td>0.007</td><td>0.01    </td><td>0.01</td><td>0.007  </td><td>0.032</td><td>0.088   </td><td>0.608   </td><td>0.703   </td><td>0.65    </td><td>0.746   </td><td>0.021</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['man', 'utøver'], 'ref_pos': ['PRON', 'VERB'], 'hyp': ['å', 'utøve'], 'hyp_pos': ['PART', 'VERB']}\n",
      "{'type': 'substitute', 'ref': ['irritert'], 'ref_pos': ['ADJ'], 'hyp': ['iritert'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'delete', 'ref': ['lurer', 'på', 'hvorfor', 'noen'], 'ref_pos': ['VERB', 'SCONJ', 'ADV', 'PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 25, ASD=0.34\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>har </td><td>det  </td><td>noe  </td><td>man  </td><td>utøver</td><td>sjefen</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>blir </td><td>sittende</td><td>etter</td><td>smøre</td><td>##sakene</td><td>de   </td><td>ansatte</td><td>er   </td><td>irritert</td><td>irritert</td><td>irritert</td><td>lurer</td><td>på   </td><td>hvorfor</td><td>noen </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>har </td><td>det  </td><td>noe  </td><td>å    </td><td>utøve </td><td>sjefen</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>blir </td><td>sittende</td><td>etter</td><td>smøre</td><td>##sakene</td><td>de   </td><td>ansatte</td><td>er   </td><td>ir      </td><td>##iter  </td><td>##t     </td><td>##t  </td><td>##t  </td><td>##t    </td><td>##t  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.077</td><td>0.06</td><td>0.059</td><td>0.172</td><td>0.545</td><td>0.264 </td><td>0.022 </td><td>0.008</td><td>0.004</td><td>0.005</td><td>0.004   </td><td>0.027</td><td>0.016   </td><td>0.031</td><td>0.017</td><td>0.026   </td><td>0.026</td><td>0.017  </td><td>0.098</td><td>0.63    </td><td>0.708   </td><td>0.611   </td><td>0.777</td><td>0.668</td><td>0.793  </td><td>0.728</td><td>0.017</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_051206_NRK_D12_NO_OOLremoved_cut_24\n",
      "ref: drept etter et mislykket attentatforsøk mot saddam i nitten åttito de to mennene som vitnet i går var øyenvitner til\n",
      "-----\n",
      "orig hyp: drept etter et mislykket attentatforsøk mot saddam i nitten åttito de to mennene som vitnet i går var øyenvitner til\n",
      "orig_wer: 0.0 orig_asd 0.0\n",
      "-----\n",
      "mod hyp: drept etter et mislykket atentatforsøk mot sadam i nitten åttito de to mennene som vitnet i går var øyenvitner til\n",
      "mod_wer: 0.1 mod_asd: 0.1856716832373073\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 31, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et</td><td>mislykket</td><td>atten</td><td>##ta</td><td>##tfor</td><td>##søk</td><td>mot</td><td>sad</td><td>##dam</td><td>i</td><td>nit</td><td>##ten</td><td>åt</td><td>##tit</td><td>##o</td><td>de</td><td>to</td><td>mennene</td><td>som</td><td>vitnet</td><td>i</td><td>går</td><td>var</td><td>øyen</td><td>##vitne</td><td>##r</td><td>til</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et</td><td>mislykket</td><td>atten</td><td>##ta</td><td>##tfor</td><td>##søk</td><td>mot</td><td>sad</td><td>##dam</td><td>i</td><td>nit</td><td>##ten</td><td>åt</td><td>##tit</td><td>##o</td><td>de</td><td>to</td><td>mennene</td><td>som</td><td>vitnet</td><td>i</td><td>går</td><td>var</td><td>øyen</td><td>##vitne</td><td>##r</td><td>til</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0    </td><td>0    </td><td>0 </td><td>0        </td><td>0    </td><td>0   </td><td>0     </td><td>0    </td><td>0  </td><td>0  </td><td>0    </td><td>0</td><td>0  </td><td>0    </td><td>0 </td><td>0    </td><td>0  </td><td>0 </td><td>0 </td><td>0      </td><td>0  </td><td>0     </td><td>0</td><td>0  </td><td>0  </td><td>0   </td><td>0      </td><td>0  </td><td>0  </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['attentatforsøk'], 'ref_pos': ['NOUN'], 'hyp': ['atentatforsøk'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['saddam'], 'ref_pos': ['NOUN'], 'hyp': ['sadam'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.19\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>atten    </td><td>atten</td><td>##ta </td><td>##ta </td><td>##tfor</td><td>##søk   </td><td>mot  </td><td>sad  </td><td>##dam</td><td>i    </td><td>nit  </td><td>##ten</td><td>åt   </td><td>##tit</td><td>##o  </td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går  </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>drept</td><td>etter</td><td>et   </td><td>mislykket</td><td>mislykket</td><td>ate  </td><td>##nt </td><td>##at </td><td>##at  </td><td>##forsøk</td><td>mot  </td><td>sad  </td><td>##am </td><td>i    </td><td>nit  </td><td>##ten</td><td>åt   </td><td>##tit</td><td>##o  </td><td>de   </td><td>to   </td><td>mennene</td><td>som  </td><td>vitnet</td><td>i    </td><td>går  </td><td>var  </td><td>øyen </td><td>##vitne</td><td>##r  </td><td>til  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.006</td><td>0.007</td><td>0.011</td><td>0.023    </td><td>0.631    </td><td>0.65 </td><td>0.624</td><td>0.536</td><td>0.649 </td><td>0.347   </td><td>0.043</td><td>0.096</td><td>0.419</td><td>0.017</td><td>0.008</td><td>0.004</td><td>0.004</td><td>0.003</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.005  </td><td>0.002</td><td>0.003 </td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.002</td><td>0.003  </td><td>0.001</td><td>0.002</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_051206_NRK_D12_NO_OOLremoved_cut_29\n",
      "ref: torturert i saddams fengsler men etter en kort stund sa saddams forsvarere at fordreiningen av stemmen gjorde at de ikke\n",
      "-----\n",
      "orig hyp: torturert i sadamsfengsler men etter en kort stund sa sa dams forsvarere at fordreiningen av stemmen gjorde at de ikke\n",
      "orig_wer: 0.2 orig_asd 0.1602890419428558\n",
      "-----\n",
      "mod hyp: torturert i sadam ⁇ sfengsler men etter en kort stund sa sa dams forsvarere at fordregningen av stemmen gjorde at de ikke\n",
      "mod_wer: 0.25 mod_asd: 0.3188164711807426\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['saddams'], 'ref_pos': ['PROPN'], 'hyp': ['sadamsfengsler'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['fengsler'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['sa'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['saddams'], 'ref_pos': ['PROPN'], 'hyp': ['dams'], 'hyp_pos': ['PROPN']}\n",
      "ASD-NorBERT ref len: 30, ASD=0.16\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>tortur</td><td>##ert</td><td>i    </td><td>sad  </td><td>##dam</td><td>##s  </td><td>fengsler</td><td>fengsler</td><td>men  </td><td>etter</td><td>en   </td><td>kort </td><td>stund</td><td>sa   </td><td>sa   </td><td>sad  </td><td>##dam</td><td>##s  </td><td>forsvarer</td><td>##e  </td><td>at   </td><td>ford </td><td>##rein</td><td>##ingen</td><td>av   </td><td>stemmen</td><td>gjorde</td><td>at   </td><td>de   </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>tortur</td><td>##ert</td><td>i    </td><td>sad  </td><td>##ams</td><td>##ams</td><td>##feng  </td><td>##sler  </td><td>men  </td><td>etter</td><td>en   </td><td>kort </td><td>stund</td><td>sa   </td><td>sa   </td><td>sa   </td><td>dam  </td><td>##s  </td><td>forsvarer</td><td>##e  </td><td>at   </td><td>ford </td><td>##rein</td><td>##ingen</td><td>av   </td><td>stemmen</td><td>gjorde</td><td>at   </td><td>de   </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.004</td><td>0.008 </td><td>0.006</td><td>0.027</td><td>0.097</td><td>0.53 </td><td>0.617</td><td>0.289   </td><td>0.435   </td><td>0.011</td><td>0.004</td><td>0.003</td><td>0.004</td><td>0.003</td><td>0.028</td><td>0.123</td><td>0.671</td><td>0.363</td><td>0.061</td><td>0.008    </td><td>0.006</td><td>0.004</td><td>0.003</td><td>0.004 </td><td>0.003  </td><td>0.003</td><td>0.005  </td><td>0.002 </td><td>0.001</td><td>0.003</td><td>0.001</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['saddams'], 'ref_pos': ['PROPN'], 'hyp': ['sadam'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['fengsler'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['stund'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['saddams'], 'ref_pos': ['PROPN'], 'hyp': ['sa'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['fordreiningen'], 'ref_pos': ['NOUN'], 'hyp': ['forsvarere'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 30, ASD=0.32\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>tortur</td><td>##ert</td><td>i    </td><td>sad  </td><td>##dam</td><td>##s  </td><td>##s  </td><td>##s   </td><td>fengsler</td><td>men  </td><td>etter</td><td>en   </td><td>kort </td><td>stund</td><td>sa   </td><td>sa  </td><td>sad  </td><td>##dam</td><td>##s  </td><td>forsvarer</td><td>##e  </td><td>at   </td><td>ford </td><td>##rein</td><td>##ingen    </td><td>av   </td><td>stemmen</td><td>gjorde</td><td>at   </td><td>de   </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>tortur</td><td>##ert</td><td>i    </td><td>sad  </td><td>##am </td><td>[UNK]</td><td>sf   </td><td>##engs</td><td>##ler   </td><td>men  </td><td>etter</td><td>en   </td><td>kort </td><td>stund</td><td>sa   </td><td>sa  </td><td>sa   </td><td>dam  </td><td>##s  </td><td>forsvarer</td><td>##e  </td><td>at   </td><td>ford </td><td>ford  </td><td>##regningen</td><td>av   </td><td>stemmen</td><td>gjorde</td><td>at   </td><td>de   </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.019 </td><td>0.019</td><td>0.068</td><td>0.124</td><td>0.508</td><td>0.6  </td><td>0.742</td><td>0.781 </td><td>0.619   </td><td>0.025</td><td>0.012</td><td>0.016</td><td>0.019</td><td>0.016</td><td>0.041</td><td>0.12</td><td>0.682</td><td>0.366</td><td>0.077</td><td>0.025    </td><td>0.021</td><td>0.017</td><td>0.112</td><td>0.579 </td><td>0.467      </td><td>0.067</td><td>0.031  </td><td>0.017 </td><td>0.012</td><td>0.012</td><td>0.009</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_62\n",
      "ref: og det kommer til å gjøre situasjonen for grøholt veldig vanskelig hva må grøholt gjøre nå for å bygge opp\n",
      "-----\n",
      "orig hyp: og det kommer til gjøre situasjonen for reol veldig vanskelig hva må grøholt gjøre nå for å bygge opp\n",
      "orig_wer: 0.1 orig_asd 0.1779261551168485\n",
      "-----\n",
      "mod hyp: og det kommer til situasjonen for rhold veldig vanskelig hva må grualt gjøre nå for å bygge opp\n",
      "mod_wer: 0.2 mod_asd: 0.3233021641011914\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['å'], 'ref_pos': ['PART'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['grøholt'], 'ref_pos': ['PROPN'], 'hyp': ['reol'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.18\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og  </td><td>det  </td><td>kommer</td><td>til  </td><td>å   </td><td>gjøre</td><td>situasjonen</td><td>for  </td><td>grø  </td><td>##holt</td><td>veldig</td><td>vanskelig</td><td>hva  </td><td>må  </td><td>grø  </td><td>##holt</td><td>gjøre</td><td>nå   </td><td>for  </td><td>å    </td><td>bygge</td><td>opp  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og  </td><td>det  </td><td>kommer</td><td>til  </td><td>til </td><td>gjøre</td><td>situasjonen</td><td>for  </td><td>re   </td><td>##ol  </td><td>veldig</td><td>vanskelig</td><td>hva  </td><td>må  </td><td>grø  </td><td>##holt</td><td>gjøre</td><td>nå   </td><td>for  </td><td>å    </td><td>bygge</td><td>opp  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.019</td><td>0.01</td><td>0.009</td><td>0.023 </td><td>0.082</td><td>0.35</td><td>0.059</td><td>0.023      </td><td>0.052</td><td>0.631</td><td>0.576 </td><td>0.034 </td><td>0.016    </td><td>0.017</td><td>0.03</td><td>0.155</td><td>0.138 </td><td>0.024</td><td>0.013</td><td>0.011</td><td>0.014</td><td>0.013</td><td>0.012</td><td>0.007</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['å', 'gjøre'], 'ref_pos': ['PART', 'VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['grøholt'], 'ref_pos': ['PROPN'], 'hyp': ['rhold'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['grøholt'], 'ref_pos': ['PROPN'], 'hyp': ['grualt'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.32\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>og   </td><td>det </td><td>kommer</td><td>til  </td><td>å    </td><td>gjøre</td><td>situasjonen</td><td>for </td><td>grø  </td><td>##holt</td><td>veldig</td><td>vanskelig</td><td>hva  </td><td>må   </td><td>grø  </td><td>##holt</td><td>gjøre</td><td>nå   </td><td>for  </td><td>å    </td><td>bygge</td><td>opp  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>og   </td><td>det </td><td>kommer</td><td>til  </td><td>til  </td><td>til  </td><td>situasjonen</td><td>for </td><td>r    </td><td>##hold</td><td>veldig</td><td>vanskelig</td><td>hva  </td><td>må   </td><td>gru  </td><td>##alt </td><td>gjøre</td><td>nå   </td><td>for  </td><td>å    </td><td>bygge</td><td>opp  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.014</td><td>0.014</td><td>0.04</td><td>0.131 </td><td>0.229</td><td>0.554</td><td>0.641</td><td>0.11       </td><td>0.12</td><td>0.611</td><td>0.566 </td><td>0.047 </td><td>0.05     </td><td>0.028</td><td>0.047</td><td>0.572</td><td>0.561 </td><td>0.042</td><td>0.021</td><td>0.017</td><td>0.018</td><td>0.025</td><td>0.017</td><td>0.008</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_161\n",
      "ref: fra sv og senterpartiet synes så synd på sine ordførere der ute de trenger tydeligvis mer inntekter mer skatteinntekter det\n",
      "-----\n",
      "orig hyp: fra sv og senterpartiet synes så synd på sine ordførere der ute så de trenger tydeligvis mer inntekter mer skatteinntekter\n",
      "orig_wer: 0.1 orig_asd 0.1014656038370646\n",
      "-----\n",
      "mod hyp: fra sv og senterpartiet syns syn på sine ordførere der ute så de trenger tydeligvis mer inntekter mer skatteinntekter\n",
      "mod_wer: 0.25 mod_asd: 0.2435216010087552\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 23, ASD=0.10\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>fra  </td><td>sv   </td><td>og   </td><td>senter</td><td>##partiet</td><td>synes</td><td>så   </td><td>synd </td><td>på   </td><td>sine </td><td>ordførere</td><td>der  </td><td>ute  </td><td>ute  </td><td>de   </td><td>trenger</td><td>tydeligvis</td><td>mer  </td><td>inntekter</td><td>mer  </td><td>skatteinntekter</td><td>det            </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>fra  </td><td>sv   </td><td>og   </td><td>senter</td><td>##partiet</td><td>synes</td><td>så   </td><td>synd </td><td>på   </td><td>sine </td><td>ordførere</td><td>der  </td><td>ute  </td><td>så   </td><td>de   </td><td>trenger</td><td>tydeligvis</td><td>mer  </td><td>inntekter</td><td>mer  </td><td>skatteinntekter</td><td>skatteinntekter</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.074</td><td>0.011</td><td>0.004</td><td>0.006</td><td>0.002 </td><td>0.001    </td><td>0.01 </td><td>0.009</td><td>0.005</td><td>0.005</td><td>0.004</td><td>0.003    </td><td>0.012</td><td>0.041</td><td>0.694</td><td>0.076</td><td>0.015  </td><td>0.024     </td><td>0.013</td><td>0.015    </td><td>0.034</td><td>0.149          </td><td>0.66           </td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['synes', 'så'], 'ref_pos': ['VERB', 'ADV'], 'hyp': ['syns', 'syn'], 'hyp_pos': ['VERB', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['synd'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['så'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 23, ASD=0.24\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>fra  </td><td>sv   </td><td>og   </td><td>senter</td><td>##partiet</td><td>synes</td><td>synes</td><td>så   </td><td>synd </td><td>på   </td><td>sine </td><td>ordførere</td><td>der  </td><td>ute  </td><td>ute  </td><td>de   </td><td>trenger</td><td>tydeligvis</td><td>mer  </td><td>inntekter</td><td>mer  </td><td>skatteinntekter</td><td>det            </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>fra  </td><td>sv   </td><td>og   </td><td>senter</td><td>##partiet</td><td>syns </td><td>syn  </td><td>syn  </td><td>syn  </td><td>på   </td><td>sine </td><td>ordførere</td><td>der  </td><td>ute  </td><td>så   </td><td>de   </td><td>trenger</td><td>tydeligvis</td><td>mer  </td><td>inntekter</td><td>mer  </td><td>skatteinntekter</td><td>skatteinntekter</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.069</td><td>0.021</td><td>0.009</td><td>0.022</td><td>0.006 </td><td>0.017    </td><td>0.395</td><td>0.614</td><td>0.777</td><td>0.767</td><td>0.122</td><td>0.028</td><td>0.02     </td><td>0.028</td><td>0.055</td><td>0.678</td><td>0.068</td><td>0.011  </td><td>0.013     </td><td>0.007</td><td>0.01     </td><td>0.029</td><td>0.137          </td><td>0.671          </td><td>0.017</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: p1_g02_m2_5_x-free_cut_10\n",
      "ref: ei ut av de dalsluktene så plutselig kom vi rundt en sving der og vind tok meg og blåste meg\n",
      "-----\n",
      "orig hyp: ut av de dalslukten så plutselig kom vi rundt en sving der og vind tok meg å blåste meg og\n",
      "orig_wer: 0.2 orig_asd 0.1566953027794862\n",
      "-----\n",
      "mod hyp: ut av di dalslukten så plutselig kom vi rundt en sving der og vind tok meg å bloste meg og\n",
      "mod_wer: 0.3 mod_asd: 0.298631382055487\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['ei'], 'ref_pos': ['DET'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['dalsluktene'], 'ref_pos': ['NOUN'], 'hyp': ['dalslukten'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['å'], 'hyp_pos': ['CCONJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.16\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ei   </td><td>ut   </td><td>av   </td><td>de   </td><td>dal  </td><td>##sluk</td><td>##tene</td><td>så   </td><td>plutselig</td><td>kom  </td><td>vi   </td><td>rundt</td><td>en   </td><td>sving</td><td>der  </td><td>og   </td><td>vind </td><td>tok  </td><td>meg  </td><td>og   </td><td>blåste</td><td>meg  </td><td>meg  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>ut   </td><td>av   </td><td>de   </td><td>dal  </td><td>##sluk</td><td>##ten </td><td>så   </td><td>plutselig</td><td>kom  </td><td>vi   </td><td>rundt</td><td>en   </td><td>sving</td><td>der  </td><td>og   </td><td>vind </td><td>tok  </td><td>meg  </td><td>å    </td><td>blåste</td><td>meg  </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.079</td><td>0.712</td><td>0.147</td><td>0.032</td><td>0.034</td><td>0.023</td><td>0.034 </td><td>0.243 </td><td>0.016</td><td>0.014    </td><td>0.008</td><td>0.009</td><td>0.012</td><td>0.011</td><td>0.011</td><td>0.011</td><td>0.013</td><td>0.012</td><td>0.032</td><td>0.041</td><td>0.343</td><td>0.058 </td><td>0.049</td><td>0.641</td><td>0.012</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['ei'], 'ref_pos': ['DET'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['de', 'dalsluktene'], 'ref_pos': ['DET', 'NOUN'], 'hyp': ['di', 'dalslukten'], 'hyp_pos': ['X', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['og', 'blåste'], 'ref_pos': ['CCONJ', 'VERB'], 'hyp': ['å', 'bloste'], 'hyp_pos': ['PART', 'VERB']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['og'], 'hyp_pos': ['CCONJ']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.30\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>ei   </td><td>ut   </td><td>av  </td><td>de   </td><td>dal  </td><td>##sluk</td><td>##tene</td><td>så   </td><td>plutselig</td><td>kom </td><td>vi   </td><td>rundt</td><td>en   </td><td>sving</td><td>der  </td><td>og   </td><td>vind </td><td>tok  </td><td>meg  </td><td>og   </td><td>og   </td><td>blåste</td><td>meg </td><td>meg  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>ut   </td><td>av  </td><td>di   </td><td>dal  </td><td>##sluk</td><td>##ten </td><td>så   </td><td>plutselig</td><td>kom </td><td>vi   </td><td>rundt</td><td>en   </td><td>sving</td><td>der  </td><td>og   </td><td>vind </td><td>tok  </td><td>meg  </td><td>å    </td><td>bl   </td><td>##oste</td><td>meg </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.077</td><td>0.708</td><td>0.146</td><td>0.06</td><td>0.482</td><td>0.068</td><td>0.05  </td><td>0.261 </td><td>0.019</td><td>0.016    </td><td>0.01</td><td>0.012</td><td>0.014</td><td>0.013</td><td>0.017</td><td>0.014</td><td>0.019</td><td>0.051</td><td>0.069</td><td>0.069</td><td>0.412</td><td>0.725</td><td>0.611 </td><td>0.07</td><td>0.659</td><td>0.017</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_051206_NRK_D12_NO_OOLremoved_cut_12\n",
      "ref: i røyk formodentlig i det flyet eksploderer så har den oppstått så mere om bakgrunnen for selve nødlandingen er ikke\n",
      "-----\n",
      "orig hyp: i røyk formodentlig i det flyet eksploderer så har den oppstått så mere om bakgrunnen for selve nødlandingen er ikke\n",
      "orig_wer: 0.0 orig_asd 0.0\n",
      "-----\n",
      "mod hyp: i røyk for muntlig i det flyet eksploderer så har den oppstått så mere om bakgrunnen for selve nødlandingen er ikke\n",
      "mod_wer: 0.1 mod_asd: 0.1317184039331683\n",
      "---Orig Error---\n",
      "ASD-NorBERT ref len: 26, ASD=0.00\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i</td><td>røyk</td><td>formod</td><td>##entlig</td><td>i</td><td>det</td><td>flyet</td><td>eksploder</td><td>##er</td><td>så</td><td>har</td><td>den</td><td>oppstått</td><td>så</td><td>mere</td><td>om</td><td>bakgrunnen</td><td>for</td><td>selve</td><td>nød</td><td>##landing</td><td>##en</td><td>er</td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>i</td><td>røyk</td><td>formod</td><td>##entlig</td><td>i</td><td>det</td><td>flyet</td><td>eksploder</td><td>##er</td><td>så</td><td>har</td><td>den</td><td>oppstått</td><td>så</td><td>mere</td><td>om</td><td>bakgrunnen</td><td>for</td><td>selve</td><td>nød</td><td>##landing</td><td>##en</td><td>er</td><td>ikke</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0    </td><td>0</td><td>0   </td><td>0     </td><td>0       </td><td>0</td><td>0  </td><td>0    </td><td>0        </td><td>0   </td><td>0 </td><td>0  </td><td>0  </td><td>0       </td><td>0 </td><td>0   </td><td>0 </td><td>0         </td><td>0  </td><td>0    </td><td>0  </td><td>0        </td><td>0   </td><td>0 </td><td>0   </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['for'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['formodentlig'], 'ref_pos': ['ADJ'], 'hyp': ['muntlig'], 'hyp_pos': ['ADJ']}\n",
      "ASD-NorBERT ref len: 26, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>røyk </td><td>røyk </td><td>røyk   </td><td>formod </td><td>##entlig</td><td>i   </td><td>det </td><td>flyet</td><td>eksploder</td><td>##er </td><td>så   </td><td>har  </td><td>den  </td><td>oppstått</td><td>så   </td><td>mere </td><td>om   </td><td>bakgrunnen</td><td>for  </td><td>selve</td><td>nød  </td><td>##landing</td><td>##en </td><td>er   </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>i    </td><td>røyk </td><td>for  </td><td>muntlig</td><td>muntlig</td><td>muntlig </td><td>i   </td><td>det </td><td>flyet</td><td>eksploder</td><td>##er </td><td>så   </td><td>har  </td><td>den  </td><td>oppstått</td><td>så   </td><td>mere </td><td>om   </td><td>bakgrunnen</td><td>for  </td><td>selve</td><td>nød  </td><td>##landing</td><td>##en </td><td>er   </td><td>ikke </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.004</td><td>0.016</td><td>0.057</td><td>0.741</td><td>0.608  </td><td>0.814  </td><td>0.753   </td><td>0.07</td><td>0.02</td><td>0.008</td><td>0.006    </td><td>0.006</td><td>0.007</td><td>0.006</td><td>0.005</td><td>0.004   </td><td>0.006</td><td>0.006</td><td>0.003</td><td>0.005     </td><td>0.008</td><td>0.004</td><td>0.002</td><td>0.002    </td><td>0.003</td><td>0.003</td><td>0.005</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_37\n",
      "ref: er skikket til denne tilliten butikksjefene ble felt fordi de mottok smøring fra en vinimportør styreleder harald arnkværn mener det\n",
      "-----\n",
      "orig hyp: skikket til denne tilliten butikksjefene ble felt fordi de mottok smøring fra en vinimportør styreleder harald arnkværn mener de\n",
      "orig_wer: 0.1 orig_asd 0.0929031326972796\n",
      "-----\n",
      "mod hyp: butikksjefene ble felt fordi de mottok smøring fra en vinimportør styreleder harald arnkværn mener de\n",
      "mod_wer: 0.3 mod_asd: 0.222046700156976\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['de'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er   </td><td>skikket</td><td>til </td><td>denne</td><td>tilliten</td><td>butikk</td><td>##sjefen</td><td>##e  </td><td>ble  </td><td>felt </td><td>fordi</td><td>de   </td><td>mottok</td><td>smør </td><td>##ing</td><td>fra  </td><td>en   </td><td>vin </td><td>##import</td><td>##ør </td><td>styreleder</td><td>har  </td><td>##ald</td><td>ar   </td><td>##nk </td><td>##vær</td><td>##n  </td><td>mener</td><td>det  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>skikket</td><td>til </td><td>denne</td><td>tilliten</td><td>butikk</td><td>##sjefen</td><td>##e  </td><td>ble  </td><td>felt </td><td>fordi</td><td>de   </td><td>mottok</td><td>smør </td><td>##ing</td><td>fra  </td><td>en   </td><td>vin </td><td>##import</td><td>##ør </td><td>styreleder</td><td>har  </td><td>##ald</td><td>ar   </td><td>##nk </td><td>##vær</td><td>##n  </td><td>mener</td><td>de   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.089</td><td>0.63 </td><td>0.209  </td><td>0.07</td><td>0.025</td><td>0.02    </td><td>0.013 </td><td>0.014   </td><td>0.017</td><td>0.028</td><td>0.013</td><td>0.01 </td><td>0.012</td><td>0.009 </td><td>0.011</td><td>0.01 </td><td>0.005</td><td>0.008</td><td>0.01</td><td>0.013   </td><td>0.013</td><td>0.018     </td><td>0.016</td><td>0.021</td><td>0.012</td><td>0.013</td><td>0.016</td><td>0.018</td><td>0.05 </td><td>0.398</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['er', 'skikket', 'til', 'denne', 'tilliten'], 'ref_pos': ['AUX', 'ADJ', 'ADP', 'DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': ['de'], 'hyp_pos': ['PRON']}\n",
      "ASD-NorBERT ref len: 31, ASD=0.22\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>er   </td><td>skikket</td><td>til  </td><td>denne</td><td>tilliten</td><td>butikk</td><td>##sjefen</td><td>##e  </td><td>ble  </td><td>felt </td><td>fordi</td><td>de   </td><td>mottok</td><td>smør </td><td>##ing</td><td>fra  </td><td>en  </td><td>vin  </td><td>##import</td><td>##ør </td><td>styreleder</td><td>har  </td><td>##ald</td><td>ar   </td><td>##nk </td><td>##vær</td><td>##n  </td><td>mener</td><td>det  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>[CLS]  </td><td>[CLS]</td><td>[CLS]</td><td>[CLS]   </td><td>butikk</td><td>##sjefen</td><td>##e  </td><td>ble  </td><td>felt </td><td>fordi</td><td>de   </td><td>mottok</td><td>smør </td><td>##ing</td><td>fra  </td><td>en  </td><td>vin  </td><td>##import</td><td>##ør </td><td>styreleder</td><td>har  </td><td>##ald</td><td>ar   </td><td>##nk </td><td>##vær</td><td>##n  </td><td>mener</td><td>de   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.066</td><td>0.689</td><td>0.783  </td><td>0.769</td><td>0.738</td><td>0.723   </td><td>0.144 </td><td>0.07    </td><td>0.086</td><td>0.084</td><td>0.051</td><td>0.039</td><td>0.036</td><td>0.038 </td><td>0.038</td><td>0.042</td><td>0.029</td><td>0.03</td><td>0.031</td><td>0.037   </td><td>0.037</td><td>0.047     </td><td>0.038</td><td>0.056</td><td>0.032</td><td>0.034</td><td>0.035</td><td>0.033</td><td>0.064</td><td>0.414</td><td>0.012</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_051206_NRK_D12_NO_OOLremoved_cut_31\n",
      "ref: gang igjen med vitnemålet for kort tid siden ja det sa reporter annette groth i dag blir det klart om\n",
      "-----\n",
      "orig hyp: gang igjen med vitnemålet for kort tid siden ja det sa reporter anette grot i dag blir det klart om\n",
      "orig_wer: 0.1 orig_asd 0.1342408172442715\n",
      "-----\n",
      "mod hyp: gang igjen med vitnemålet for kort tid siden en sa reporter anette grot i dag blir det klart om\n",
      "mod_wer: 0.2 mod_asd: 0.2599159753863423\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['annette', 'groth'], 'ref_pos': ['ADJ', 'PROPN'], 'hyp': ['anette', 'grot'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.13\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>gang </td><td>igjen</td><td>med  </td><td>vitnemål</td><td>##et </td><td>for  </td><td>kort </td><td>tid  </td><td>siden</td><td>ja   </td><td>det  </td><td>sa   </td><td>reporter</td><td>annet</td><td>##te </td><td>gro  </td><td>##th </td><td>i    </td><td>dag  </td><td>blir </td><td>det  </td><td>klart</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>gang </td><td>igjen</td><td>med  </td><td>vitnemål</td><td>##et </td><td>for  </td><td>kort </td><td>tid  </td><td>siden</td><td>ja   </td><td>det  </td><td>sa   </td><td>reporter</td><td>ane  </td><td>##tte</td><td>gro  </td><td>##t  </td><td>i    </td><td>dag  </td><td>blir </td><td>det  </td><td>klart</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.004   </td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.003</td><td>0.008</td><td>0.014</td><td>0.025</td><td>0.068   </td><td>0.509</td><td>0.351</td><td>0.196</td><td>0.456</td><td>0.016</td><td>0.006</td><td>0.004</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['ja'], 'ref_pos': ['INTJ'], 'hyp': ['en'], 'hyp_pos': ['DET']}\n",
      "{'type': 'delete', 'ref': ['det'], 'ref_pos': ['PRON'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['annette', 'groth'], 'ref_pos': ['ADJ', 'PROPN'], 'hyp': ['anette', 'grot'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.26\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>gang </td><td>igjen</td><td>med  </td><td>vitnemål</td><td>##et </td><td>for  </td><td>kort </td><td>tid  </td><td>siden</td><td>ja   </td><td>det  </td><td>sa   </td><td>reporter</td><td>annet</td><td>##te </td><td>gro  </td><td>##th </td><td>i    </td><td>dag  </td><td>blir </td><td>det  </td><td>klart</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>gang </td><td>igjen</td><td>med  </td><td>vitnemål</td><td>##et </td><td>for  </td><td>kort </td><td>tid  </td><td>siden</td><td>siden</td><td>en   </td><td>sa   </td><td>reporter</td><td>ane  </td><td>##tte</td><td>gro  </td><td>##t  </td><td>i    </td><td>dag  </td><td>blir </td><td>det  </td><td>klart</td><td>om   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.009</td><td>0.004</td><td>0.005</td><td>0.006</td><td>0.011   </td><td>0.014</td><td>0.016</td><td>0.007</td><td>0.011</td><td>0.067</td><td>0.783</td><td>0.545</td><td>0.238</td><td>0.183   </td><td>0.553</td><td>0.389</td><td>0.222</td><td>0.467</td><td>0.038</td><td>0.019</td><td>0.017</td><td>0.017</td><td>0.011</td><td>0.009</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_277\n",
      "ref: rødgrønne partiene stemte det ned hadde kommunen blitt med den gangen i tjue femten da man innførte dette ja så\n",
      "-----\n",
      "orig hyp: rødgrønne partiene stemte det ned hadde kommunen den gangen blitt med i to tusen og femten om man innførte dette\n",
      "orig_wer: 0.45 orig_asd 0.3953942579363506\n",
      "-----\n",
      "mod hyp: rødgrønne partiene stemte det jeg hadde kommunen den gangen blitt med i to tusen og femten år man innførte dette\n",
      "mod_wer: 0.5 mod_asd: 0.5104928457833728\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['den', 'gangen'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['den', 'gangen', 'i', 'tjue'], 'ref_pos': ['DET', 'NOUN', 'ADP', 'NUM'], 'hyp': ['i', 'to', 'tusen', 'og'], 'hyp_pos': ['ADP', 'NUM', 'NOUN', 'CCONJ']}\n",
      "{'type': 'substitute', 'ref': ['da'], 'ref_pos': ['SCONJ'], 'hyp': ['om'], 'hyp_pos': ['SCONJ']}\n",
      "{'type': 'delete', 'ref': ['ja', 'så'], 'ref_pos': ['NOUN', 'ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 22, ASD=0.40\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>rødgrønne</td><td>partiene</td><td>stemte</td><td>det  </td><td>ned  </td><td>hadde</td><td>kommunen</td><td>kommunen</td><td>kommunen</td><td>blitt</td><td>med  </td><td>den  </td><td>gangen</td><td>i    </td><td>tjue </td><td>tjue </td><td>tjue </td><td>femten</td><td>da   </td><td>man  </td><td>innførte</td><td>dette</td><td>ja   </td><td>så   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>rødgrønne</td><td>partiene</td><td>stemte</td><td>det  </td><td>ned  </td><td>hadde</td><td>kommunen</td><td>den     </td><td>gangen  </td><td>blitt</td><td>med  </td><td>med  </td><td>med   </td><td>i    </td><td>to   </td><td>tusen</td><td>og   </td><td>femten</td><td>om   </td><td>man  </td><td>innførte</td><td>dette</td><td>dette</td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.055</td><td>0.014    </td><td>0.005   </td><td>0.005 </td><td>0.005</td><td>0.006</td><td>0.029</td><td>0.015   </td><td>0.724   </td><td>0.711   </td><td>0.044</td><td>0.057</td><td>0.735</td><td>0.766 </td><td>0.151</td><td>0.411</td><td>0.484</td><td>0.665</td><td>0.203 </td><td>0.534</td><td>0.045</td><td>0.049   </td><td>0.061</td><td>0.669</td><td>0.596</td><td>0.011</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['ned'], 'ref_pos': ['ADP'], 'hyp': ['jeg'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['den', 'gangen'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['den', 'gangen', 'i', 'tjue'], 'ref_pos': ['DET', 'NOUN', 'ADP', 'NUM'], 'hyp': ['i', 'to', 'tusen', 'og'], 'hyp_pos': ['ADP', 'NUM', 'NOUN', 'CCONJ']}\n",
      "{'type': 'substitute', 'ref': ['da'], 'ref_pos': ['SCONJ'], 'hyp': ['år'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ja', 'så'], 'ref_pos': ['NOUN', 'ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 22, ASD=0.51\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>rødgrønne</td><td>partiene</td><td>stemte</td><td>det  </td><td>det  </td><td>ned</td><td>hadde</td><td>kommunen</td><td>kommunen</td><td>kommunen</td><td>blitt</td><td>med  </td><td>den  </td><td>gangen</td><td>i    </td><td>tjue </td><td>tjue </td><td>tjue </td><td>femten</td><td>femten</td><td>da   </td><td>man  </td><td>innførte</td><td>dette</td><td>ja   </td><td>så   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>rødgrønne</td><td>partiene</td><td>stemte</td><td>det  </td><td>jeg  </td><td>jeg</td><td>hadde</td><td>kommunen</td><td>den     </td><td>gangen  </td><td>blitt</td><td>med  </td><td>med  </td><td>med   </td><td>i    </td><td>to   </td><td>tusen</td><td>og   </td><td>femten</td><td>år    </td><td>år   </td><td>man  </td><td>innførte</td><td>dette</td><td>dette</td><td>dette</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.051</td><td>0.017    </td><td>0.019   </td><td>0.062 </td><td>0.169</td><td>0.659</td><td>0.7</td><td>0.071</td><td>0.044   </td><td>0.735   </td><td>0.717   </td><td>0.042</td><td>0.042</td><td>0.732</td><td>0.767 </td><td>0.137</td><td>0.437</td><td>0.497</td><td>0.716</td><td>0.307 </td><td>0.589 </td><td>0.616</td><td>0.109</td><td>0.055   </td><td>0.066</td><td>0.678</td><td>0.593</td><td>0.012</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: p1_g03_m2_4_x-free_cut_7\n",
      "ref: å ha stor glede av skidagen etterpå var det handling med typiske innkjøp av svenske varer som knekkebrød kjøttboller og\n",
      "-----\n",
      "orig hyp: å ha stor glede av skidagen etterpå var det handling med typisk innkjøp av svenske varer som knekkebrød kjøttbullar og\n",
      "orig_wer: 0.1 orig_asd 0.0811759664604408\n",
      "-----\n",
      "mod hyp: ha stor glede av skidagen etterpå var det handling med typisk innkjøp av svenske varer som knekker brød kjøttbullar og\n",
      "mod_wer: 0.25 mod_asd: 0.1957857465566977\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['typiske'], 'ref_pos': ['ADJ'], 'hyp': ['typisk'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'substitute', 'ref': ['kjøttboller'], 'ref_pos': ['NOUN'], 'hyp': ['kjøttbullar'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>å    </td><td>ha   </td><td>stor </td><td>glede</td><td>av </td><td>ski  </td><td>##dagen</td><td>etterpå</td><td>var  </td><td>det  </td><td>handling</td><td>med  </td><td>typiske</td><td>innkjøp</td><td>av   </td><td>svenske</td><td>varer</td><td>som  </td><td>knekke</td><td>##brød</td><td>kjøtt</td><td>kjøtt</td><td>##boller</td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>å    </td><td>ha   </td><td>stor </td><td>glede</td><td>av </td><td>ski  </td><td>##dagen</td><td>etterpå</td><td>var  </td><td>det  </td><td>handling</td><td>med  </td><td>typisk </td><td>innkjøp</td><td>av   </td><td>svenske</td><td>varer</td><td>som  </td><td>knekke</td><td>##brød</td><td>kjøtt</td><td>##bul</td><td>##lar   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.003</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.0</td><td>0.001</td><td>0.001  </td><td>0.001  </td><td>0.001</td><td>0.001</td><td>0.003   </td><td>0.013</td><td>0.172  </td><td>0.016  </td><td>0.005</td><td>0.003  </td><td>0.003</td><td>0.004</td><td>0.004 </td><td>0.01  </td><td>0.036</td><td>0.526</td><td>0.452   </td><td>0.019</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['å'], 'ref_pos': ['PART'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['typiske'], 'ref_pos': ['ADJ'], 'hyp': ['typisk'], 'hyp_pos': ['ADJ']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['knekker'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['knekkebrød', 'kjøttboller'], 'ref_pos': ['NOUN', 'NOUN'], 'hyp': ['brød', 'kjøttbullar'], 'hyp_pos': ['NOUN', 'NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>å    </td><td>ha   </td><td>stor </td><td>glede</td><td>av   </td><td>ski  </td><td>##dagen</td><td>etterpå</td><td>var  </td><td>det </td><td>handling</td><td>med  </td><td>typiske</td><td>innkjøp</td><td>av   </td><td>svenske</td><td>varer</td><td>som </td><td>knekke</td><td>knekke</td><td>##brød</td><td>kjøtt</td><td>kjøtt</td><td>##boller</td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>ha   </td><td>ha   </td><td>stor </td><td>glede</td><td>av   </td><td>ski  </td><td>##dagen</td><td>etterpå</td><td>var  </td><td>det </td><td>handling</td><td>med  </td><td>typisk </td><td>innkjøp</td><td>av   </td><td>svenske</td><td>varer</td><td>som </td><td>knekke</td><td>##r   </td><td>brød  </td><td>kjøtt</td><td>##bul</td><td>##lar   </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.011</td><td>0.356</td><td>0.155</td><td>0.019</td><td>0.016</td><td>0.012</td><td>0.012</td><td>0.011  </td><td>0.011  </td><td>0.008</td><td>0.01</td><td>0.014   </td><td>0.019</td><td>0.183  </td><td>0.027  </td><td>0.018</td><td>0.016  </td><td>0.02 </td><td>0.09</td><td>0.126 </td><td>0.655 </td><td>0.275 </td><td>0.054</td><td>0.516</td><td>0.457   </td><td>0.023</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_161\n",
      "ref: i erstatning til familien til anne slåtten sa reporter stian sjursen i rogaland ble kim poulsen dømt til seksten års\n",
      "-----\n",
      "orig hyp: erstatning til familien til anne slåtten sa reporter stian sjursen i rogaland ble kim paulsen dømt til seksten års\n",
      "orig_wer: 0.1 orig_asd 0.0870183145914304\n",
      "-----\n",
      "mod hyp: erstatning til familien til hanne slotten sa reporter stian sjursen i rogaland ble kim paulsen dømt til seksten års\n",
      "mod_wer: 0.2 mod_asd: 0.2002938319328333\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['poulsen'], 'ref_pos': ['PROPN'], 'hyp': ['paulsen'], 'hyp_pos': ['PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>erstatning</td><td>til  </td><td>familien</td><td>til  </td><td>anne </td><td>slåtte</td><td>##n  </td><td>sa   </td><td>reporter</td><td>sti  </td><td>##an </td><td>sju  </td><td>##rs </td><td>##en </td><td>i    </td><td>rogaland</td><td>ble  </td><td>kim  </td><td>po   </td><td>##uls</td><td>##en </td><td>dømt </td><td>til  </td><td>seks</td><td>##ten</td><td>års  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>erstatning</td><td>til  </td><td>familien</td><td>til  </td><td>anne </td><td>slåtte</td><td>##n  </td><td>sa   </td><td>reporter</td><td>sti  </td><td>##an </td><td>sju  </td><td>##rs </td><td>##en </td><td>i    </td><td>rogaland</td><td>ble  </td><td>kim  </td><td>pa   </td><td>##uls</td><td>##en </td><td>dømt </td><td>til  </td><td>seks</td><td>##ten</td><td>års  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.023</td><td>0.63 </td><td>0.091     </td><td>0.028</td><td>0.01    </td><td>0.011</td><td>0.022</td><td>0.014 </td><td>0.012</td><td>0.037</td><td>0.021   </td><td>0.014</td><td>0.013</td><td>0.013</td><td>0.014</td><td>0.009</td><td>0.009</td><td>0.014   </td><td>0.009</td><td>0.025</td><td>0.439</td><td>0.055</td><td>0.015</td><td>0.008</td><td>0.006</td><td>0.01</td><td>0.023</td><td>0.012</td><td>0.003</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'delete', 'ref': ['i'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['anne', 'slåtten'], 'ref_pos': ['PROPN', 'NOUN'], 'hyp': ['hanne', 'slotten'], 'hyp_pos': ['PROPN', 'PROPN']}\n",
      "{'type': 'substitute', 'ref': ['poulsen'], 'ref_pos': ['PROPN'], 'hyp': ['paulsen'], 'hyp_pos': ['PROPN']}\n",
      "ASD-NorBERT ref len: 29, ASD=0.20\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>i    </td><td>erstatning</td><td>til  </td><td>familien</td><td>til  </td><td>anne </td><td>anne </td><td>slåtte</td><td>##n  </td><td>sa   </td><td>reporter</td><td>sti  </td><td>##an </td><td>sju  </td><td>##rs </td><td>##en </td><td>i    </td><td>rogaland</td><td>ble  </td><td>kim  </td><td>po   </td><td>##uls</td><td>##en </td><td>dømt </td><td>til  </td><td>seks </td><td>##ten</td><td>års  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>[CLS]</td><td>erstatning</td><td>til  </td><td>familien</td><td>til  </td><td>hann </td><td>##e  </td><td>slott </td><td>##en </td><td>sa   </td><td>reporter</td><td>sti  </td><td>##an </td><td>sju  </td><td>##rs </td><td>##en </td><td>i    </td><td>rogaland</td><td>ble  </td><td>kim  </td><td>pa   </td><td>##uls</td><td>##en </td><td>dømt </td><td>til  </td><td>seks </td><td>##ten</td><td>års  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.028</td><td>0.642</td><td>0.11      </td><td>0.046</td><td>0.022   </td><td>0.047</td><td>0.619</td><td>0.487</td><td>0.556 </td><td>0.256</td><td>0.044</td><td>0.015   </td><td>0.009</td><td>0.005</td><td>0.006</td><td>0.012</td><td>0.013</td><td>0.005</td><td>0.011   </td><td>0.008</td><td>0.019</td><td>0.442</td><td>0.048</td><td>0.013</td><td>0.004</td><td>0.003</td><td>0.002</td><td>0.003</td><td>0.007</td><td>0.001</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_98\n",
      "ref: milliarder kroner mindre jeg må si det er friskt av fremskrittspartiet å gå til valg på å tappe landets kommuner\n",
      "-----\n",
      "orig hyp: milliarder kroner mindre jeg må si det er friskt av fremskrittspartiet går til valg på å tappe landets kommuner\n",
      "orig_wer: 0.1 orig_asd 0.0795567018234984\n",
      "-----\n",
      "mod hyp: milliarder kroner mindre jeg må si det er friskt av fremskrittspartiet går til valg på å ta med landets kommuner\n",
      "mod_wer: 0.2 mod_asd: 0.1923401390872277\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['å'], 'ref_pos': ['PART'], 'hyp': ['går'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['gå'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.08\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>milliarder</td><td>kroner</td><td>mindre</td><td>jeg  </td><td>må   </td><td>si   </td><td>det  </td><td>er   </td><td>friskt</td><td>av   </td><td>fremskritt</td><td>##spartiet</td><td>å         </td><td>gå   </td><td>til  </td><td>valg </td><td>på   </td><td>å    </td><td>tapp </td><td>##e  </td><td>landets</td><td>kommuner</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>milliarder</td><td>kroner</td><td>mindre</td><td>jeg  </td><td>må   </td><td>si   </td><td>det  </td><td>er   </td><td>friskt</td><td>av   </td><td>fremskritt</td><td>##spartiet</td><td>##spartiet</td><td>går  </td><td>til  </td><td>valg </td><td>på   </td><td>å    </td><td>tapp </td><td>##e  </td><td>landets</td><td>kommuner</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.002</td><td>0.001     </td><td>0.001 </td><td>0.001 </td><td>0.001</td><td>0.001</td><td>0.004</td><td>0.021</td><td>0.033</td><td>0.035 </td><td>0.117</td><td>0.01      </td><td>0.018     </td><td>0.734     </td><td>0.226</td><td>0.035</td><td>0.019</td><td>0.017</td><td>0.013</td><td>0.007</td><td>0.009</td><td>0.008  </td><td>0.007   </td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['å'], 'ref_pos': ['PART'], 'hyp': ['går'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['gå'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['ta'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['tappe'], 'ref_pos': ['VERB'], 'hyp': ['med'], 'hyp_pos': ['ADP']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.19\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>milliarder</td><td>kroner</td><td>mindre</td><td>jeg  </td><td>må   </td><td>si   </td><td>det  </td><td>er   </td><td>friskt</td><td>av   </td><td>fremskritt</td><td>##spartiet</td><td>å         </td><td>gå   </td><td>til  </td><td>valg </td><td>på   </td><td>å    </td><td>tapp </td><td>##e  </td><td>##e  </td><td>##e  </td><td>landets</td><td>kommuner</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>milliarder</td><td>kroner</td><td>mindre</td><td>jeg  </td><td>må   </td><td>si   </td><td>det  </td><td>er   </td><td>friskt</td><td>av   </td><td>fremskritt</td><td>##spartiet</td><td>##spartiet</td><td>går  </td><td>til  </td><td>valg </td><td>på   </td><td>å    </td><td>å    </td><td>å    </td><td>ta   </td><td>med  </td><td>landets</td><td>kommuner</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.007</td><td>0.002     </td><td>0.003 </td><td>0.002 </td><td>0.002</td><td>0.002</td><td>0.004</td><td>0.021</td><td>0.031</td><td>0.034 </td><td>0.117</td><td>0.01      </td><td>0.019     </td><td>0.733     </td><td>0.229</td><td>0.037</td><td>0.022</td><td>0.032</td><td>0.051</td><td>0.716</td><td>0.559</td><td>0.592</td><td>0.664</td><td>0.026  </td><td>0.024   </td><td>0.005</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_050211_NRK_D17_NO_OOLremoved_cut_0\n",
      "ref: kundene har presset rema synnøve finden er tilbake i hyllene de ansatte i vinmonopolet knurrer fordi sjefen fikk bli og\n",
      "-----\n",
      "orig hyp: kundene har presset rema synnøve finden er tilbake i hyllene de ansatte i vinmonopolet knurrer fordi sjefen fikk bli\n",
      "orig_wer: 0.05 orig_asd 0.0336379011105335\n",
      "-----\n",
      "mod hyp: kundene har presset rema synnøve finden er tilbake i hyllene de ansatte i vimonopole knurrer fordi sjefen fikk bli\n",
      "mod_wer: 0.1 mod_asd: 0.1459632927891202\n",
      "---Orig Error---\n",
      "{'type': 'delete', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 28, ASD=0.03\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kundene</td><td>har  </td><td>presset</td><td>rema </td><td>syn  </td><td>##nøve</td><td>find </td><td>##en </td><td>er   </td><td>tilbake</td><td>i  </td><td>hyllene</td><td>de   </td><td>ansatte</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>kn   </td><td>##ur </td><td>##rer</td><td>fordi</td><td>sjefen</td><td>fikk </td><td>bli  </td><td>og   </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>kundene</td><td>har  </td><td>presset</td><td>rema </td><td>syn  </td><td>##nøve</td><td>find </td><td>##en </td><td>er   </td><td>tilbake</td><td>i  </td><td>hyllene</td><td>de   </td><td>ansatte</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>kn   </td><td>##ur </td><td>##rer</td><td>fordi</td><td>sjefen</td><td>fikk </td><td>bli  </td><td>bli  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.062</td><td>0.003  </td><td>0.002</td><td>0.001  </td><td>0.002</td><td>0.002</td><td>0.003 </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001  </td><td>0.0</td><td>0.001  </td><td>0.002</td><td>0.001  </td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001   </td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.006</td><td>0.004 </td><td>0.011</td><td>0.091</td><td>0.571</td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['vinmonopolet'], 'ref_pos': ['NOUN'], 'hyp': ['vimonopole'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 28, ASD=0.15\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kundene</td><td>har  </td><td>presset</td><td>rema </td><td>syn  </td><td>##nøve</td><td>find </td><td>##en </td><td>er   </td><td>tilbake</td><td>i    </td><td>hyllene</td><td>de   </td><td>ansatte</td><td>i    </td><td>vin  </td><td>##mon</td><td>##opolet</td><td>##opolet</td><td>kn   </td><td>##ur </td><td>##rer</td><td>fordi</td><td>sjefen</td><td>fikk </td><td>bli  </td><td>og  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>kundene</td><td>har  </td><td>presset</td><td>rema </td><td>syn  </td><td>##nøve</td><td>find </td><td>##en </td><td>er   </td><td>tilbake</td><td>i    </td><td>hyllene</td><td>de   </td><td>ansatte</td><td>i    </td><td>vi   </td><td>##mon</td><td>##opol  </td><td>##e     </td><td>kn   </td><td>##ur </td><td>##rer</td><td>fordi</td><td>sjefen</td><td>fikk </td><td>bli  </td><td>bli </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.057</td><td>0.004  </td><td>0.002</td><td>0.002  </td><td>0.003</td><td>0.003</td><td>0.004 </td><td>0.004</td><td>0.004</td><td>0.002</td><td>0.002  </td><td>0.001</td><td>0.007  </td><td>0.004</td><td>0.007  </td><td>0.035</td><td>0.661</td><td>0.116</td><td>0.328   </td><td>0.65    </td><td>0.024</td><td>0.018</td><td>0.018</td><td>0.015</td><td>0.015 </td><td>0.019</td><td>0.103</td><td>0.56</td><td>0.009</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_23\n",
      "ref: bustad og fritidseigedom er redusert frå sju til fire promille frå tjue fjorten kan kommunane nytta formuesgrunnlaget statens bustadverdiar ved\n",
      "-----\n",
      "orig hyp: bustad fritidseigendom er redusert fra sju til fire promille fra tjue fjorten kan kommunene nytte av formuesgrunnlaget statens bostadsverdie\n",
      "orig_wer: 0.45 orig_asd 0.3198505894963955\n",
      "-----\n",
      "mod hyp: busta fritidseiendom er redusert fra sju til fire promille fra tjuefjorten kan kommunene nytte formuesgrunnlaget statens bostadsverdie\n",
      "mod_wer: 0.55 mod_asd: 0.42435427015962\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['og'], 'ref_pos': ['CCONJ'], 'hyp': ['fritidseigendom'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['fritidseigedom'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['frå'], 'ref_pos': ['ADP'], 'hyp': ['fra'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['frå'], 'ref_pos': ['ADP'], 'hyp': ['fra'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['kommunene'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['kommunane', 'nytta'], 'ref_pos': ['NOUN', 'VERB'], 'hyp': ['nytte', 'av'], 'hyp_pos': ['VERB', 'ADP']}\n",
      "{'type': 'substitute', 'ref': ['bustadverdiar'], 'ref_pos': ['NOUN'], 'hyp': ['bostadsverdie'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ved'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.32\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>bustad</td><td>og    </td><td>fritids</td><td>##eig</td><td>##edom </td><td>er   </td><td>redusert</td><td>frå  </td><td>sju  </td><td>til  </td><td>fire</td><td>promille</td><td>frå  </td><td>tjue </td><td>fjorten</td><td>kan  </td><td>kommunane</td><td>nytta</td><td>nytta</td><td>formues</td><td>##grunnlaget</td><td>statens</td><td>bustad</td><td>bustad</td><td>##verdi</td><td>##ar </td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>bustad</td><td>bustad</td><td>fritids</td><td>##eig</td><td>##endom</td><td>er   </td><td>redusert</td><td>fra  </td><td>sju  </td><td>til  </td><td>fire</td><td>promille</td><td>fra  </td><td>tjue </td><td>fjorten</td><td>kan  </td><td>kommunene</td><td>nytte</td><td>av   </td><td>formues</td><td>##grunnlaget</td><td>statens</td><td>bost  </td><td>##ads </td><td>##verdi</td><td>##e  </td><td>##e  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.197</td><td>0.321 </td><td>0.546 </td><td>0.08   </td><td>0.064</td><td>0.26   </td><td>0.049</td><td>0.028   </td><td>0.128</td><td>0.019</td><td>0.018</td><td>0.02</td><td>0.026   </td><td>0.118</td><td>0.039</td><td>0.036  </td><td>0.058</td><td>0.144    </td><td>0.367</td><td>0.596</td><td>0.029  </td><td>0.029       </td><td>0.057  </td><td>0.522 </td><td>0.593 </td><td>0.102  </td><td>0.525</td><td>0.593</td><td>0.016</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['bustad', 'og'], 'ref_pos': ['NOUN', 'CCONJ'], 'hyp': ['busta', 'fritidseiendom'], 'hyp_pos': ['ADJ', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['fritidseigedom'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['frå'], 'ref_pos': ['ADP'], 'hyp': ['fra'], 'hyp_pos': ['ADP']}\n",
      "{'type': 'substitute', 'ref': ['frå', 'tjue'], 'ref_pos': ['ADP', 'NUM'], 'hyp': ['fra', 'tjuefjorten'], 'hyp_pos': ['ADP', 'NUM']}\n",
      "{'type': 'delete', 'ref': ['fjorten'], 'ref_pos': ['NUM'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['kommunane', 'nytta'], 'ref_pos': ['NOUN', 'VERB'], 'hyp': ['kommunene', 'nytte'], 'hyp_pos': ['NOUN', 'VERB']}\n",
      "{'type': 'substitute', 'ref': ['bustadverdiar'], 'ref_pos': ['NOUN'], 'hyp': ['bostadsverdie'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'delete', 'ref': ['ved'], 'ref_pos': ['ADP'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 27, ASD=0.42\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>bustad</td><td>og   </td><td>fritids</td><td>##eig</td><td>##edom</td><td>##edom</td><td>er   </td><td>redusert</td><td>frå  </td><td>sju  </td><td>til  </td><td>fire </td><td>promille</td><td>frå  </td><td>tjue </td><td>fjorten</td><td>fjorten</td><td>fjorten</td><td>kan </td><td>kommunane</td><td>nytta</td><td>formues</td><td>##grunnlaget</td><td>statens</td><td>bustad</td><td>bustad</td><td>##verdi</td><td>##ar</td><td>ved  </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>bust  </td><td>##a  </td><td>fritids</td><td>##eie</td><td>##nd  </td><td>##om  </td><td>er   </td><td>redusert</td><td>fra  </td><td>sju  </td><td>til  </td><td>fire </td><td>promille</td><td>fra  </td><td>tjue </td><td>tjue   </td><td>##fj   </td><td>##orten</td><td>kan </td><td>kommunene</td><td>nytte</td><td>formues</td><td>##grunnlaget</td><td>statens</td><td>bost  </td><td>##ads </td><td>##verdi</td><td>##e </td><td>##e  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.142</td><td>0.413 </td><td>0.599</td><td>0.093  </td><td>0.385</td><td>0.355 </td><td>0.354 </td><td>0.038</td><td>0.02    </td><td>0.098</td><td>0.026</td><td>0.019</td><td>0.024</td><td>0.033   </td><td>0.111</td><td>0.139</td><td>0.434  </td><td>0.607  </td><td>0.548  </td><td>0.05</td><td>0.105    </td><td>0.235</td><td>0.039  </td><td>0.036       </td><td>0.066  </td><td>0.539 </td><td>0.627 </td><td>0.104  </td><td>0.52</td><td>0.614</td><td>0.019</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_126\n",
      "ref: kom i regjering jeg vil minne om at det var sosialdemokraten göran persson som sørget for at den ble fjernet\n",
      "-----\n",
      "orig hyp: regjering president det kan jeg minne om at det var sosialdemokratene ørjan person som sørget for at den ble fjernet\n",
      "orig_wer: 0.4 orig_asd 0.4157073475380339\n",
      "-----\n",
      "mod hyp: regjering president det kan jeg minne om at det var sosialdemokratene jøran person som sørgeforat den ble fjernet\n",
      "mod_wer: 0.55 mod_asd: 0.5193784657175174\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regjering'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['kom', 'i', 'regjering'], 'ref_pos': ['VERB', 'ADP', 'NOUN'], 'hyp': ['president', 'det', 'kan'], 'hyp_pos': ['NOUN', 'PRON', 'AUX']}\n",
      "{'type': 'delete', 'ref': ['vil'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['sosialdemokraten', 'göran', 'persson'], 'ref_pos': ['NOUN', 'PROPN', 'PROPN'], 'hyp': ['sosialdemokratene', 'ørjan', 'person'], 'hyp_pos': ['NOUN', 'PROPN', 'NOUN']}\n",
      "ASD-NorBERT ref len: 24, ASD=0.42\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kom      </td><td>i        </td><td>regjering</td><td>regjering</td><td>jeg  </td><td>jeg  </td><td>jeg  </td><td>vil  </td><td>minne</td><td>om  </td><td>at   </td><td>det  </td><td>var  </td><td>sosialdemokra</td><td>##ten </td><td>[UNK]</td><td>pers </td><td>pers </td><td>pers  </td><td>##son </td><td>som  </td><td>sørget</td><td>for  </td><td>at   </td><td>den  </td><td>ble </td><td>fjernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>regjering</td><td>regjering</td><td>regjering</td><td>president</td><td>det  </td><td>kan  </td><td>jeg  </td><td>jeg  </td><td>minne</td><td>om  </td><td>at   </td><td>det  </td><td>var  </td><td>sosialdemokra</td><td>##tene</td><td>ør   </td><td>ør   </td><td>##jan</td><td>person</td><td>person</td><td>som  </td><td>sørget</td><td>for  </td><td>at   </td><td>den  </td><td>ble </td><td>fjernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.058</td><td>0.506    </td><td>0.665    </td><td>0.321    </td><td>0.573    </td><td>0.529</td><td>0.589</td><td>0.148</td><td>0.546</td><td>0.053</td><td>0.02</td><td>0.017</td><td>0.017</td><td>0.021</td><td>0.041        </td><td>0.349 </td><td>0.599</td><td>0.611</td><td>0.691</td><td>0.641 </td><td>0.643 </td><td>0.031</td><td>0.014 </td><td>0.005</td><td>0.005</td><td>0.016</td><td>0.01</td><td>0.01   </td><td>0.002</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['regjering'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'substitute', 'ref': ['kom', 'i', 'regjering'], 'ref_pos': ['VERB', 'ADP', 'NOUN'], 'hyp': ['president', 'det', 'kan'], 'hyp_pos': ['NOUN', 'PRON', 'AUX']}\n",
      "{'type': 'delete', 'ref': ['vil'], 'ref_pos': ['AUX'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['sosialdemokraten', 'göran', 'persson'], 'ref_pos': ['NOUN', 'PROPN', 'PROPN'], 'hyp': ['sosialdemokratene', 'jøran', 'person'], 'hyp_pos': ['NOUN', 'PROPN', 'NOUN']}\n",
      "{'type': 'substitute', 'ref': ['sørget'], 'ref_pos': ['VERB'], 'hyp': ['sørgeforat'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'delete', 'ref': ['for', 'at'], 'ref_pos': ['SCONJ', 'SCONJ'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.52\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>kom      </td><td>i        </td><td>regjering</td><td>regjering</td><td>jeg  </td><td>jeg  </td><td>jeg  </td><td>vil  </td><td>minne</td><td>om   </td><td>at   </td><td>det  </td><td>var  </td><td>sosialdemokra</td><td>##ten </td><td>[UNK]</td><td>[UNK]</td><td>pers  </td><td>##son </td><td>som  </td><td>sørget</td><td>for  </td><td>at   </td><td>den  </td><td>ble  </td><td>fjernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>regjering</td><td>regjering</td><td>regjering</td><td>president</td><td>det  </td><td>kan  </td><td>jeg  </td><td>jeg  </td><td>minne</td><td>om   </td><td>at   </td><td>det  </td><td>var  </td><td>sosialdemokra</td><td>##tene</td><td>jø   </td><td>##ran</td><td>person</td><td>person</td><td>som  </td><td>sørge </td><td>##for</td><td>##at </td><td>den  </td><td>ble  </td><td>fjernet</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.066</td><td>0.517    </td><td>0.671    </td><td>0.333    </td><td>0.573    </td><td>0.525</td><td>0.588</td><td>0.145</td><td>0.552</td><td>0.053</td><td>0.021</td><td>0.019</td><td>0.028</td><td>0.032</td><td>0.044        </td><td>0.357 </td><td>0.605</td><td>0.654</td><td>0.631 </td><td>0.651 </td><td>0.122</td><td>0.307 </td><td>0.361</td><td>0.394</td><td>0.049</td><td>0.027</td><td>0.02   </td><td>0.003</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_53\n",
      "ref: havaristedet men det som folk stiller seg spørsmål om det er jo hvorfor denne denne dette flyet altså sjøflyet i\n",
      "-----\n",
      "orig hyp: istedet men det som folk spiller seg spørsmål om det er jo hvorfor denne denne dette flyet altså sjøflyet i\n",
      "orig_wer: 0.1 orig_asd 0.1919381762099126\n",
      "-----\n",
      "mod hyp: var istedet men det som folk spiller seg spørsmålet om det er jo hvorfor denne denne dette flyet altså sjøflyet i går\n",
      "mod_wer: 0.25 mod_asd: 0.2934181994566945\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['havaristedet'], 'ref_pos': ['NOUN'], 'hyp': ['istedet'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['stiller'], 'ref_pos': ['VERB'], 'hyp': ['spiller'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.19\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>havar  </td><td>##isted</td><td>##et   </td><td>men  </td><td>det  </td><td>som  </td><td>folk</td><td>stiller</td><td>seg  </td><td>spørsmål</td><td>om   </td><td>det  </td><td>er   </td><td>jo   </td><td>hvorfor</td><td>denne</td><td>denne</td><td>dette</td><td>flyet</td><td>altså</td><td>sjø  </td><td>##flyet</td><td>i    </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>istedet</td><td>istedet</td><td>istedet</td><td>men  </td><td>det  </td><td>som  </td><td>folk</td><td>spiller</td><td>seg  </td><td>spørsmål</td><td>om   </td><td>det  </td><td>er   </td><td>jo   </td><td>hvorfor</td><td>denne</td><td>denne</td><td>dette</td><td>flyet</td><td>altså</td><td>sjø  </td><td>##flyet</td><td>i    </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.036</td><td>0.587  </td><td>0.711  </td><td>0.69   </td><td>0.068</td><td>0.025</td><td>0.026</td><td>0.04</td><td>0.485  </td><td>0.078</td><td>0.063   </td><td>0.027</td><td>0.023</td><td>0.017</td><td>0.019</td><td>0.014  </td><td>0.019</td><td>0.023</td><td>0.023</td><td>0.032</td><td>0.021</td><td>0.026</td><td>0.03   </td><td>0.026</td><td>0.006</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['var'], 'hyp_pos': ['AUX']}\n",
      "{'type': 'substitute', 'ref': ['havaristedet'], 'ref_pos': ['NOUN'], 'hyp': ['istedet'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'substitute', 'ref': ['stiller'], 'ref_pos': ['VERB'], 'hyp': ['spiller'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['spørsmål'], 'ref_pos': ['NOUN'], 'hyp': ['spørsmålet'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['går'], 'hyp_pos': ['NOUN']}\n",
      "ASD-NorBERT ref len: 25, ASD=0.29\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>havar</td><td>##isted</td><td>##et   </td><td>men  </td><td>det  </td><td>som  </td><td>folk </td><td>stiller</td><td>seg  </td><td>spørsmål  </td><td>om   </td><td>det  </td><td>er   </td><td>jo   </td><td>hvorfor</td><td>denne</td><td>denne</td><td>dette</td><td>flyet</td><td>altså</td><td>sjø  </td><td>##flyet</td><td>i    </td><td>i    </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>var  </td><td>istedet</td><td>istedet</td><td>men  </td><td>det  </td><td>som  </td><td>folk </td><td>spiller</td><td>seg  </td><td>spørsmålet</td><td>om   </td><td>det  </td><td>er   </td><td>jo   </td><td>hvorfor</td><td>denne</td><td>denne</td><td>dette</td><td>flyet</td><td>altså</td><td>sjø  </td><td>##flyet</td><td>i    </td><td>går  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.098</td><td>0.514</td><td>0.677  </td><td>0.684  </td><td>0.076</td><td>0.048</td><td>0.052</td><td>0.056</td><td>0.488  </td><td>0.116</td><td>0.206     </td><td>0.087</td><td>0.047</td><td>0.062</td><td>0.028</td><td>0.026  </td><td>0.028</td><td>0.03 </td><td>0.027</td><td>0.03 </td><td>0.037</td><td>0.028</td><td>0.037  </td><td>0.236</td><td>0.647</td><td>0.02 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: N2_030505_NRK_D12_NO_cut_25\n",
      "ref: at fremskrittspartiet endrer standpunkt det nye nå er at de har to standpunkter samtidig vanligvis så hadde de hvert fall\n",
      "-----\n",
      "orig hyp: at fremskrittspartiet endrer standpunkt det nye nå er at vi har to standpunkter samtidig vanligvis hadde et al\n",
      "orig_wer: 0.25 orig_asd 0.250790237193693\n",
      "-----\n",
      "mod hyp: at fremskrittspartiet endrer standpunkt det nye nå er ikke ha to standpunkter samtidig vanligvis hadde her\n",
      "mod_wer: 0.35 mod_asd: 0.3503774609673085\n",
      "---Orig Error---\n",
      "{'type': 'substitute', 'ref': ['de'], 'ref_pos': ['PRON'], 'hyp': ['vi'], 'hyp_pos': ['PRON']}\n",
      "{'type': 'delete', 'ref': ['så'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['de', 'hvert'], 'ref_pos': ['PRON', 'DET'], 'hyp': ['et', 'al'], 'hyp_pos': ['DET', 'NOUN']}\n",
      "{'type': 'delete', 'ref': ['fall'], 'ref_pos': ['NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.25\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at   </td><td>fremskritt</td><td>##spartiet</td><td>endrer</td><td>standpunkt</td><td>det </td><td>nye  </td><td>nå   </td><td>er   </td><td>at   </td><td>de   </td><td>har  </td><td>to   </td><td>standpunkt</td><td>##er </td><td>samtidig</td><td>vanligvis</td><td>så       </td><td>hadde</td><td>de   </td><td>hvert</td><td>fall</td><td>fall </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at   </td><td>fremskritt</td><td>##spartiet</td><td>endrer</td><td>standpunkt</td><td>det </td><td>nye  </td><td>nå   </td><td>er   </td><td>at   </td><td>vi   </td><td>har  </td><td>to   </td><td>standpunkt</td><td>##er </td><td>samtidig</td><td>vanligvis</td><td>vanligvis</td><td>hadde</td><td>et   </td><td>et   </td><td>et  </td><td>al   </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.084</td><td>0.014</td><td>0.005     </td><td>0.007     </td><td>0.005 </td><td>0.003     </td><td>0.01</td><td>0.004</td><td>0.006</td><td>0.008</td><td>0.016</td><td>0.408</td><td>0.031</td><td>0.013</td><td>0.007     </td><td>0.016</td><td>0.058   </td><td>0.136    </td><td>0.688    </td><td>0.179</td><td>0.583</td><td>0.699</td><td>0.75</td><td>0.753</td><td>0.013</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['at', 'de'], 'ref_pos': ['SCONJ', 'PRON'], 'hyp': ['ikke', 'ha'], 'hyp_pos': ['PART', 'VERB']}\n",
      "{'type': 'delete', 'ref': ['har'], 'ref_pos': ['VERB'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'delete', 'ref': ['så'], 'ref_pos': ['ADV'], 'hyp': [], 'hyp_pos': []}\n",
      "{'type': 'substitute', 'ref': ['de'], 'ref_pos': ['PRON'], 'hyp': ['her'], 'hyp_pos': ['ADV']}\n",
      "{'type': 'delete', 'ref': ['hvert', 'fall'], 'ref_pos': ['DET', 'NOUN'], 'hyp': [], 'hyp_pos': []}\n",
      "ASD-NorBERT ref len: 24, ASD=0.35\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>at  </td><td>fremskritt</td><td>##spartiet</td><td>endrer</td><td>standpunkt</td><td>det  </td><td>nye  </td><td>nå   </td><td>er   </td><td>at   </td><td>de   </td><td>de   </td><td>har</td><td>to   </td><td>standpunkt</td><td>##er </td><td>samtidig</td><td>vanligvis</td><td>så       </td><td>hadde</td><td>de   </td><td>de   </td><td>hvert</td><td>fall </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>at  </td><td>fremskritt</td><td>##spartiet</td><td>endrer</td><td>standpunkt</td><td>det  </td><td>nye  </td><td>nå   </td><td>er   </td><td>er   </td><td>er   </td><td>ikke </td><td>ha </td><td>to   </td><td>standpunkt</td><td>##er </td><td>samtidig</td><td>vanligvis</td><td>vanligvis</td><td>hadde</td><td>hadde</td><td>her  </td><td>her  </td><td>her  </td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.077</td><td>0.03</td><td>0.007     </td><td>0.01      </td><td>0.013 </td><td>0.008     </td><td>0.046</td><td>0.041</td><td>0.042</td><td>0.214</td><td>0.589</td><td>0.636</td><td>0.707</td><td>0.4</td><td>0.031</td><td>0.017     </td><td>0.031</td><td>0.058   </td><td>0.149    </td><td>0.704    </td><td>0.192</td><td>0.687</td><td>0.755</td><td>0.849</td><td>0.797</td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n",
      "segment id: 2021H264-fullStorting0510Stortinget-20210510-115459-Sak1_cut_176\n",
      "ref: periode før jeg kom på stortinget og jeg synes ikke synd på noen av de kommunene to av kommunene er\n",
      "-----\n",
      "orig hyp: periode før jeg kom på stortinget og jeg synes ikke synd på noen av de kommunene to av de kommunene arbeide\n",
      "orig_wer: 0.1 orig_asd 0.0922402564304996\n",
      "-----\n",
      "mod hyp: periode før jeg kom på stortinget og jeg syns ikke syn på noen av de kommunene to av de kommunene arbeider\n",
      "mod_wer: 0.2 mod_asd: 0.1918214716633738\n",
      "---Orig Error---\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['de'], 'hyp_pos': ['DET']}\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['arbeide'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.09\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>periode</td><td>før  </td><td>jeg  </td><td>kom  </td><td>på   </td><td>storting</td><td>##et </td><td>og   </td><td>jeg  </td><td>synes</td><td>ikke </td><td>synd </td><td>på   </td><td>noen </td><td>av   </td><td>de   </td><td>kommunene</td><td>to   </td><td>av   </td><td>av   </td><td>kommunene</td><td>er     </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>periode</td><td>før  </td><td>jeg  </td><td>kom  </td><td>på   </td><td>storting</td><td>##et </td><td>og   </td><td>jeg  </td><td>synes</td><td>ikke </td><td>synd </td><td>på   </td><td>noen </td><td>av   </td><td>de   </td><td>kommunene</td><td>to   </td><td>av   </td><td>de   </td><td>kommunene</td><td>arbeide</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.092</td><td>0.009  </td><td>0.003</td><td>0.003</td><td>0.003</td><td>0.002</td><td>0.003   </td><td>0.002</td><td>0.004</td><td>0.004</td><td>0.005</td><td>0.004</td><td>0.008</td><td>0.008</td><td>0.008</td><td>0.012</td><td>0.026</td><td>0.025    </td><td>0.041</td><td>0.081</td><td>0.508</td><td>0.078    </td><td>0.416  </td><td>0.017</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Mod Error---\n",
      "{'type': 'substitute', 'ref': ['synes'], 'ref_pos': ['VERB'], 'hyp': ['syns'], 'hyp_pos': ['VERB']}\n",
      "{'type': 'substitute', 'ref': ['synd'], 'ref_pos': ['NOUN'], 'hyp': ['syn'], 'hyp_pos': ['NOUN']}\n",
      "{'type': 'insert', 'ref': [], 'ref_pos': [], 'hyp': ['de'], 'hyp_pos': ['DET']}\n",
      "{'type': 'substitute', 'ref': ['er'], 'ref_pos': ['AUX'], 'hyp': ['arbeider'], 'hyp_pos': ['VERB']}\n",
      "ASD-NorBERT ref len: 23, ASD=0.19\n",
      "Token alignment table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>REF:    </td><td>[CLS]</td><td>periode</td><td>før  </td><td>jeg  </td><td>kom  </td><td>på   </td><td>storting</td><td>##et </td><td>og   </td><td>jeg  </td><td>synes</td><td>ikke </td><td>synd </td><td>på   </td><td>noen </td><td>av   </td><td>de   </td><td>kommunene</td><td>to   </td><td>av   </td><td>av   </td><td>kommunene</td><td>er      </td><td>[SEP]</td></tr>\n",
       "<tr><td>HYP:    </td><td>[CLS]</td><td>periode</td><td>før  </td><td>jeg  </td><td>kom  </td><td>på   </td><td>storting</td><td>##et </td><td>og   </td><td>jeg  </td><td>syns </td><td>ikke </td><td>syn  </td><td>på   </td><td>noen </td><td>av   </td><td>de   </td><td>kommunene</td><td>to   </td><td>av   </td><td>de   </td><td>kommunene</td><td>arbeider</td><td>[SEP]</td></tr>\n",
       "<tr><td>CosDist:</td><td>0.046</td><td>0.009  </td><td>0.004</td><td>0.004</td><td>0.004</td><td>0.003</td><td>0.004   </td><td>0.003</td><td>0.013</td><td>0.019</td><td>0.275</td><td>0.106</td><td>0.663</td><td>0.134</td><td>0.027</td><td>0.025</td><td>0.027</td><td>0.018    </td><td>0.026</td><td>0.075</td><td>0.498</td><td>0.063    </td><td>0.416   </td><td>0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "num_utterances = 20\n",
    "\n",
    "for row in higher_asd_df.itertuples():\n",
    "\n",
    "    print(\"segment id:\", row.segment_id)\n",
    "    print(\"ref:\", row.orig_ref)\n",
    "    print(\"-----\")\n",
    "    print(\"orig hyp:\", row.orig_asr)\n",
    "    print(\"orig_wer:\", row.orig_wer, \"orig_asd\", row.orig_asd)\n",
    "    print(\"-----\")\n",
    "    print(\"mod hyp:\", row.mod_asr)\n",
    "    print(\"mod_wer:\", row.mod_wer, \"mod_asd:\", row.mod_asd)\n",
    "\n",
    "    print(\"---Orig Error---\")\n",
    "    for item in row.orig_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.orig_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.orig_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"---Mod Error---\")\n",
    "    for item in row.mod_error_dict:\n",
    "        print(item)\n",
    "    tokenized_ref = metric_tokenizer(row.mod_ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = metric_tokenizer(row.mod_asr, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "    asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, asd_score = get_asd_alignment(tokenized_ref, tokenized_hyp, metric_model)\n",
    "    print(f\"ASD-NorBERT ref len: {len(ref_embedding_sequence)}, ASD={asd_score:.2f}\")\n",
    "    print_token_alignment(metric_tokenizer, asd_alignment, ref_embedding_sequence, hyp_embedding_sequence, ref_input_ids, hyp_input_ids)\n",
    "\n",
    "    print(\"===================\\n\\n\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter > num_utterances - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  word  lemma\n",
      "0       -abel           -abel\n",
      "1       -abels          -abel\n",
      "2       -abelt          -abel\n",
      "3       -abelts         -abel\n",
      "4       -able           -abel\n",
      "...       ...             ...\n",
      "744437  jordskokkpuré   -    \n",
      "744438  rotgrønnsakene  -    \n",
      "744439  albóndigas      -    \n",
      "744440  gelatinplatene  -    \n",
      "744441  kyllinglårene   -    \n",
      "\n",
      "[744442 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/talebase/data/lex/Sprakbanken/NLB/nlb_nob_20181129.lex\"\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines:\n",
    "        lex_list = line.split(\"\\t\")\n",
    "        lemma = lex_list[13].split(\":\")[1].split(\"|\")[0]\n",
    "        word_list.append(lex_list[0])\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "lex_dict = {\"word\":word_list, \"lemma\":lemma_list}\n",
    "lex_df = pd.DataFrame(lex_dict)\n",
    "print(lex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origloss_NBTale = pd.read_csv(\"./logs/OrigLoss_V5/NBTale_results_wav2vec2_NO_origLossV5_2023-09-18.csv\")\n",
    "# origloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "# origloss_NBTale.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/OrigLoss_V5/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "origloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_origloss\", \"wer\":\"wer_origloss\", \"asd\":\"asd_origloss\"}, inplace=True)\n",
    "    origloss_dfs.append(df)\n",
    "\n",
    "origloss = pd.concat(origloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customloss_NBTale = pd.read_csv(\"./logs/CustomLoss_V7/NBTale_results_wav2vec2_NO_customLossV7_2023-10-02.csv\")\n",
    "# customloss_NBTale.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "# customloss_NBTale.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./logs/CustomLoss_V7/\"\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "customloss_dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop(columns=[\"Unnamed: 0\", \"wav_file\", \"path\", \"text\", \"segment_id\"], inplace=True)\n",
    "    df.rename(columns={\"asr_str\": \"asr_customloss\", \"wer\":\"wer_customloss\", \"asd\":\"asd_customloss\"}, inplace=True)\n",
    "    customloss_dfs.append(df)\n",
    "\n",
    "customloss = pd.concat(customloss_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged results of orig loss & custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = pd.merge(origloss, customloss, on=\"ref_str\")\n",
    "merged_results = merged_results.reindex(columns=[\"segment_id\", \"ref_str\", \"asr_origloss\", \"wer_origloss\", \"asd_origloss\", \"asr_customloss\", \"wer_customloss\", \"asd_customloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer = merged_results[merged_results[\"wer_origloss\"] > merged_results[\"wer_customloss\"]]\n",
    "worse_wer = merged_results[merged_results[\"wer_origloss\"] < merged_results[\"wer_customloss\"]]\n",
    "better_asd = merged_results[merged_results[\"asd_origloss\"] > merged_results[\"asd_customloss\"]]\n",
    "worse_asd = merged_results[merged_results[\"asd_origloss\"] < merged_results[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better WER but *worse* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_wer[better_wer[\"asd_origloss\"] < better_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worse WER but *better* ASD when using custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_wer[worse_wer[\"asd_origloss\"] > worse_wer[\"asd_customloss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_improved_percent = (len(better_wer)/len(merged_results)) * 100\n",
    "asd_improved_percent = (len(better_asd)/len(merged_results)) * 100\n",
    "\n",
    "wer_worsened_percent = (len(worse_wer)/len(merged_results)) * 100\n",
    "asd_worsened_percent = (len(worse_asd)/len(merged_results)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WER improved: {wer_improved_percent:.2f}%\")\n",
    "print(f\"ASD improved: {asd_improved_percent:.2f}%\")\n",
    "\n",
    "print(f\"WER worsened: {wer_worsened_percent:.2f}%\")\n",
    "print(f\"ASD worsened: {asd_worsened_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC LOSS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3535249/3756479965.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, \"Container\"):\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "import types\n",
    "from tabulate import tabulate\n",
    "from dtw import *\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'snapshot_download': allow_regex. Will not be supported from version '0.12'.\n",
      "\n",
      "Please use `allow_patterns` and `ignore_patterns` instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 1868.49it/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(model_name)\n",
    "processor_woLM = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank: 31\n",
      "reduction: mean\n",
      "zero inf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"blank:\", model.config.pad_token_id)\n",
    "print(\"reduction:\", model.config.ctc_loss_reduction)\n",
    "print(\"zero inf:\", model.config.ctc_zero_infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.04, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.047, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.047, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.055, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.047, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 |\n",
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n",
      "27 å\n",
      "28 æ\n",
      "29 ø\n",
      "30 [UNK]\n",
      "31 [PAD]\n",
      "32 <s>\n",
      "33 </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1688 [00:00<?, ?ex/s]\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   3%|▎         | 50/1688 [00:00<00:03, 494.87ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:   8%|▊         | 140/1688 [00:00<00:02, 729.49ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  15%|█▌        | 256/1688 [00:00<00:01, 923.91ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  22%|██▏       | 374/1688 [00:00<00:01, 1022.37ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  29%|██▉       | 492/1688 [00:00<00:01, 1078.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  36%|███▌      | 607/1688 [00:00<00:00, 1102.70ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  43%|████▎     | 725/1688 [00:00<00:00, 1125.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|████▉     | 841/1688 [00:00<00:00, 1134.06ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  57%|█████▋    | 959/1688 [00:00<00:00, 1147.61ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  64%|██████▎   | 1074/1688 [00:01<00:01, 565.97ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  72%|███████▏  | 1209/1688 [00:01<00:00, 705.40ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|███████▉  | 1343/1688 [00:01<00:00, 834.04ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  87%|████████▋ | 1476/1688 [00:01<00:00, 945.22ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 1688/1688 [00:01<00:00, 974.09ex/s] \n",
      "#3: 100%|██████████| 1687/1687 [00:01<00:00, 973.48ex/s] \n",
      "\n",
      "#2: 100%|██████████| 1687/1687 [00:01<00:00, 960.13ex/s] \n",
      "#0: 100%|██████████| 1688/1688 [00:01<00:00, 937.21ex/s] \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../model_ckpts/fine-tuning_wav2vec2_expectedASD2_3ep_0p01_Rundkast_aulus6/runs/train_set.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.cast_column(\"path\", Audio())\n",
    "train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:584: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_feature = [{\"input_values\": train_dataset[73][\"input_values\"]}]\n",
    "label_feature = [{\"input_ids\": train_dataset[73][\"labels\"]}]\n",
    "\n",
    "# input_feature = [{\"input_values\": input_values} for input_values in train_dataset[34:41][\"input_values\"]]\n",
    "# label_feature = [{\"input_ids\": labels} for labels in train_dataset[34:41][\"labels\"]]\n",
    "\n",
    "batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with processor.as_target_processor():\n",
    "    label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "batch[\"labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch[\"labels\"][0]\n",
    "labels_mask = labels >= 0\n",
    "flattened_targets = labels.masked_select(labels_mask)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 15, 14, 20, 18, 15, 12, 12,  0, 13,  5,  4,  0, 18,  5,  9, 19,  5,\n",
       "         4,  9,  5, 20, 20,  0, 15,  7,  0, 18,  5, 16, 18,  5, 19,  5, 14, 20,\n",
       "         1, 19, 10, 15, 14, 19, 18,  5,  7, 14,  9, 14,  7,  5, 18,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166, 34])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_output.pkl', 'wb') as file:\n",
    "    pickle.dump(output, file)\n",
    "\n",
    "with open('sample_input.pkl', 'wb') as file:\n",
    "    pickle.dump(batch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asd_score_single_utt(model, tokenizer, reference, hypothesis):\n",
    "    ref_text = reference.replace(\"[UNK]\", \"\")  # removes the [UNK] token in the reference text, observed during training\n",
    "    hyp_text = hypothesis.replace(\"[UNK]\", \"\")\n",
    "    tokenized_ref = tokenizer(ref_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = tokenizer(hyp_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = [hidden_states_ref[1].squeeze(), hidden_states_ref[2].squeeze(), hidden_states_ref[3].squeeze(), hidden_states_ref[4].squeeze(),\n",
    "                            hidden_states_ref[5].squeeze(), hidden_states_ref[6].squeeze(), hidden_states_ref[7].squeeze(), hidden_states_ref[8].squeeze(),\n",
    "                            hidden_states_ref[9].squeeze(), hidden_states_ref[10].squeeze(), hidden_states_ref[11].squeeze(), hidden_states_ref[12].squeeze()]\n",
    "    all_layers_hypothesis = [hidden_states_hyp[1].squeeze(), hidden_states_hyp[2].squeeze(), hidden_states_hyp[3].squeeze(), hidden_states_hyp[4].squeeze(),\n",
    "                                hidden_states_hyp[5].squeeze(), hidden_states_hyp[6].squeeze(), hidden_states_hyp[7].squeeze(), hidden_states_hyp[8].squeeze(),\n",
    "                                hidden_states_hyp[9].squeeze(), hidden_states_hyp[10].squeeze(), hidden_states_hyp[11].squeeze(), hidden_states_hyp[12].squeeze()]\n",
    "    output_mean_reference = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    output_mean_hypothesis = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "    alignment = dtw(output_mean_hypothesis, output_mean_reference, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(output_mean_reference)\n",
    "    asd_score = alignment.distance / num_tokens\n",
    "    return asd_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import cuda_ctc_decoder, ctc_decoder, download_pretrained_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model.wav2vec2.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'å', 'æ', 'ø', '[UNK]', '[PAD]', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = [None] * 34\n",
    "\n",
    "for key in processor.tokenizer.vocab:\n",
    "    tokens[processor.tokenizer.vocab[key]] = key\n",
    "tokens[32] = \"</s>\"\n",
    "tokens[33] = \"<s>\"\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# f = open(\"tokens.txt\", \"w\")\n",
    "# for i, item in enumerate(tokens):\n",
    "#     if i == (len(tokens) - 1):\n",
    "#         f.write(item)\n",
    "#     else:\n",
    "#         f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 34])\n",
      "torch.Size([1, 34])\n",
      "tensor([[ 5, 18,  0, 22,  9,  0, 19, 20,  1,  4,  9,  7,  0, 14,  5,  4,  5,  0,\n",
      "         15,  7,  0, 11, 21, 18, 19,  5, 18,  0,  4,  9, 19, 19,  5,  0]])\n",
      "['er vi stadig nede og kurser dise']\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# output = model(**batch)\n",
    "# logits = output[\"logits\"]\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# print(\"LABEL:\", processor_woLM.batch_decode(batch[\"labels\"]), \"\\n\")\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "# log_probs = F.log_softmax(logits[0], dim=-1, dtype=torch.float32).to(device)\n",
    "# encoder_out_lens = torch.tensor(log_probs.shape[0], device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "# cuda_decoder = cuda_ctc_decoder(tokens, nbest=10, beam_size=10, blank_skip_threshold=0.95)\n",
    "# results = cuda_decoder(log_probs, encoder_out_lens)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: ikke minst er det viktig i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig\n",
      "idx: 6 \thyp:  ikke minst er det viktige i næringer som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n",
      "idx: 0 \thyp:  ikke minst er det viktige næringe som går gjennom krevende tider og når det å investere i kompetanseheving kan virke vanskelig \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "# print(batch[\"labels\"].shape)\n",
    "# print(batch[\"labels\"])\n",
    "# characters = model_tokenizer.convert_ids_to_tokens(batch[\"labels\"][0])\n",
    "# ref_text = re.sub(\" +\", \" \", \"\".join(characters).replace(\"|\", \" \"))\n",
    "# print(\"LABEL:\", ref_text, \"\\n\")\n",
    "\n",
    "# lm=\"./language_model/\"\n",
    "decoder = ctc_decoder(lexicon=None, tokens=\"tokens.txt\", nbest=10, beam_size=50, blank_token=\"[PAD]\",\n",
    "                      sil_token=\"|\", unk_word=\"[UNK]\")\n",
    "results = decoder(logits)\n",
    "\n",
    "asd_score_list = [0] * len(results[0])\n",
    "beam_score_list = [0] * len(results[0])\n",
    "hyp_list = []\n",
    "for i, item in enumerate(results[0]):\n",
    "    # print(item.tokens.shape, item.tokens)\n",
    "    chars = decoder.idxs_to_tokens(item.tokens)\n",
    "    hyp_text = re.sub(\" +\", \" \", \"\".join(chars).replace(\"|\", \" \"))\n",
    "    hyp_list.append(hyp_text)\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    asd_score_list[i] = asd_score\n",
    "    beam_score_list[i] = item.score\n",
    "\n",
    "lowest_asd_idx = np.argmin(asd_score_list)\n",
    "highest_beam_score_idx = np.argmax(beam_score_list)\n",
    "\n",
    "print(\"idx:\", lowest_asd_idx, \"\\thyp:\", hyp_list[lowest_asd_idx])\n",
    "print(\"idx:\", highest_beam_score_idx, \"\\thyp:\", hyp_list[highest_beam_score_idx])\n",
    "\n",
    "nbest_loss = torch.tensor([beam_score_list[lowest_asd_idx]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4052.2563], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=nbest_loss, inputs=logits, grad_outputs=nbest_loss, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of log_prob: torch.Size([1, 175, 500]), the shape of encoder_out_lens: torch.Size([1])\n",
      "tensor([175], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "from torchaudio.models.decoder import cuda_ctc_decoder\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio\n",
    "\n",
    "def download_asset_external(url, key):\n",
    "    path = Path(torch.hub.get_dir()) / \"torchaudio\" / Path(key)\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.hub.download_url_to_file(url, path)\n",
    "    return str(path)\n",
    "\n",
    "url_prefix = \"https://huggingface.co/Zengwei/icefall-asr-librispeech-pruned-transducer-stateless7-ctc-2022-12-01\"\n",
    "model_link = f\"{url_prefix}/resolve/main/exp/cpu_jit.pt\"\n",
    "model_path = download_asset_external(model_link, \"cuda_ctc_decoder/cpu_jit.pt\")\n",
    "\n",
    "speech_file = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n",
    "waveform, sample_rate = torchaudio.load(speech_file)\n",
    "\n",
    "actual_transcript = \"i really was very much afraid of showing him how much shocked i was at some parts of what he said\"\n",
    "actual_transcript = actual_transcript.split()\n",
    "\n",
    "device = torch.device(\"cuda\", 0)\n",
    "acoustic_model = torch.jit.load(model_path)\n",
    "acoustic_model.to(device)\n",
    "acoustic_model.eval()\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "feat = torchaudio.compliance.kaldi.fbank(waveform, num_mel_bins=80, snip_edges=False)\n",
    "feat = feat.unsqueeze(0)\n",
    "feat_lens = torch.tensor(feat.size(1), device=device).unsqueeze(0)\n",
    "\n",
    "encoder_out, encoder_out_lens = acoustic_model.encoder(feat, feat_lens)\n",
    "nnet_output = acoustic_model.ctc_output(encoder_out)\n",
    "log_prob = torch.nn.functional.log_softmax(nnet_output, -1)\n",
    "\n",
    "print(f\"The shape of log_prob: {log_prob.shape}, the shape of encoder_out_lens: {encoder_out_lens.shape}\")\n",
    "print(encoder_out_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimum ASD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnvrtc.so.12: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mk2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/wav2vec/lib/python3.9/site-packages/k2/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     k2_torch_cuda_version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m!=\u001b[39m k2_torch_cuda_version\n\u001b[1;32m     17\u001b[0m ):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk2 was built using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk2_torch_cuda_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut you are using CUDA \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to run it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeterminizeWeightPushingType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_ragged_index_select\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_k2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swoosh_l\n",
      "\u001b[0;31mImportError\u001b[0m: libnvrtc.so.12: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import k2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lattice(\n",
    "    nnet_output: torch.Tensor,\n",
    "    decoding_graph: k2.Fsa,\n",
    "    supervision_segments: torch.Tensor,\n",
    "    search_beam: float,\n",
    "    output_beam: float,\n",
    "    min_active_states: int,\n",
    "    max_active_states: int,\n",
    "    subsampling_factor: int = 1,\n",
    ") -> k2.Fsa:\n",
    "    \"\"\"Get the decoding lattice from a decoding graph and neural\n",
    "    network output.\n",
    "    Args:\n",
    "      nnet_output:\n",
    "        It is the output of a neural model of shape `(N, T, C)`.\n",
    "      decoding_graph:\n",
    "        An Fsa, the decoding graph. It can be either an HLG\n",
    "        (see `compile_HLG.py`) or an H (see `k2.ctc_topo`).\n",
    "      supervision_segments:\n",
    "        A 2-D **CPU** tensor of dtype `torch.int32` with 3 columns.\n",
    "        Each row contains information for a supervision segment. Column 0\n",
    "        is the `sequence_index` indicating which sequence this segment\n",
    "        comes from; column 1 specifies the `start_frame` of this segment\n",
    "        within the sequence; column 2 contains the `duration` of this\n",
    "        segment.\n",
    "      search_beam:\n",
    "        Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "        (less pruning). This is the default value; it may be modified by\n",
    "        `min_active_states` and `max_active_states`.\n",
    "      output_beam:\n",
    "         Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "         to best path of output.\n",
    "      min_active_states:\n",
    "        Minimum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to have fewer than this number active.\n",
    "        Set it to zero if there is no constraint.\n",
    "      max_active_states:\n",
    "        Maximum number of FSA states that are allowed to be active on any given\n",
    "        frame for any given intersection/composition task. This is advisory,\n",
    "        in that it will try not to exceed that but may not always succeed.\n",
    "        You can use a very large number if no constraint is needed.\n",
    "      subsampling_factor:\n",
    "        The subsampling factor of the model.\n",
    "    Returns:\n",
    "      An FsaVec containing the decoding result. It has axes [utt][state][arc].\n",
    "    \"\"\"\n",
    "    dense_fsa_vec = k2.DenseFsaVec(\n",
    "        nnet_output,\n",
    "        supervision_segments,\n",
    "        allow_truncate=subsampling_factor - 1,\n",
    "    )\n",
    "\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        decoding_graph,\n",
    "        dense_fsa_vec,\n",
    "        search_beam=search_beam,\n",
    "        output_beam=output_beam,\n",
    "        min_active_states=min_active_states,\n",
    "        max_active_states=max_active_states,\n",
    "    )\n",
    "\n",
    "    return lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWERLoss(torch.nn.Module):\n",
    "    '''Minimum Word Error Rate Loss compuration in k2.\n",
    "\n",
    "    See equation 2 of https://arxiv.org/pdf/2106.02302.pdf about its definition.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        subsampling_factor: int,\n",
    "        search_beam: int = 20,\n",
    "        output_beam: int = 8,\n",
    "        min_active_states: int = 30,\n",
    "        max_active_states: int = 10000,\n",
    "        temperature: float = 1.0,\n",
    "        num_paths: int = 100,\n",
    "        use_double_scores: bool = True,\n",
    "        nbest_scale: float = 0.5,\n",
    "        reduction: Literal['none', 'mean', 'sum'] = 'sum'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          search_beam:\n",
    "            Decoding beam, e.g. 20.  Smaller is faster, larger is more exact\n",
    "            (less pruning). This is the default value; it may be modified by\n",
    "            `min_active_states` and `max_active_states`.\n",
    "          output_beam:\n",
    "             Beam to prune output, similar to lattice-beam in Kaldi.  Relative\n",
    "             to best path of output.\n",
    "          min_active_states:\n",
    "            Minimum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to have fewer than this number active.\n",
    "            Set it to zero if there is no constraint.\n",
    "          max_active_states:\n",
    "            Maximum number of FSA states that are allowed to be active on any given\n",
    "            frame for any given intersection/composition task. This is advisory,\n",
    "            in that it will try not to exceed that but may not always succeed.\n",
    "            You can use a very large number if no constraint is needed.\n",
    "          subsampling_factor:\n",
    "            The subsampling factor of the model.\n",
    "          temperature:\n",
    "            For long utterances, the dynamic range of scores will be too large\n",
    "            and the posteriors will be mostly 0 or 1.\n",
    "            To prevent this it might be a good idea to have an extra argument\n",
    "            that functions like a temperature.\n",
    "            We scale the logprobs by before doing the normalization.\n",
    "          use_double_scores:\n",
    "            True to use double precision floating point.\n",
    "            False to use single precision.\n",
    "          reduction:\n",
    "            Specifies the reduction to apply to the output:\n",
    "            'none' | 'sum' | 'mean'.\n",
    "            'none': no reduction will be applied.\n",
    "                    The returned 'loss' is a k2.RaggedTensor, with\n",
    "                    loss.tot_size(0) == batch_size.\n",
    "                    loss.tot_size(1) == total_num_paths_of_current_batch\n",
    "                    If you want the MWER loss for each utterance, just do:\n",
    "                    `loss_per_utt = loss.sum()`\n",
    "                    Then loss_per_utt.shape[0] should be batch_size.\n",
    "                    See more example usages in 'k2/python/tests/mwer_test.py'\n",
    "            'sum': sum loss of each path over the whole batch together.\n",
    "            'mean': divide above 'sum' by total num paths over the whole batch.\n",
    "          nbest_scale:\n",
    "            Scale `lattice.score` before passing it to :func:`k2.random_paths`.\n",
    "            A smaller value leads to more unique paths at the risk of being not\n",
    "            to sample the path with the best score.\n",
    "          num_paths:\n",
    "            Number of paths to **sample** from the lattice\n",
    "            using :func:`k2.random_paths`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.search_beam = search_beam\n",
    "        self.output_beam = output_beam\n",
    "        self.min_active_states = min_active_states\n",
    "        self.max_active_states = max_active_states\n",
    "\n",
    "        self.num_paths = num_paths\n",
    "        self.nbest_scale = nbest_scale\n",
    "        self.subsampling_factor = subsampling_factor\n",
    "\n",
    "        self.mwer_loss = k2.MWERLoss(\n",
    "            temperature=temperature,\n",
    "            use_double_scores=use_double_scores,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        emissions: torch.Tensor,\n",
    "        emissions_lengths: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        labels_length: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
    "                probability distribution over labels; output of acoustic model.\n",
    "            labels (torch.FloatTensor): CPU tensor of shape `(batch, label_len)` storing labels.\n",
    "            emissions_lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                in time axis of the output Tensor in each batch.\n",
    "            labels_length (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
    "                label in each batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor:\n",
    "                Minimum Word Error Rate loss.\n",
    "        \"\"\"\n",
    "        H = k2.ctc_topo(\n",
    "            max_token=self.vocab_size-1,\n",
    "            modified=False,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "\n",
    "        supervision_segments = torch.stack(\n",
    "            (\n",
    "                torch.tensor(range(emissions_lengths.shape[0])),\n",
    "                torch.zeros(emissions_lengths.shape[0]),\n",
    "                emissions_lengths.cpu(),\n",
    "            ),\n",
    "            1,\n",
    "        ).to(torch.int32)\n",
    "\n",
    "        lattice = get_lattice(\n",
    "            nnet_output=emissions,\n",
    "            decoding_graph=H,\n",
    "            supervision_segments=supervision_segments,\n",
    "            search_beam=self.search_beam,\n",
    "            output_beam=self.output_beam,\n",
    "            min_active_states=self.min_active_states,\n",
    "            max_active_states=self.max_active_states,\n",
    "            subsampling_factor=self.subsampling_factor,\n",
    "        )\n",
    "\n",
    "        token_ids = []\n",
    "        for i in range(labels_length.size(0)):\n",
    "            token_ids.append(labels[i, : labels_length[i]].cpu().tolist())\n",
    "\n",
    "        loss = self.mwer_loss(\n",
    "            lattice, token_ids,\n",
    "            nbest_scale=self.nbest_scale,\n",
    "            num_paths=self.num_paths\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Gumbel Softmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "asd_scores = [0] * 10\n",
    "for i in range(10):\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=10, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_scores[i] = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    print(\"asd:\", asd_scores[i], \"hyp:\", hyp_text)\n",
    "\n",
    "mean_asd = np.mean(asd_scores)\n",
    "loss[0] = mean_asd\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "def get_mass_prob(paths: Tensor,\n",
    "                        softmax_ctc: Tensor,\n",
    "                        model_pred_length: Tensor,\n",
    "                        eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    compute the path probability mass\n",
    "    :param paths: ctc alignments\n",
    "    :param softmax_ctc: model logits after softmax\n",
    "    :param model_pred_length:  max length of all given paths\n",
    "    :return: avg of the paths probability\n",
    "    \"\"\"\n",
    "    log_indexes_probs = softmax_ctc.gather(dim=1, index=paths) + eps\n",
    "    if len(log_indexes_probs) > model_pred_length:\n",
    "        log_indexes_probs[model_pred_length:, :] = torch.zeros((log_indexes_probs.shape[0] - model_pred_length, 0))\n",
    "    return torch.sum(log_indexes_probs, dim=0) / (model_pred_length.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: pågripelsen av toska gjorde altså denne pågripelsen mulig\n",
      "[0.037175211785556765, 0.037175211785556765, 0.1520948636867659, 0.24277221896598888]\n",
      "['pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsene av toska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåska gjorde altså denne pågripelsen mulig', 'pågripelsen av tåskha gjorde altså denne pågripelsen mulig']\n",
      "0\n",
      "tensor([0.2708, 0.1336, 0.1375], grad_fn=<CopySlices>)\n",
      "tensor(0.5420, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "output = model(**batch)\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "input_lengths = model._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(output[\"logits\"][0], dim=-1, dtype=torch.float32)\n",
    "\n",
    "loss = torch.zeros(1, requires_grad=True).double()\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "ref_text = label_str[0]\n",
    "print(\"REF:\", ref_text)\n",
    "\n",
    "num_samples = 10\n",
    "non_zero_logits = []\n",
    "non_zero_asd = []\n",
    "non_zero_hyp = []\n",
    "# for i in range(num_samples):\n",
    "while len(non_zero_asd) < 4:\n",
    "    sampled_logits = F.gumbel_softmax(logits, tau=100, hard=True, dim=-1)\n",
    "    tokens = torch.argmax(sampled_logits, dim=1)\n",
    "    hyp_text = processor.decode(sampled_logits.detach().numpy()).text\n",
    "    asd_score = compute_asd_score_single_utt(asd_model, asd_tokenizer, ref_text, hyp_text)\n",
    "    if asd_score != 0:\n",
    "        non_zero_logits.append(sampled_logits)\n",
    "        non_zero_asd.append(asd_score)\n",
    "        non_zero_hyp.append(hyp_text)\n",
    "\n",
    "print(non_zero_asd)\n",
    "print(non_zero_hyp)\n",
    "\n",
    "lowest_asd_idx = np.argmin(np.array(non_zero_asd))\n",
    "print(lowest_asd_idx)\n",
    "\n",
    "mass_prob_list = []\n",
    "for i in range(len(non_zero_asd)):\n",
    "    logits_selected = non_zero_logits[i].type(torch.LongTensor)\n",
    "    mass_prob_list.append(get_mass_prob(logits_selected, log_probs, input_lengths))\n",
    "\n",
    "loss = torch.zeros((len(mass_prob_list)-1))\n",
    "j=0\n",
    "for i in range(len(mass_prob_list)):\n",
    "    if i != lowest_asd_idx:\n",
    "        subtract_path_probs = mass_prob_list[i] - mass_prob_list[lowest_asd_idx]\n",
    "        loss[j] = torch.sum(torch.clamp((subtract_path_probs), min=0))\n",
    "        j += 1\n",
    "print(loss)\n",
    "print(torch.sum(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating aligned cosdist (1 batch, size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    ref_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "\n",
    "    return ref_alignments\n",
    "\n",
    "\n",
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed\n",
    "\n",
    "\n",
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                # if tokens_compressed[token_count][2] == 0:\n",
    "                #     print(\"OH NOOO\", tokens_compressed[token_count][2])\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    return cosdist_for_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdist_register = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    input_feature = [{\"input_values\": input_values} for input_values in train_dataset[i*8:(i+1)*8][\"input_values\"]]\n",
    "    label_feature = [{\"input_ids\": labels} for labels in train_dataset[i*8:(i+1)*8][\"labels\"]]\n",
    "\n",
    "    batch = processor.pad(input_feature, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label = processor.pad(label_feature, padding=True, return_tensors=\"pt\")\n",
    "    label = label[\"input_ids\"].masked_fill(label.attention_mask.ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = label\n",
    "\n",
    "    output = model(**batch)\n",
    "    output_logits = output[\"logits\"]\n",
    "    pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "\n",
    "    labels = batch[\"labels\"]\n",
    "    labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "    for i in range(len(pred_str)):\n",
    "        ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "        pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "        label_ids = labels[i]\n",
    "        labels_mask = label_ids >= 0\n",
    "        flattened_labels = label_ids.masked_select(labels_mask)\n",
    "        ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "        tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "        cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "        cosdist_register.append(cosdist_for_ctc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_976418/112888293.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cosdist_register_arr = np.asarray(cosdist_register)\n"
     ]
    }
   ],
   "source": [
    "cosdist_register_arr = np.asarray(cosdist_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(cosdist_register_arr.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(params, seq, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    numphones = params.shape[0]  # number of labels, not even used in the code\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    # print(seqLen, numphones, L, T)\n",
    "\n",
    "    alphas = np.zeros((L,T))\n",
    "    betas = np.zeros((L,T))\n",
    "\n",
    "    # convert logits to log softmax\n",
    "    params = params - np.max(params,axis=0)\n",
    "    params = np.exp(params)\n",
    "    params = params / np.sum(params,axis=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = np.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = np.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = np.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward += np.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = np.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = np.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = np.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward += np.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = np.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] += ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = np.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = np.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or np.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(np.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_tensor(logits, seq, cosdist_for_ctc, blank=0):\n",
    "\n",
    "    cosdist_contribution_alphas = []\n",
    "    cosdist_contribution_betas = []\n",
    "\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    # print(\"label seq:\", seqLen)\n",
    "    # print(\"label seq length with blanks:\", L)\n",
    "    # print(\"utterance length:\", T)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # print(0, L-2*(T-t), \"time:\", t, \"start:\", start, \"L:\", L, \"s:\", s, \"l:\", l)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "                cosdist_contribution_alphas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "                cosdist_contribution_betas.append(cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    # llDiff = torch.abs(llForward-llBackward)\n",
    "    # if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "    #     print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "    #     print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "    #     return (-llForward, grad)\n",
    "    # else:\n",
    "    #     grad = params - grad / (params * absum)\n",
    "    #     return (-llForward, grad)\n",
    "\n",
    "    for t in range(T):\n",
    "        for s in range(numphones):\n",
    "            tmp = (params[s,t]*absum[t])\n",
    "            if tmp > 0:\n",
    "                grad[s,t] = params[s,t] - grad[s,t] / tmp\n",
    "            else:\n",
    "                grad[s,t] = params[s,t]\n",
    "\n",
    "    return (-llForward, grad, sum(cosdist_contribution_alphas), sum(cosdist_contribution_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_newgrad(logits, seq, blank=0):\n",
    "    params = logits.transpose(1,0)\n",
    "\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "    numphones = params.shape[0]  # number of labels/characters in \"alphabet\"\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0] / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t]\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t] / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1] / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t]\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t]\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t] / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient according to https://github.com/yehudabab/NumpyCTC/blob/main/ctc.py\n",
    "    padded_labels = torch.zeros((L))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(L):\n",
    "        if i%2 == 0:\n",
    "            padded_labels[i] = 0\n",
    "        else:\n",
    "            padded_labels[i] = seq[j]\n",
    "            j += 1\n",
    "\n",
    "    # print(len(seq), seq)\n",
    "    # print(len(padded_labels), padded_labels)\n",
    "\n",
    "    grad = torch.zeros(params.shape)\n",
    "\n",
    "    score_last = alphas[L-1, T-1]\n",
    "    score_before_last = betas[L-2, T-1]\n",
    "    p_l_given_ctc = score_last + score_before_last\n",
    "\n",
    "    for t in range(T):\n",
    "        for k in range(numphones):\n",
    "            d_p_d_ytk = 0\n",
    "            lb_lk = np.nonzero(list(map(lambda x: 1 if k in x else 0, padded_labels)))[0]\n",
    "            for s in lb_lk:\n",
    "                d_p_d_ytk += alphas[s, t] * betas[s, t]\n",
    "\n",
    "            d_p_d_ytk /= (params[k, t] ** 2)\n",
    "            d_lnp_d_ytk = (1. / p_l_given_ctc) * d_p_d_ytk\n",
    "            grad[k, t] = d_lnp_d_ytk\n",
    "\n",
    "    return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Stanford CTC Loss script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].detach().cpu().numpy().transpose()\n",
    "seq = batch[\"labels\"][0].detach().cpu().numpy()\n",
    "\n",
    "loss, grad = ctc_loss(logits, seq.squeeze(), blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(output[\"logits\"][0], dim=1)\n",
    "print(token_ids)\n",
    "for token in token_ids:\n",
    "    if token == 33:\n",
    "        print(\"HUY\")\n",
    "\n",
    "decoded_tokens = []\n",
    "for token in token_ids:\n",
    "    decoded_tokens.append(model_tokenizer.decode(token))\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = output[\"logits\"][0]\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "targets = batch[\"labels\"][0]\n",
    "input_lengths = torch.tensor(output[\"logits\"][0].shape[0])\n",
    "target_lengths = torch.tensor(batch[\"labels\"][0].shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=targets, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[\"logits\"][0].transpose(1,0)\n",
    "seq = batch[\"labels\"][0]\n",
    "\n",
    "loss, grad = ctc_loss_tensor(logits, seq, blank=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad.shape)\n",
    "transposed_grad = grad.transpose(1,0)\n",
    "print(transposed_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    print(i, model_tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad.shape[0]):\n",
    "    print(i, grad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosdist alphas: 845.0103612840176\n",
      "cosdist betas: 972.7007383406162\n",
      "torch.Size([201, 34]) tensor(608.0905, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1854.4355, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 606.3643108587712\n",
      "cosdist betas: 550.9175247717649\n",
      "torch.Size([201, 34]) tensor(680.7924, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1999.4266, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 0\n",
      "cosdist betas: 0\n",
      "torch.Size([201, 34]) tensor(1113.2689, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 307.15736174583435\n",
      "cosdist betas: 318.57188880443573\n",
      "torch.Size([201, 34]) tensor(2163.9688, grad_fn=<NegBackward0>)\n",
      "cosdist alphas: 2056.844472726263\n",
      "cosdist betas: 1902.4644071857585\n",
      "torch.Size([201, 34]) tensor(860.0428, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_to_inspect = []\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    loss, grad, cosdist_contribution_alphas, cosdist_contribution_betas = ctc_loss_tensor(logits, flattened_labels, cosdist_for_ctc)\n",
    "    grad_to_inspect.append(grad)\n",
    "\n",
    "    print(\"cosdist alphas:\", cosdist_contribution_alphas)\n",
    "    print(\"cosdist betas:\", cosdist_contribution_betas)\n",
    "    print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for logits in output_logits:\n",
    "    token_ids = torch.argmax(logits, dim=1)\n",
    "    print(token_ids)\n",
    "\n",
    "    decoded_tokens = []\n",
    "    for token in token_ids:\n",
    "        decoded_tokens.append(model_tokenizer.decode(token))\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing only the first utterance\n",
    "model.train()\n",
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "logits = output_logits[0]\n",
    "label_ids = labels[0]\n",
    "labels_mask = label_ids >= 0\n",
    "flattened_labels = label_ids.masked_select(labels_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([235, 34])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "# python-implemented CTC loss\n",
    "loss, grad = ctc_loss_tensor(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6441, grad_fn=<NegBackward0>)\n",
      "torch.Size([34, 235])\n"
     ]
    }
   ],
   "source": [
    "loss, grad = ctc_loss_newgrad(logits, flattened_labels)\n",
    "print(loss)\n",
    "print(grad.shape)\n",
    "\n",
    "# for i, item in enumerate(grad):\n",
    "#     print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(532.6440, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pytorch CTC loss\n",
    "import torch.nn.functional as F\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = labels_mask.sum(-1)\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=flattened_labels, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.grad)\n",
    "#         print(param.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating ASD into CTC Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: start running here! (when loading from pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOV-8 DATA\n",
    "# ref = \"wozniackis drøm om seier knust av russiske jekaterina\"\n",
    "# hyp = \"asnake drøm om sier knut av russiske karine\"\n",
    "\n",
    "# logit_frames_decoded = pickle.load(open(\"logit_frames_decoded_8Nov.pkl\", \"rb\"))\n",
    "\n",
    "# alignment_table = [logit_frames_decoded]\n",
    "# table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "# print(\"logits decoded\")\n",
    "# display(HTML(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL FOR ASD METRIC\n",
    "modelname = 'ltg/norbert2'\n",
    "asd_model = BertModel.from_pretrained(modelname)\n",
    "asd_tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning ASD cosdist alignment to reference tokens\n",
    "\n",
    "def get_asd_align(ref, hyp, asd_model, asd_tokenizer):\n",
    "    tokenized_ref = asd_tokenizer(ref, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_hyp = asd_tokenizer(hyp, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ref_input_ids = tokenized_ref[\"input_ids\"].squeeze()\n",
    "    hyp_input_ids = tokenized_hyp[\"input_ids\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_ref = asd_model(**tokenized_ref, output_hidden_states=True)\n",
    "        model_output_hyp = asd_model(**tokenized_hyp, output_hidden_states=True)\n",
    "    hidden_states_ref = model_output_ref.hidden_states\n",
    "    hidden_states_hyp = model_output_hyp.hidden_states\n",
    "    all_layers_reference = []\n",
    "    all_layers_hypothesis = []\n",
    "    for i in range(1,13):\n",
    "        all_layers_reference.append(hidden_states_ref[i].squeeze())\n",
    "        all_layers_hypothesis.append(hidden_states_hyp[i].squeeze())\n",
    "    ref_embedding_sequence = torch.stack(all_layers_reference).mean(dim=0)\n",
    "    hyp_embedding_sequence = torch.stack(all_layers_hypothesis).mean(dim=0)\n",
    "\n",
    "    alignment = dtw(hyp_embedding_sequence, ref_embedding_sequence, dist_method=distance.cosine, keep_internals=True)\n",
    "    num_tokens = len(ref_embedding_sequence)\n",
    "    asd_score = (alignment.distance / num_tokens)\n",
    "\n",
    "    ref_alignment_idxs = alignment.index2\n",
    "    hyp_alignment_idxs = alignment.index1\n",
    "\n",
    "    cosdist_alignment_tokens = []  # FOR PRINTING\n",
    "    ref_alignments = []\n",
    "    # hyp_alignments = []\n",
    "    for i in range(len(ref_alignment_idxs)):\n",
    "        ref_embedding = ref_embedding_sequence[ref_alignment_idxs[i]]\n",
    "        hyp_embedding = hyp_embedding_sequence[hyp_alignment_idxs[i]]\n",
    "        cosdist = distance.cosine(ref_embedding, hyp_embedding)\n",
    "        cosdist_alignment_tokens.append(round(cosdist, 3))  # FOR PRINTING\n",
    "        ref_token = asd_tokenizer.convert_ids_to_tokens(ref_input_ids[ref_alignment_idxs[i]].reshape(1))[0]\n",
    "        # hyp_token = asd_tokenizer.convert_ids_to_tokens(hyp_input_ids[hyp_alignment_idxs[i]].reshape(1))[0]\n",
    "        ref_alignments.append((ref_alignment_idxs[i], ref_token, cosdist))\n",
    "        # hyp_alignments.append((hyp_alignment_idxs[i], hyp_token, cosdist))\n",
    "\n",
    "    # FOR PRINTING\n",
    "    # ref_alignment_input_ids = np.empty(len(ref_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(ref_alignment_idxs):\n",
    "    #     ref_alignment_input_ids[i] = (ref_input_ids[index])\n",
    "    # ref_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(ref_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_input_ids = np.empty(len(hyp_alignment_idxs), dtype=int)\n",
    "    # for i, index in enumerate(hyp_alignment_idxs):\n",
    "    #     hyp_alignment_input_ids[i] = (hyp_input_ids[index])\n",
    "    # hyp_alignment_tokens = asd_tokenizer.convert_ids_to_tokens(torch.from_numpy(hyp_alignment_input_ids))\n",
    "\n",
    "    # hyp_alignment_tokens.insert(0, \"HYP:\")\n",
    "    # ref_alignment_tokens.insert(0, \"REF:\")\n",
    "    # cosdist_alignment_tokens.insert(0, \"CosDist:\")\n",
    "    # alignment_table = [ref_alignment_tokens, hyp_alignment_tokens, cosdist_alignment_tokens]\n",
    "    # table = tabulate(alignment_table, tablefmt=\"html\")\n",
    "\n",
    "    # print(\"Token alignment table:\")\n",
    "    # display(HTML(table))\n",
    "\n",
    "    return ref_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_token_cosdist(asd_alignments):\n",
    "    # collapse repetitions in tokens and wordpieces in the HYP alignment from ASD\n",
    "    clean_alignment = []\n",
    "    for i, item in enumerate(asd_alignments):\n",
    "        if i < (len(asd_alignments) - 1):\n",
    "            if len(clean_alignment) == 0:\n",
    "                if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "                    clean_alignment.append(item)\n",
    "            else:\n",
    "                if item[0] == clean_alignment[-1][0]:\n",
    "                    averaged_cosdist = sum([item[2], clean_alignment[-1][2]]) / 2\n",
    "                    clean_alignment.pop(-1)\n",
    "                    clean_alignment.append((item[0], item[1], averaged_cosdist))\n",
    "                else:\n",
    "                    clean_alignment.append(item)\n",
    "\n",
    "    # GROUPING THE TOKENS FROM ASD CALCULATION, SUCH THAT WORDPIECES ARE TOGETHER\n",
    "    regrouped_tokens = []\n",
    "    for i, item in enumerate(clean_alignment):\n",
    "        if item[1] != \"[CLS]\" and item[1] != \"[SEP]\":\n",
    "            if \"##\" not in item[1]:\n",
    "                if i < (len(clean_alignment)-1) and \"##\" in clean_alignment[i+1][1]:  # start of a group of wordpieces\n",
    "                    wordpiece_group = []\n",
    "                    wordpiece_group.append(item)\n",
    "                    regrouped_tokens.append(wordpiece_group)\n",
    "                else:\n",
    "                    regrouped_tokens.append(item)\n",
    "            elif \"##\" in item[1]:  # parts of the word\n",
    "                wordpiece_group.append(item)\n",
    "\n",
    "    # COLLAPSE WORDPIECES INTO WORDS & TAKE AVERAGE OF COSDIST\n",
    "    tokens_compressed = []\n",
    "    for token_group in regrouped_tokens:\n",
    "        if isinstance(token_group, list):\n",
    "            wp_combined = ''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in token_group])\n",
    "            token_ave_cosdist = sum([wordpiece[2] for wordpiece in token_group]) / len(token_group)\n",
    "            tokens_compressed.append((\"combined\", wp_combined, token_ave_cosdist))\n",
    "        else:\n",
    "            tokens_compressed.append(token_group)\n",
    "\n",
    "    return tokens_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligning ASD cosdist values to label sequence\n",
    "def get_cosdist_for_ctc(tokens_compressed, label_ids):\n",
    "    cosdist_for_ctc = []\n",
    "    token_count = 0\n",
    "    for i, label in enumerate(label_ids):\n",
    "        # for the first utterance\n",
    "        if len(cosdist_for_ctc) == 0 or all(cosdist == 0 for cosdist in cosdist_for_ctc):\n",
    "            if label == 0 or label > 29:\n",
    "                cosdist_for_ctc.append(0)\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "        # for the next utterances\n",
    "        else:\n",
    "            if label == 0:\n",
    "                cosdist_for_ctc.append(0)\n",
    "                if i < (len(label_ids)-1) and 0 < label_ids[i+1] < 30:\n",
    "                    token_count += 1\n",
    "            else:\n",
    "                cosdist_for_ctc.append(tokens_compressed[token_count][2])\n",
    "    # print(tokens_compressed)\n",
    "    # if len(label_ids) == len(cosdist_for_ctc):\n",
    "    #     print(\"SAME\")\n",
    "    # else:\n",
    "    #     print(\"DIFF!\")\n",
    "    # for i, label in enumerate(label_ids):\n",
    "    #     print(label, cosdist_for_ctc[i])\n",
    "\n",
    "    # try normalizing\n",
    "    x = np.array(cosdist_for_ctc)\n",
    "    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    if np.isnan(np.sum(x_norm)):\n",
    "        print(\"boo!\")\n",
    "\n",
    "    return cosdist_for_ctc, x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07505041360855103, 0.07505041360855103, 0.07505041360855103, 0, 0.12715423107147217, 0.12715423107147217, 0.12715423107147217, 0, 0.029700636863708496, 0, 0.13597166538238525, 0.13597166538238525, 0, 0.5323134958744049, 0.5323134958744049, 0, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0.21112477779388428, 0, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0.021208584308624268, 0, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0.017002344131469727, 0, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0.022676706314086914, 0, 0.6916015446186066, 0.6916015446186066, 0, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0.15313732624053955, 0, 0, 0.032095909118652344, 0.032095909118652344, 0, 0.018728435039520264, 0.018728435039520264, 0.018728435039520264, 0, 0.016046226024627686, 0.016046226024627686, 0, 0.016792714595794678, 0.016792714595794678, 0, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0.017138659954071045, 0, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414, 0.020952463150024414]\n",
      "[0.10851684 0.10851684 0.10851684 0.         0.18385475 0.18385475\n",
      " 0.18385475 0.         0.04294472 0.         0.19660405 0.19660405\n",
      " 0.         0.76968234 0.76968234 0.         0.30526938 0.30526938\n",
      " 0.30526938 0.30526938 0.         0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.0306659  0.0306659  0.0306659  0.0306659  0.0306659\n",
      " 0.0306659  0.         0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402 0.02458402\n",
      " 0.02458402 0.02458402 0.         0.03278869 0.03278869 0.03278869\n",
      " 0.03278869 0.         1.         1.         0.         0.22142421\n",
      " 0.22142421 0.22142421 0.22142421 0.         0.         0.04640809\n",
      " 0.04640809 0.         0.02707981 0.02707981 0.02707981 0.\n",
      " 0.02320155 0.02320155 0.         0.02428091 0.02428091 0.\n",
      " 0.02478112 0.02478112 0.02478112 0.02478112 0.         0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557\n",
      " 0.03029557 0.03029557 0.03029557 0.03029557 0.03029557]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4022911/3475542975.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007969975471496582, 0.007969975471496582, 0.007969975471496582, 0, 0.006086826324462891, 0.006086826324462891, 0, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0.006697893142700195, 0, 0.015287041664123535, 0.015287041664123535, 0, 0.0193021297454834, 0.0193021297454834, 0, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144, 0.2972220443189144]\n",
      "[0.02681489 0.02681489 0.02681489 0.         0.02047905 0.02047905\n",
      " 0.         0.02253498 0.02253498 0.02253498 0.02253498 0.02253498\n",
      " 0.         0.05143307 0.05143307 0.         0.06494178 0.06494178\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan]\n",
      "boo!\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "[0.03979825973510742, 0.03979825973510742, 0.03979825973510742, 0, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0.1796589493751526, 0, 0.01816505193710327, 0.01816505193710327, 0.01816505193710327, 0, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0.008841156959533691, 0]\n",
      "[0.22152116 0.22152116 0.22152116 0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.         0.10110853 0.10110853 0.10110853 0.         0.04921078\n",
      " 0.04921078 0.04921078 0.04921078 0.04921078 0.        ]\n",
      "[0.1013750433921814, 0.1013750433921814, 0, 0.3244396448135376, 0.3244396448135376, 0.3244396448135376, 0, 0.03615701198577881, 0, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0.022709965705871582, 0, 0.018376052379608154, 0.018376052379608154, 0.018376052379608154, 0, 0.032556354999542236, 0, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0.023349285125732422, 0, 0.6256595551967621, 0.6256595551967621, 0, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0.5259131193161011, 0, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566, 0.2584627711524566]\n",
      "[0.16202908 0.16202908 0.         0.51855621 0.51855621 0.51855621\n",
      " 0.         0.05779023 0.         0.03629764 0.03629764 0.03629764\n",
      " 0.03629764 0.03629764 0.         0.02937069 0.02937069 0.02937069\n",
      " 0.         0.05203526 0.         0.03731947 0.03731947 0.03731947\n",
      " 0.03731947 0.         1.         1.         0.         0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394 0.84057394\n",
      " 0.84057394 0.84057394 0.84057394 0.         0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449\n",
      " 0.41310449 0.41310449 0.41310449 0.41310449 0.41310449]\n"
     ]
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output_logits = output[\"logits\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "pred_str = processor.batch_decode(output_logits.detach().numpy()).text\n",
    "labels = batch[\"labels\"]\n",
    "labels_str = processor_woLM.batch_decode(labels, group_tokens=False)\n",
    "\n",
    "for i, logits in enumerate(output_logits):\n",
    "    ref_text = labels_str[i].replace(\"[UNK]\", \"\")\n",
    "    pred_text = pred_str[i].replace(\"[UNK]\", \"\")\n",
    "\n",
    "    label_ids = labels[i]\n",
    "    labels_mask = label_ids >= 0\n",
    "    flattened_labels = label_ids.masked_select(labels_mask)\n",
    "\n",
    "    max_per_frame = torch.argmax(logits, dim=1)\n",
    "    relevant_frames = []\n",
    "    for max_id in max_per_frame:\n",
    "        if max_id in flattened_labels:\n",
    "            relevant_frames.append(1)\n",
    "        else:\n",
    "            relevant_frames.append(0)\n",
    "\n",
    "    ref_alignments = get_asd_align(ref_text, pred_text, asd_model, asd_tokenizer)\n",
    "    tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "    cosdist_for_ctc, x_norm = get_cosdist_for_ctc(tokens_compressed, flattened_labels)\n",
    "\n",
    "    print(cosdist_for_ctc)\n",
    "    print(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CTC with ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCORPORATING ASD COSDIST VALUES TO THE CTC CALCULATION\n",
    "\n",
    "def ctc_loss_with_ASD(params, seq, cosdist_for_ctc, blank=0):\n",
    "    seqLen = seq.shape[0]  # length of label sequence\n",
    "    L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "    T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "    alphas = torch.zeros((L,T))\n",
    "    betas = torch.zeros((L,T))\n",
    "\n",
    "    # convert logits to log probs\n",
    "    params = params - (torch.max(params, dim=0)[0])\n",
    "    params = torch.exp(params)\n",
    "    params = params / torch.sum(params, dim=0)\n",
    "\n",
    "    # initialize alphas and forward pass\n",
    "    alphas[0,0] = params[blank,0]\n",
    "    alphas[1,0] = params[seq[0],0]\n",
    "    c = torch.sum(alphas[:,0])\n",
    "    alphas[:,0] = alphas[:,0].clone() / c\n",
    "    llForward = torch.log(c)\n",
    "\n",
    "    for t in range(1,T):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(start,L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s==0:\n",
    "                    alphas[s,t] = alphas[s,t-1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == 1 or seq[l] == seq[l-1]:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])  # scale 0 to 1\n",
    "            else:\n",
    "                alphas[s,t] = (alphas[s,t-1].clone() + alphas[s-1,t-1].clone() + alphas[s-2,t-1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time (prevent underflow)\n",
    "        c = torch.sum(alphas[start:end,t])\n",
    "        alphas[start:end,t] = alphas[start:end,t].clone() / c\n",
    "        llForward = llForward + torch.log(c)\n",
    "\n",
    "    # initialize betas and backwards pass\n",
    "    betas[-1,-1] = params[blank,-1]\n",
    "    betas[-2,-1] = params[seq[-1],-1]\n",
    "    c = torch.sum(betas[:,-1])\n",
    "    betas[:,-1] = betas[:,-1].clone() / c\n",
    "    llBackward = torch.log(c)\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        start = max(0,L-2*(T-t))\n",
    "        end = min(2*t+2,L)\n",
    "        for s in range(end-1,-1,-1):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                if s == L-1:\n",
    "                    betas[s,t] = betas[s,t+1].clone() * params[blank,t]\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[blank,t]\n",
    "            # same label twice\n",
    "            elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "            else:\n",
    "                betas[s,t] = (betas[s,t+1].clone() + betas[s+1,t+1].clone() + betas[s+2,t+1].clone()) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "        # normalize at current time\n",
    "        c = torch.sum(betas[start:end,t])\n",
    "        betas[start:end,t] = betas[start:end,t].clone() / c\n",
    "        llBackward = llBackward + torch.log(c)\n",
    "\n",
    "    # Compute gradient with respect to unnormalized input parameters\n",
    "    grad = torch.zeros(params.shape)\n",
    "    ab = alphas*betas\n",
    "    for s in range(L):\n",
    "        l = int((s-1)/2)\n",
    "        # blank\n",
    "        if s%2 == 0:\n",
    "            grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/params[blank,:]\n",
    "        else:\n",
    "            grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "            ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "    absum = torch.sum(ab,axis=0)\n",
    "\n",
    "    # Check for underflow or zeros in denominator of gradient\n",
    "    llDiff = torch.abs(llForward-llBackward)\n",
    "    if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "        print(\"Diff in forward/backward LL : %f\"%llDiff)\n",
    "        print(\"Zeros found : (%d/%d)\"%(torch.sum(absum==0),absum.shape[0]))\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)\n",
    "    else:\n",
    "        grad = params - grad / (params * absum)\n",
    "        # print(-llForward, grad)\n",
    "        return (-llForward, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out custom ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import gradcheck\n",
    "\n",
    "# output = model(**batch)\n",
    "# ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "# label_ids = batch[\"labels\"][0]\n",
    "# logits = output[\"logits\"][0]\n",
    "# hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "# ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "# tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "# cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "# inputs = (logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "\n",
    "# test = gradcheck(ctc_loss_with_ASD, inputs)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "print(tokens_compressed)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "\n",
    "loss, grad = ctc_loss_with_ASD(logits.transpose(1,0), label_ids, cosdist_for_ctc, blank=0)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "log_probs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n",
    "input_lengths = torch.tensor(logits.shape[0])\n",
    "target_lengths = torch.tensor(label_ids.shape[0])\n",
    "\n",
    "loss = F.ctc_loss(log_probs=log_probs, targets=label_ids, input_lengths=input_lengths, target_lengths=target_lengths, reduction=\"sum\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extending torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCTC(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, params, seq, cosdist_for_ctc, blank=0):\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "        T = params.shape[1]  # length of utterance (time)\n",
    "\n",
    "        alphas = torch.zeros((L,T)).double()\n",
    "        betas = torch.zeros((L,T)).double()\n",
    "\n",
    "        # convert logits to log probs\n",
    "        params = params - (torch.max(params, dim=0)[0])\n",
    "        params = torch.exp(params)\n",
    "        params = params / torch.sum(params, dim=0)\n",
    "\n",
    "        # initialize alphas and forward pass\n",
    "        alphas[0,0] = params[blank,0]\n",
    "        alphas[1,0] = params[seq[0],0]\n",
    "        c = torch.sum(alphas[:,0])\n",
    "        alphas[:,0] = alphas[:,0] / c\n",
    "        llForward = torch.log(c)\n",
    "\n",
    "        for t in range(1,T):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(start,L):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s==0:\n",
    "                        alphas[s,t] = alphas[s,t-1] * params[blank,t]\n",
    "                    else:\n",
    "                        alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == 1 or seq[l] == seq[l-1]:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    alphas[s,t] = (alphas[s,t-1] + alphas[s-1,t-1] + alphas[s-2,t-1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time (prevent underflow)\n",
    "            c = torch.sum(alphas[start:end,t])\n",
    "            alphas[start:end,t] = alphas[start:end,t] / c\n",
    "            llForward = llForward + torch.log(c)\n",
    "\n",
    "        # initialize betas and backwards pass\n",
    "        betas[-1,-1] = params[blank,-1]\n",
    "        betas[-2,-1] = params[seq[-1],-1]\n",
    "        c = torch.sum(betas[:,-1])\n",
    "        betas[:,-1] = betas[:,-1] / c\n",
    "        llBackward = torch.log(c)\n",
    "\n",
    "        for t in range(T-2,-1,-1):\n",
    "            start = max(0,L-2*(T-t))\n",
    "            end = min(2*t+2,L)\n",
    "            for s in range(end-1,-1,-1):\n",
    "                l = int((s-1)/2)\n",
    "                # blank\n",
    "                if s%2 == 0:\n",
    "                    if s == L-1:\n",
    "                        betas[s,t] = betas[s,t+1] * params[blank,t]\n",
    "                    else:\n",
    "                        betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[blank,t]\n",
    "                # same label twice\n",
    "                elif s == L-2 or seq[l] == seq[l+1]:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "                else:\n",
    "                    betas[s,t] = (betas[s,t+1] + betas[s+1,t+1] + betas[s+2,t+1]) * params[seq[l],t] * (1 - cosdist_for_ctc[l])\n",
    "\n",
    "            # normalize at current time\n",
    "            c = torch.sum(betas[start:end,t])\n",
    "            betas[start:end,t] = betas[start:end,t] / c\n",
    "            llBackward = llBackward + torch.log(c)\n",
    "\n",
    "        ctx.save_for_backward(params, seq, alphas, betas, llForward, llBackward)\n",
    "\n",
    "        return -llForward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        params, seq, alphas, betas, llForward, llBackward = ctx.saved_tensors\n",
    "        blank = 0\n",
    "        seqLen = seq.shape[0]  # length of label sequence\n",
    "        L = 2*seqLen + 1  # length of the label sequence with blanks\n",
    "\n",
    "        # Compute gradient with respect to unnormalized input parameters\n",
    "        grad = torch.zeros(params.shape)\n",
    "        ab = alphas*betas\n",
    "        for s in range(L):\n",
    "            l = int((s-1)/2)\n",
    "            # blank\n",
    "            if s%2 == 0:\n",
    "                grad[blank,:] = grad[blank,:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/params[blank,:]\n",
    "            else:\n",
    "                grad[seq[l],:] = grad[seq[l],:] + ab[s,:]\n",
    "                ab[s,:] = ab[s,:]/(params[seq[l],:])\n",
    "        absum = torch.sum(ab,axis=0)\n",
    "\n",
    "        llDiff = torch.abs(llForward-llBackward)\n",
    "        if llDiff > 1e-5 or torch.sum(absum==0) > 0:\n",
    "            return (grad, None, None)\n",
    "        else:\n",
    "            grad = params - grad / (params * absum)\n",
    "            return (grad, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "output = model(**batch)\n",
    "ref = processor_woLM.batch_decode(batch[\"labels\"], group_tokens=False)[0]\n",
    "label_ids = batch[\"labels\"][0]\n",
    "logits = output[\"logits\"][0]\n",
    "hyp = processor.batch_decode(output[\"logits\"].detach().numpy()).text[0]\n",
    "\n",
    "print(\"REF:\", ref)\n",
    "print(\"HYP:\", hyp)\n",
    "\n",
    "ref_alignments = get_asd_align(ref, hyp, asd_model, asd_tokenizer)\n",
    "tokens_compressed = get_per_token_cosdist(ref_alignments)\n",
    "cosdist_for_ctc = get_cosdist_for_ctc(tokens_compressed, label_ids)\n",
    "print(tokens_compressed)\n",
    "\n",
    "myctcloss = MyCTC.apply\n",
    "loss = myctcloss(logits.transpose(1,0), label_ids, cosdist_for_ctc)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS w/ FRAME NOS. & DISREGARDING ALL THE PADS\n",
    "\n",
    "clean_logit_frames = []\n",
    "for i, item in enumerate(logit_frames_decoded):\n",
    "    if item != \"[PAD]\":\n",
    "        clean_logit_frames.append((i, item))\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(clean_logit_frames):\n",
    "    if i == 0:\n",
    "        if char[1] == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if char[1] != \"\":\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if clean_logit_frames[i-1][1] == \"\" and char[1] != \"\":  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append(char)\n",
    "                char_group_list.append(new_word)\n",
    "            elif char[1] != \"\":\n",
    "                new_word.append(char)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(\" \".join([char[1] for char in item]))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING CHARACTERS FROM LOGITS MATRIX INTO THEIR CORRESPONDING WORDS, WHILE RETAINING ROW NOS. OR FRAME NOS.\n",
    "\n",
    "char_group_list = []\n",
    "for i, char in enumerate(logit_frames_decoded):\n",
    "    if i == 0:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "    else:\n",
    "        if char == \"[PAD]\" or char == \"\":\n",
    "            continue\n",
    "        if len(char_group_list) == 0:  # first word\n",
    "            if logit_frames_decoded[i-1] in [\"\", \"[PAD]\"] and char not in [\"\", \"[PAD]\"]:\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "        else:\n",
    "            if \"\" in logit_frames_decoded[i-2:i-1] and char not in [\"\", \"[PAD]\"]:  # start of the word\n",
    "                new_word = []\n",
    "                new_word.append((i, char))\n",
    "                char_group_list.append(new_word)\n",
    "            elif logit_frames_decoded[i-1] == char and char != \"\":  # repeating characters\n",
    "                new_word.append((i, char))\n",
    "            elif logit_frames_decoded[i-1] != char and char != \"\":  # new character detected\n",
    "                new_word.append((i, char))\n",
    "\n",
    "\n",
    "for item in char_group_list:\n",
    "    print(' '.join([char[1] for char in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # detecting applicable frames & caclulating average cosdist\n",
    "        # frame_group = []\n",
    "        # for token_char in token_char_list:\n",
    "        #     detect_count = 0\n",
    "        #     if token_char in [x[1] for x in char_group_copy]:\n",
    "        #         for i, char in enumerate(char_group_copy):\n",
    "        #             # print(char[1], token_char, [x[1] for x in char_group_copy])\n",
    "        #             if char[1] == token_char:\n",
    "        #                 frame_group.append(char)\n",
    "        #                 detect_count += 1\n",
    "        #             else:\n",
    "        #                 if detect_count > 0:\n",
    "        #                     for count in range(detect_count):\n",
    "        #                         char_group_copy.pop(0)\n",
    "        #                     break\n",
    "        #     else:\n",
    "        #         continue\n",
    "        # cosdist_divided = token_cosdist / len(frame_group)  # divide ASD token cosdist by number of frames for word\n",
    "        # assign divided ASD token cosdist\n",
    "        # frame_cosdist = []\n",
    "        # for char in frame_group:\n",
    "        #     if char[1] in token_char_list:\n",
    "        #         frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDING THE ASD COSDIST PER NO. FRAMES\n",
    "if len(char_group_list) == len(regrouped_tokens):\n",
    "    ASD_cosdist_list = []\n",
    "    for i, char_group in enumerate(char_group_list):\n",
    "        # char_group_copy = copy.deepcopy(char_group)\n",
    "        if isinstance(regrouped_tokens[i], list): # token character list differs when there are whole word tokens or wordpieces\n",
    "            token_char_list = list(''.join([wordpiece[1].replace(\"##\", \"\") for wordpiece in regrouped_tokens[i]]))\n",
    "            token_cosdist = sum([wordpiece[2] for wordpiece in regrouped_tokens[i]])\n",
    "        else:\n",
    "            token_char_list = list(regrouped_tokens[i][1])\n",
    "            token_cosdist = regrouped_tokens[i][2]\n",
    "\n",
    "        # simpler implementation by not checking if each frame belongs to the decoded word\n",
    "        # it probably does not need checking anyway since we're taking the whole sequence when decoding\n",
    "        cosdist_divided = token_cosdist / len(char_group)  # divide ASD token cosdist by number of frames for word utterance\n",
    "        frame_cosdist = []\n",
    "        for char in char_group:\n",
    "            frame_cosdist.append((char[0], char[1], cosdist_divided))\n",
    "        ASD_cosdist_list.append(frame_cosdist)\n",
    "else:\n",
    "    print(\"logit char grouped frames quantity not equal to regrouped tokens from ASD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
