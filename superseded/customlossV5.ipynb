{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3068008/3036173106.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, \"Container\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # or \"0,1\" for multiple GPUs\n",
    "\n",
    "import collections\n",
    "if not hasattr(collections, \"Container\"):\n",
    "    import collections.abc\n",
    "    collections.Container = collections.abc.Container\n",
    "# import transformers\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Audio, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "# import math\n",
    "import numpy as np\n",
    "# import librosa\n",
    "import os\n",
    "import torch\n",
    "# from pydub import AudioSegment\n",
    "# from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import wandb\n",
    "import argparse\n",
    "# import types\n",
    "from customCTCwithASD import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\*\\(\\)\\'\\_]'\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_files(data_dir_list:list[str], split_ratio=0.1, csv_export=True):\n",
    "    frames = []\n",
    "    for path in data_dir_list:\n",
    "        source = os.path.basename(os.path.dirname(path))\n",
    "        wavfile_data = []\n",
    "        textfile_data = []\n",
    "        for (root, dirs, files) in os.walk(path, topdown=True):\n",
    "            if source == \"Rundkast\":  # to modify depending on Rundkast cuts folder name\n",
    "                for fn in files:\n",
    "                    if fn.endswith(\".wav\"):\n",
    "                        wav_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        path = os.path.join(root, fn)\n",
    "                        wavfile_data.append((wav_id, fn, path, source))\n",
    "                    elif fn.endswith(\".txt\"):\n",
    "                        text_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        with open(os.path.join(root, fn), encoding=\"utf-8\") as text_file:\n",
    "                            text = text_file.read()\n",
    "                        textfile_data.append((text_id, text))\n",
    "            else:\n",
    "                for fn in files:\n",
    "                    if fn.endswith(\".wav\"):\n",
    "                        wav_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        path = os.path.join(root, fn)\n",
    "                        wavfile_data.append((wav_id, fn, path, source))\n",
    "                    elif fn.endswith(\".txt-utf8\"):\n",
    "                        text_id = source + \"_\" + os.path.splitext(fn)[0]\n",
    "                        with open(os.path.join(root, fn), encoding=\"utf-8-sig\") as text_file:\n",
    "                            text = text_file.read()\n",
    "                        textfile_data.append((text_id, text))\n",
    "        df_wav = pd.DataFrame(wavfile_data, columns=[\"segment_id\", \"wav_file\", \"path\", \"source\"])\n",
    "        df_wav = df_wav.set_index(\"segment_id\")\n",
    "        df_text = pd.DataFrame(textfile_data, columns=[\"segment_id\", \"text\"])\n",
    "        df_text = df_text.set_index(\"segment_id\")\n",
    "        dataset_df = df_wav.merge(df_text, left_index=True, right_index=True)\n",
    "        frames.append(dataset_df)\n",
    "    # concat to full dataframe and convert to Dataset with special characters removed\n",
    "    full_dataset_df = pd.concat(frames)\n",
    "    raw_dataset = Dataset.from_pandas(full_dataset_df)\n",
    "    raw_dataset = raw_dataset.map(remove_special_characters)\n",
    "    # split dataset\n",
    "    raw_dataset = raw_dataset.train_test_split(test_size=split_ratio)\n",
    "    # save copy of dataset\n",
    "    if csv_export is True:\n",
    "        df_train = pd.DataFrame(raw_dataset[\"train\"])\n",
    "        # df_train.to_csv(os.path.join(csv_export_dir, \"train_set.csv\"))\n",
    "        df_dev = pd.DataFrame(raw_dataset[\"test\"])\n",
    "        # df_dev.to_csv(os.path.join(csv_export_dir, \"dev_set.csv\"))\n",
    "    # loading audio\n",
    "    dataset = raw_dataset.cast_column(\"path\", Audio())\n",
    "    dataset = dataset.rename_column(\"path\", \"audio\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    # preprocess dataset\n",
    "    dataset = dataset.map(prepare_dataset,\n",
    "                          remove_columns=dataset.column_names[\"train\"],\n",
    "                          num_proc=4)\n",
    "    return raw_dataset, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NbAiLab/nb-wav2vec2-300m-bokmaal\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset direct from data dir to pandas dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [00:00<00:00, 20779.89ex/s]\n",
      "#0:   0%|          | 0/1688 [00:00<?, ?ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   0%|          | 1/1688 [00:00<09:27,  2.97ex/s]/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:   7%|▋         | 112/1688 [00:00<00:04, 332.93ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  14%|█▍        | 233/1688 [00:00<00:02, 593.52ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  21%|██▏       | 359/1688 [00:00<00:01, 792.44ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  29%|██▊       | 482/1688 [00:00<00:01, 922.16ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  36%|███▌      | 604/1688 [00:00<00:01, 1009.46ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  43%|████▎     | 725/1688 [00:00<00:00, 1067.76ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  50%|█████     | 847/1688 [00:01<00:00, 1113.19ex/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  58%|█████▊    | 971/1688 [00:01<00:00, 1150.72ex/s]\n",
      "\n",
      "#0:  65%|██████▍   | 1091/1688 [00:01<00:00, 609.09ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  73%|███████▎  | 1228/1688 [00:01<00:00, 747.24ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  80%|████████  | 1357/1688 [00:01<00:00, 859.20ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  88%|████████▊ | 1488/1688 [00:01<00:00, 961.55ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  96%|█████████▌| 1622/1688 [00:01<00:00, 1054.28ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 1688/1688 [00:01<00:00, 861.57ex/s] \n",
      "#0: 100%|██████████| 1688/1688 [00:01<00:00, 844.98ex/s] \n",
      "#2: 100%|██████████| 1687/1687 [00:01<00:00, 852.93ex/s] \n",
      "#3: 100%|██████████| 1687/1687 [00:01<00:00, 868.62ex/s] \n",
      "#0:   0%|          | 0/188 [00:00<?, ?ex/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "#0:   1%|          | 1/188 [00:00<00:48,  3.84ex/s]/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A/home/janinelr/micromamba/envs/wav2vec/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "#0:  59%|█████▊    | 110/188 [00:00<00:00, 387.73ex/s]\n",
      "\u001b[A\n",
      "\n",
      "#0: 100%|██████████| 188/188 [00:00<00:00, 443.18ex/s]\n",
      "#2: 100%|██████████| 187/187 [00:00<00:00, 486.38ex/s]\n",
      "#1: 100%|██████████| 188/188 [00:00<00:00, 447.57ex/s]\n",
      "#3: 100%|██████████| 187/187 [00:00<00:00, 456.67ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['wav_file', 'path', 'source', 'text', 'segment_id'],\n",
      "        num_rows: 6750\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['wav_file', 'path', 'source', 'text', 'segment_id'],\n",
      "        num_rows: 750\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 6750\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 750\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset direct from data dir to pandas dataframe\")\n",
    "\n",
    "data_dir_list = [\"../../datasets/NordTrans_TUL/train_small/Rundkast/\"]\n",
    "\n",
    "raw_dataset, dataset = load_dataset_from_files(data_dir_list, split_ratio=0.1, csv_export=True)\n",
    "\n",
    "print(raw_dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
